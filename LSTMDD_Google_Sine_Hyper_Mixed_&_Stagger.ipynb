{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tajwarresearch/LSTMDD/blob/main/LSTMDD_Google_Sine_Hyper_Mixed_%26_Stagger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4kidZ3cM2Nv"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erdvdFBzPzoT"
      },
      "source": [
        "#libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdD9SIa3KUON",
        "outputId": "12d9e74d-fdd8-433c-cf6a-aac29156bb55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dynarray in /usr/local/lib/python3.10/dist-packages (0.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dynarray) (1.23.5)\n",
            "Requirement already satisfied: deap in /usr/local/lib/python3.10/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deap) (1.23.5)\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install dynarray\n",
        "!pip install deap\n",
        "!pip install keras-tuner\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wGiZM8dOVCel"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import kstest\n",
        "from dynarray import DynamicArray\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import csv\n",
        "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as stats\n",
        "import seaborn as sb\n",
        "from keras.layers import Layer, TimeDistributed, RepeatVector\n",
        "from keras.layers import Bidirectional, Flatten, Concatenate\n",
        "from keras.layers import Lambda, Permute, dot\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from keras.layers import Attention\n",
        "from deap import base, creator, tools, algorithms\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Lcgw8wlLP1uW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# importing the necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model# Download the dataset\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import kstest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
        "from matplotlib import pyplot as plt\n",
        "from dynarray import DynamicArray\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "import csv\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import kstest\n",
        "# importing the necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model# Download the dataset\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import kstest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
        "from matplotlib import pyplot as plt\n",
        "from dynarray import DynamicArray\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "import csv\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import kstest\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as mp\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "sns.set()\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import kstest\n",
        "import keras.backend as K\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout\n",
        "from keras.layers import Layer, TimeDistributed, RepeatVector\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Flatten, Input, Concatenate\n",
        "from keras.layers import Lambda, Permute, dot\n",
        "from keras import backend as K\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "import numpy as np\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.layers import Attention\n",
        "from keras.models import Model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from deap import base, creator, tools, algorithms\n",
        "from keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.layers import Attention\n",
        "from keras.models import Model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from deap import base, creator, tools, algorithms\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "from sklearn import metrics\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import kstest\n",
        "from dynarray import DynamicArray\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import roc_curve\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as stats\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "from keras.layers import Layer, TimeDistributed, RepeatVector\n",
        "from keras.layers import Bidirectional, Flatten, Concatenate\n",
        "from keras.layers import Lambda, Permute, dot\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from keras.layers import Attention\n",
        "from deap import base, creator, tools, algorithms\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.models import Model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHrhxJ5xNafj"
      },
      "source": [
        "#Artifical Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKRdFchGNC1d",
        "outputId": "7045c1a4-ff4a-4add-95fd-46934a6ce370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of ones: 20501\n",
            "Number of zeros: 20499\n",
            "Units: 64 Batch Size: 48 Dropout Rate: 0.1\n",
            "Epoch 1/500\n",
            "479/479 [==============================] - 9s 9ms/step - loss: 0.6751 - val_loss: 0.6685\n",
            "Epoch 2/500\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.6689 - val_loss: 0.6680\n",
            "Epoch 3/500\n",
            "479/479 [==============================] - 5s 10ms/step - loss: 0.6689 - val_loss: 0.6678\n",
            "Epoch 4/500\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.6686 - val_loss: 0.6685\n",
            "Epoch 5/500\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.6684 - val_loss: 0.6684\n",
            "Epoch 6/500\n",
            "479/479 [==============================] - 5s 10ms/step - loss: 0.6684 - val_loss: 0.6674\n",
            "Epoch 7/500\n",
            "479/479 [==============================] - 5s 10ms/step - loss: 0.6682 - val_loss: 0.6678\n",
            "Epoch 8/500\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.6682 - val_loss: 0.6672\n",
            "Epoch 9/500\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.6682 - val_loss: 0.6670\n",
            "Epoch 10/500\n",
            "479/479 [==============================] - 3s 6ms/step - loss: 0.6680 - val_loss: 0.6669\n",
            "Epoch 11/500\n",
            "479/479 [==============================] - 3s 6ms/step - loss: 0.6679 - val_loss: 0.6668\n",
            "Epoch 12/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6679 - val_loss: 0.6677\n",
            "Epoch 13/500\n",
            "479/479 [==============================] - 2s 5ms/step - loss: 0.6674 - val_loss: 0.6671\n",
            "Epoch 14/500\n",
            "479/479 [==============================] - 2s 5ms/step - loss: 0.6676 - val_loss: 0.6660\n",
            "Epoch 15/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6674 - val_loss: 0.6660\n",
            "Epoch 16/500\n",
            "479/479 [==============================] - 3s 5ms/step - loss: 0.6672 - val_loss: 0.6658\n",
            "Epoch 17/500\n",
            "479/479 [==============================] - 3s 6ms/step - loss: 0.6669 - val_loss: 0.6655\n",
            "Epoch 18/500\n",
            "479/479 [==============================] - 2s 5ms/step - loss: 0.6670 - val_loss: 0.6659\n",
            "Epoch 19/500\n",
            "479/479 [==============================] - 2s 5ms/step - loss: 0.6669 - val_loss: 0.6661\n",
            "Epoch 20/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6665 - val_loss: 0.6653\n",
            "Epoch 21/500\n",
            "479/479 [==============================] - 2s 5ms/step - loss: 0.6665 - val_loss: 0.6654\n",
            "Epoch 22/500\n",
            "479/479 [==============================] - 3s 5ms/step - loss: 0.6665 - val_loss: 0.6649\n",
            "Epoch 23/500\n",
            "479/479 [==============================] - 3s 6ms/step - loss: 0.6664 - val_loss: 0.6654\n",
            "Epoch 24/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6663 - val_loss: 0.6657\n",
            "Epoch 25/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6661 - val_loss: 0.6658\n",
            "Epoch 26/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6665 - val_loss: 0.6653\n",
            "Epoch 27/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6660 - val_loss: 0.6648\n",
            "Epoch 28/500\n",
            "479/479 [==============================] - 2s 5ms/step - loss: 0.6658 - val_loss: 0.6648\n",
            "Epoch 29/500\n",
            "479/479 [==============================] - 3s 6ms/step - loss: 0.6658 - val_loss: 0.6649\n",
            "Epoch 30/500\n",
            "479/479 [==============================] - 2s 5ms/step - loss: 0.6658 - val_loss: 0.6657\n",
            "Epoch 31/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6657 - val_loss: 0.6646\n",
            "Epoch 32/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6657 - val_loss: 0.6648\n",
            "Epoch 33/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6657 - val_loss: 0.6647\n",
            "Epoch 34/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6656 - val_loss: 0.6651\n",
            "Epoch 35/500\n",
            "479/479 [==============================] - 3s 6ms/step - loss: 0.6657 - val_loss: 0.6650\n",
            "Epoch 36/500\n",
            "479/479 [==============================] - 3s 6ms/step - loss: 0.6656 - val_loss: 0.6655\n",
            "Epoch 37/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6655 - val_loss: 0.6644\n",
            "Epoch 38/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6654 - val_loss: 0.6648\n",
            "Epoch 39/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6652 - val_loss: 0.6649\n",
            "Epoch 40/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6653 - val_loss: 0.6652\n",
            "Epoch 41/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6654 - val_loss: 0.6649\n",
            "Epoch 42/500\n",
            "479/479 [==============================] - 3s 6ms/step - loss: 0.6653 - val_loss: 0.6648\n",
            "Epoch 43/500\n",
            "479/479 [==============================] - 2s 5ms/step - loss: 0.6652 - val_loss: 0.6650\n",
            "Epoch 44/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6651 - val_loss: 0.6650\n",
            "Epoch 45/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6653 - val_loss: 0.6653\n",
            "Epoch 46/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6652 - val_loss: 0.6645\n",
            "Epoch 47/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6651 - val_loss: 0.6648\n",
            "385/385 [==============================] - 1s 2ms/step\n",
            "Predicted Class Labels: [0 1 0 ... 0 1 1]\n",
            "Accuracy: 0.23260162601626017\n",
            "Precision: 0.23259958282185036\n",
            "Recall: 0.23260162601626017\n",
            "F-score: 0.23260016009945317\n",
            "ROC AUC: 0.23260162601626017\n",
            "False Positive Rate (FPR): 0.7660162601626016\n",
            "False Negative Rate (FNR): 0.7687804878048781\n",
            "TN: 1439 FP: 4711 FN: 4728 TP: 1422\n",
            "Units: 288 Batch Size: 160 Dropout Rate: 0.1\n",
            "Epoch 1/500\n",
            "144/144 [==============================] - 6s 22ms/step - loss: 0.6763 - val_loss: 0.6701\n",
            "Epoch 2/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6696 - val_loss: 0.6679\n",
            "Epoch 3/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6690 - val_loss: 0.6677\n",
            "Epoch 4/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6688 - val_loss: 0.6679\n",
            "Epoch 5/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6685 - val_loss: 0.6677\n",
            "Epoch 6/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6684 - val_loss: 0.6680\n",
            "Epoch 7/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6686 - val_loss: 0.6676\n",
            "Epoch 8/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6684 - val_loss: 0.6674\n",
            "Epoch 9/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6686 - val_loss: 0.6673\n",
            "Epoch 10/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6683 - val_loss: 0.6681\n",
            "Epoch 11/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6683 - val_loss: 0.6684\n",
            "Epoch 12/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6684 - val_loss: 0.6678\n",
            "Epoch 13/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6682 - val_loss: 0.6672\n",
            "Epoch 14/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6680 - val_loss: 0.6670\n",
            "Epoch 15/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6679 - val_loss: 0.6667\n",
            "Epoch 16/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6678 - val_loss: 0.6665\n",
            "Epoch 17/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6674 - val_loss: 0.6667\n",
            "Epoch 18/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6674 - val_loss: 0.6663\n",
            "Epoch 19/500\n",
            "144/144 [==============================] - 3s 23ms/step - loss: 0.6672 - val_loss: 0.6662\n",
            "Epoch 20/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6673 - val_loss: 0.6669\n",
            "Epoch 21/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6673 - val_loss: 0.6665\n",
            "Epoch 22/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6672 - val_loss: 0.6656\n",
            "Epoch 23/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6670 - val_loss: 0.6655\n",
            "Epoch 24/500\n",
            "144/144 [==============================] - 3s 23ms/step - loss: 0.6669 - val_loss: 0.6654\n",
            "Epoch 25/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6667 - val_loss: 0.6654\n",
            "Epoch 26/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6667 - val_loss: 0.6656\n",
            "Epoch 27/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6665 - val_loss: 0.6652\n",
            "Epoch 28/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6666 - val_loss: 0.6652\n",
            "Epoch 29/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6667 - val_loss: 0.6651\n",
            "Epoch 30/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6664 - val_loss: 0.6651\n",
            "Epoch 31/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6665 - val_loss: 0.6652\n",
            "Epoch 32/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6665 - val_loss: 0.6655\n",
            "Epoch 33/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6665 - val_loss: 0.6651\n",
            "Epoch 34/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6663 - val_loss: 0.6649\n",
            "Epoch 35/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6663 - val_loss: 0.6652\n",
            "Epoch 36/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6661 - val_loss: 0.6657\n",
            "Epoch 37/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6661 - val_loss: 0.6648\n",
            "Epoch 38/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6662 - val_loss: 0.6648\n",
            "Epoch 39/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6661 - val_loss: 0.6653\n",
            "Epoch 40/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6661 - val_loss: 0.6647\n",
            "Epoch 41/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6659 - val_loss: 0.6648\n",
            "Epoch 42/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6658 - val_loss: 0.6648\n",
            "Epoch 43/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6660 - val_loss: 0.6654\n",
            "Epoch 44/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6658 - val_loss: 0.6646\n",
            "Epoch 45/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6657 - val_loss: 0.6644\n",
            "Epoch 46/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6657 - val_loss: 0.6651\n",
            "Epoch 47/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6658 - val_loss: 0.6654\n",
            "Epoch 48/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6659 - val_loss: 0.6645\n",
            "Epoch 49/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6653 - val_loss: 0.6661\n",
            "Epoch 50/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6657 - val_loss: 0.6645\n",
            "Epoch 51/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6658 - val_loss: 0.6644\n",
            "Epoch 52/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6655 - val_loss: 0.6645\n",
            "Epoch 53/500\n",
            "144/144 [==============================] - 4s 31ms/step - loss: 0.6656 - val_loss: 0.6645\n",
            "Epoch 54/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6655 - val_loss: 0.6650\n",
            "Epoch 55/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6657 - val_loss: 0.6651\n",
            "Epoch 56/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6654 - val_loss: 0.6648\n",
            "Epoch 57/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6655 - val_loss: 0.6646\n",
            "Epoch 58/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6655 - val_loss: 0.6647\n",
            "Epoch 59/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6653 - val_loss: 0.6646\n",
            "Epoch 60/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6653 - val_loss: 0.6648\n",
            "Epoch 61/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6653 - val_loss: 0.6645\n",
            "385/385 [==============================] - 2s 3ms/step\n",
            "Predicted Class Labels: [0 1 0 ... 0 1 1]\n",
            "Accuracy: 0.2348780487804878\n",
            "Precision: 0.23479165491387918\n",
            "Recall: 0.2348780487804878\n",
            "F-score: 0.23481573246506512\n",
            "ROC AUC: 0.2348780487804878\n",
            "False Positive Rate (FPR): 0.7560975609756098\n",
            "False Negative Rate (FNR): 0.7741463414634147\n",
            "TN: 1500 FP: 4650 FN: 4761 TP: 1389\n",
            "Units: 288 Batch Size: 48 Dropout Rate: 0.1\n",
            "Epoch 1/500\n",
            "479/479 [==============================] - 10s 13ms/step - loss: 0.6747 - val_loss: 0.6706\n",
            "Epoch 2/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6693 - val_loss: 0.6686\n",
            "Epoch 3/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6691 - val_loss: 0.6683\n",
            "Epoch 4/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6689 - val_loss: 0.6676\n",
            "Epoch 5/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6688 - val_loss: 0.6677\n",
            "Epoch 6/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6687 - val_loss: 0.6675\n",
            "Epoch 7/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6687 - val_loss: 0.6672\n",
            "Epoch 8/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6684 - val_loss: 0.6682\n",
            "Epoch 9/500\n",
            "479/479 [==============================] - 7s 16ms/step - loss: 0.6683 - val_loss: 0.6683\n",
            "Epoch 10/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6680 - val_loss: 0.6670\n",
            "Epoch 11/500\n",
            "479/479 [==============================] - 9s 19ms/step - loss: 0.6677 - val_loss: 0.6668\n",
            "Epoch 12/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6680 - val_loss: 0.6663\n",
            "Epoch 13/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6676 - val_loss: 0.6660\n",
            "Epoch 14/500\n",
            "479/479 [==============================] - 6s 14ms/step - loss: 0.6675 - val_loss: 0.6658\n",
            "Epoch 15/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6673 - val_loss: 0.6655\n",
            "Epoch 16/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6674 - val_loss: 0.6654\n",
            "Epoch 17/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6669 - val_loss: 0.6659\n",
            "Epoch 18/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6671 - val_loss: 0.6658\n",
            "Epoch 19/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6668 - val_loss: 0.6664\n",
            "Epoch 20/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6669 - val_loss: 0.6666\n",
            "Epoch 21/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6666 - val_loss: 0.6652\n",
            "Epoch 22/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6671 - val_loss: 0.6653\n",
            "Epoch 23/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6664 - val_loss: 0.6653\n",
            "Epoch 24/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6666 - val_loss: 0.6650\n",
            "Epoch 25/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6663 - val_loss: 0.6650\n",
            "Epoch 26/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6663 - val_loss: 0.6658\n",
            "Epoch 27/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6663 - val_loss: 0.6656\n",
            "Epoch 28/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6663 - val_loss: 0.6651\n",
            "Epoch 29/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6658 - val_loss: 0.6659\n",
            "Epoch 30/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6664 - val_loss: 0.6648\n",
            "Epoch 31/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6656 - val_loss: 0.6656\n",
            "Epoch 32/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6658 - val_loss: 0.6649\n",
            "Epoch 33/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6658 - val_loss: 0.6646\n",
            "Epoch 34/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6655 - val_loss: 0.6649\n",
            "Epoch 35/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6655 - val_loss: 0.6651\n",
            "Epoch 36/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6653 - val_loss: 0.6651\n",
            "Epoch 37/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6656 - val_loss: 0.6660\n",
            "Epoch 38/500\n",
            "479/479 [==============================] - 6s 14ms/step - loss: 0.6655 - val_loss: 0.6659\n",
            "Epoch 39/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6652 - val_loss: 0.6647\n",
            "Epoch 40/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6653 - val_loss: 0.6648\n",
            "Epoch 41/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6654 - val_loss: 0.6652\n",
            "Epoch 42/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6652 - val_loss: 0.6647\n",
            "Epoch 43/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6652 - val_loss: 0.6652\n",
            "385/385 [==============================] - 3s 5ms/step\n",
            "Predicted Class Labels: [0 1 0 ... 0 1 1]\n",
            "Accuracy: 0.23536585365853657\n",
            "Precision: 0.234704770703524\n",
            "Recall: 0.23536585365853657\n",
            "F-score: 0.23488921325573497\n",
            "ROC AUC: 0.23536585365853663\n",
            "False Positive Rate (FPR): 0.7396747967479674\n",
            "False Negative Rate (FNR): 0.7895934959349593\n",
            "TN: 1601 FP: 4549 FN: 4856 TP: 1294\n",
            "Units: 288 Batch Size: 64 Dropout Rate: 0.1\n",
            "Epoch 1/500\n",
            "359/359 [==============================] - 8s 17ms/step - loss: 0.6744 - val_loss: 0.6679\n",
            "Epoch 2/500\n",
            "359/359 [==============================] - 5s 15ms/step - loss: 0.6687 - val_loss: 0.6679\n",
            "Epoch 3/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6689 - val_loss: 0.6694\n",
            "Epoch 4/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6688 - val_loss: 0.6677\n",
            "Epoch 5/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6687 - val_loss: 0.6678\n",
            "Epoch 6/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6687 - val_loss: 0.6676\n",
            "Epoch 7/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6686 - val_loss: 0.6698\n",
            "Epoch 8/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6685 - val_loss: 0.6675\n",
            "Epoch 9/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6682 - val_loss: 0.6676\n",
            "Epoch 10/500\n",
            "359/359 [==============================] - 6s 17ms/step - loss: 0.6684 - val_loss: 0.6673\n",
            "Epoch 11/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6683 - val_loss: 0.6674\n",
            "Epoch 12/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6680 - val_loss: 0.6673\n",
            "Epoch 13/500\n",
            "359/359 [==============================] - 6s 18ms/step - loss: 0.6677 - val_loss: 0.6667\n",
            "Epoch 14/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6675 - val_loss: 0.6665\n",
            "Epoch 15/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6674 - val_loss: 0.6658\n",
            "Epoch 16/500\n",
            "359/359 [==============================] - 6s 15ms/step - loss: 0.6669 - val_loss: 0.6656\n",
            "Epoch 17/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6669 - val_loss: 0.6666\n",
            "Epoch 18/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6669 - val_loss: 0.6661\n",
            "Epoch 19/500\n",
            "359/359 [==============================] - 5s 15ms/step - loss: 0.6668 - val_loss: 0.6653\n",
            "Epoch 20/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6668 - val_loss: 0.6652\n",
            "Epoch 21/500\n",
            "359/359 [==============================] - 6s 18ms/step - loss: 0.6667 - val_loss: 0.6663\n",
            "Epoch 22/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6668 - val_loss: 0.6650\n",
            "Epoch 23/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6666 - val_loss: 0.6657\n",
            "Epoch 24/500\n",
            "359/359 [==============================] - 6s 17ms/step - loss: 0.6667 - val_loss: 0.6651\n",
            "Epoch 25/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6665 - val_loss: 0.6656\n",
            "Epoch 26/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6664 - val_loss: 0.6652\n",
            "Epoch 27/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6663 - val_loss: 0.6654\n",
            "Epoch 28/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6662 - val_loss: 0.6650\n",
            "Epoch 29/500\n",
            "359/359 [==============================] - 5s 15ms/step - loss: 0.6660 - val_loss: 0.6654\n",
            "Epoch 30/500\n",
            "359/359 [==============================] - 6s 17ms/step - loss: 0.6662 - val_loss: 0.6650\n",
            "Epoch 31/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6659 - val_loss: 0.6647\n",
            "Epoch 32/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6658 - val_loss: 0.6656\n",
            "Epoch 33/500\n",
            "359/359 [==============================] - 5s 15ms/step - loss: 0.6659 - val_loss: 0.6646\n",
            "Epoch 34/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6661 - val_loss: 0.6655\n",
            "Epoch 35/500\n",
            "359/359 [==============================] - 6s 17ms/step - loss: 0.6659 - val_loss: 0.6652\n",
            "Epoch 36/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6656 - val_loss: 0.6656\n",
            "Epoch 37/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6658 - val_loss: 0.6646\n",
            "Epoch 38/500\n",
            "359/359 [==============================] - 6s 18ms/step - loss: 0.6657 - val_loss: 0.6655\n",
            "Epoch 39/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6655 - val_loss: 0.6646\n",
            "Epoch 40/500\n",
            "359/359 [==============================] - 7s 19ms/step - loss: 0.6654 - val_loss: 0.6647\n",
            "Epoch 41/500\n",
            "359/359 [==============================] - 6s 15ms/step - loss: 0.6653 - val_loss: 0.6645\n",
            "Epoch 42/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6651 - val_loss: 0.6655\n",
            "Epoch 43/500\n",
            "359/359 [==============================] - 6s 15ms/step - loss: 0.6652 - val_loss: 0.6650\n",
            "Epoch 44/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6653 - val_loss: 0.6645\n",
            "Epoch 45/500\n",
            "359/359 [==============================] - 5s 15ms/step - loss: 0.6652 - val_loss: 0.6645\n",
            "Epoch 46/500\n",
            "359/359 [==============================] - 8s 23ms/step - loss: 0.6651 - val_loss: 0.6643\n",
            "Epoch 47/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6651 - val_loss: 0.6647\n",
            "Epoch 48/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6650 - val_loss: 0.6642\n",
            "Epoch 49/500\n",
            "359/359 [==============================] - 6s 17ms/step - loss: 0.6649 - val_loss: 0.6644\n",
            "Epoch 50/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6648 - val_loss: 0.6647\n",
            "Epoch 51/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6649 - val_loss: 0.6647\n",
            "Epoch 52/500\n",
            "359/359 [==============================] - 7s 18ms/step - loss: 0.6648 - val_loss: 0.6642\n",
            "Epoch 53/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6649 - val_loss: 0.6643\n",
            "Epoch 54/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6648 - val_loss: 0.6649\n",
            "Epoch 55/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6648 - val_loss: 0.6643\n",
            "Epoch 56/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6648 - val_loss: 0.6646\n",
            "Epoch 57/500\n",
            "359/359 [==============================] - 5s 15ms/step - loss: 0.6645 - val_loss: 0.6640\n",
            "Epoch 58/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6643 - val_loss: 0.6648\n",
            "Epoch 59/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6646 - val_loss: 0.6641\n",
            "Epoch 60/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6645 - val_loss: 0.6642\n",
            "Epoch 61/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6643 - val_loss: 0.6644\n",
            "Epoch 62/500\n",
            "359/359 [==============================] - 4s 12ms/step - loss: 0.6643 - val_loss: 0.6645\n",
            "Epoch 63/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6641 - val_loss: 0.6649\n",
            "Epoch 64/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6643 - val_loss: 0.6639\n",
            "Epoch 65/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6642 - val_loss: 0.6642\n",
            "Epoch 66/500\n",
            "359/359 [==============================] - 6s 17ms/step - loss: 0.6642 - val_loss: 0.6647\n",
            "Epoch 67/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6644 - val_loss: 0.6642\n",
            "Epoch 68/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6639 - val_loss: 0.6642\n",
            "Epoch 69/500\n",
            "359/359 [==============================] - 6s 17ms/step - loss: 0.6638 - val_loss: 0.6644\n",
            "Epoch 70/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6637 - val_loss: 0.6642\n",
            "Epoch 71/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6637 - val_loss: 0.6648\n",
            "Epoch 72/500\n",
            "359/359 [==============================] - 6s 17ms/step - loss: 0.6632 - val_loss: 0.6639\n",
            "Epoch 73/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6635 - val_loss: 0.6642\n",
            "Epoch 74/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6634 - val_loss: 0.6651\n",
            "385/385 [==============================] - 2s 4ms/step\n",
            "Predicted Class Labels: [0 1 0 ... 0 1 0]\n",
            "Accuracy: 0.2202439024390244\n",
            "Precision: 0.21919988057919096\n",
            "Recall: 0.2202439024390244\n",
            "F-score: 0.2195184399538536\n",
            "ROC AUC: 0.22024390243902436\n",
            "False Positive Rate (FPR): 0.7492682926829268\n",
            "False Negative Rate (FNR): 0.8102439024390244\n",
            "TN: 1542 FP: 4608 FN: 4983 TP: 1167\n",
            "gen\tnevals\tavg     \tmin     \tmax     \n",
            "0  \t4     \t0.230772\t0.220244\t0.235366\n",
            "Units: 288 Batch Size: 48 Dropout Rate: 0.1\n",
            "Epoch 1/500\n",
            "479/479 [==============================] - 10s 13ms/step - loss: 0.6738 - val_loss: 0.6683\n",
            "Epoch 2/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6691 - val_loss: 0.6680\n",
            "Epoch 3/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6689 - val_loss: 0.6687\n",
            "Epoch 4/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6688 - val_loss: 0.6676\n",
            "Epoch 5/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6688 - val_loss: 0.6681\n",
            "Epoch 6/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6686 - val_loss: 0.6681\n",
            "Epoch 7/500\n",
            "479/479 [==============================] - 7s 16ms/step - loss: 0.6684 - val_loss: 0.6674\n",
            "Epoch 8/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6687 - val_loss: 0.6678\n",
            "Epoch 9/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6683 - val_loss: 0.6673\n",
            "Epoch 10/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6682 - val_loss: 0.6679\n",
            "Epoch 11/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6683 - val_loss: 0.6668\n",
            "Epoch 12/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6680 - val_loss: 0.6665\n",
            "Epoch 13/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6675 - val_loss: 0.6663\n",
            "Epoch 14/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6675 - val_loss: 0.6672\n",
            "Epoch 15/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6671 - val_loss: 0.6660\n",
            "Epoch 16/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6671 - val_loss: 0.6666\n",
            "Epoch 17/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6668 - val_loss: 0.6656\n",
            "Epoch 18/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6669 - val_loss: 0.6657\n",
            "Epoch 19/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6668 - val_loss: 0.6654\n",
            "Epoch 20/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6668 - val_loss: 0.6654\n",
            "Epoch 21/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6667 - val_loss: 0.6650\n",
            "Epoch 22/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6665 - val_loss: 0.6651\n",
            "Epoch 23/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6665 - val_loss: 0.6652\n",
            "Epoch 24/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6664 - val_loss: 0.6650\n",
            "Epoch 25/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6664 - val_loss: 0.6649\n",
            "Epoch 26/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6661 - val_loss: 0.6651\n",
            "Epoch 27/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6664 - val_loss: 0.6650\n",
            "Epoch 28/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6664 - val_loss: 0.6645\n",
            "Epoch 29/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6659 - val_loss: 0.6652\n",
            "Epoch 30/500\n",
            "479/479 [==============================] - 8s 17ms/step - loss: 0.6661 - val_loss: 0.6649\n",
            "Epoch 31/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6659 - val_loss: 0.6655\n",
            "Epoch 32/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6657 - val_loss: 0.6656\n",
            "Epoch 33/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6657 - val_loss: 0.6650\n",
            "Epoch 34/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6658 - val_loss: 0.6652\n",
            "Epoch 35/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6656 - val_loss: 0.6651\n",
            "Epoch 36/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6656 - val_loss: 0.6647\n",
            "Epoch 37/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6656 - val_loss: 0.6647\n",
            "Epoch 38/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6654 - val_loss: 0.6648\n",
            "385/385 [==============================] - 2s 3ms/step\n",
            "Predicted Class Labels: [0 1 0 ... 0 1 1]\n",
            "Accuracy: 0.2348780487804878\n",
            "Precision: 0.23484657879019605\n",
            "Recall: 0.2348780487804878\n",
            "F-score: 0.2348553457971266\n",
            "ROC AUC: 0.23487804878048782\n",
            "False Positive Rate (FPR): 0.7596747967479675\n",
            "False Negative Rate (FNR): 0.770569105691057\n",
            "TN: 1478 FP: 4672 FN: 4739 TP: 1411\n",
            "Units: 288 Batch Size: 48 Dropout Rate: 0.1\n",
            "Epoch 1/500\n",
            "479/479 [==============================] - 11s 16ms/step - loss: 0.6726 - val_loss: 0.6687\n",
            "Epoch 2/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6693 - val_loss: 0.6682\n",
            "Epoch 3/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6689 - val_loss: 0.6679\n",
            "Epoch 4/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6691 - val_loss: 0.6685\n",
            "Epoch 5/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6687 - val_loss: 0.6676\n",
            "Epoch 6/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6688 - val_loss: 0.6673\n",
            "Epoch 7/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6688 - val_loss: 0.6676\n",
            "Epoch 8/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6686 - val_loss: 0.6672\n",
            "Epoch 9/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6681 - val_loss: 0.6667\n",
            "Epoch 10/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6680 - val_loss: 0.6671\n",
            "Epoch 11/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6678 - val_loss: 0.6664\n",
            "Epoch 12/500\n",
            "479/479 [==============================] - 8s 17ms/step - loss: 0.6675 - val_loss: 0.6668\n",
            "Epoch 13/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6674 - val_loss: 0.6664\n",
            "Epoch 14/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6673 - val_loss: 0.6659\n",
            "Epoch 15/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6672 - val_loss: 0.6660\n",
            "Epoch 16/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6670 - val_loss: 0.6652\n",
            "Epoch 17/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6668 - val_loss: 0.6651\n",
            "Epoch 18/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6670 - val_loss: 0.6653\n",
            "Epoch 19/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6669 - val_loss: 0.6653\n",
            "Epoch 20/500\n",
            "479/479 [==============================] - 8s 18ms/step - loss: 0.6668 - val_loss: 0.6676\n",
            "Epoch 21/500\n",
            "479/479 [==============================] - 9s 19ms/step - loss: 0.6669 - val_loss: 0.6654\n",
            "Epoch 22/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6667 - val_loss: 0.6651\n",
            "Epoch 23/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6665 - val_loss: 0.6658\n",
            "Epoch 24/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6662 - val_loss: 0.6656\n",
            "Epoch 25/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6661 - val_loss: 0.6663\n",
            "Epoch 26/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6663 - val_loss: 0.6652\n",
            "Epoch 27/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6661 - val_loss: 0.6650\n",
            "Epoch 28/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6660 - val_loss: 0.6651\n",
            "Epoch 29/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6663 - val_loss: 0.6657\n",
            "Epoch 30/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6658 - val_loss: 0.6653\n",
            "Epoch 31/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6659 - val_loss: 0.6648\n",
            "Epoch 32/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6658 - val_loss: 0.6658\n",
            "Epoch 33/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6657 - val_loss: 0.6647\n",
            "Epoch 34/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6658 - val_loss: 0.6657\n",
            "Epoch 35/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6654 - val_loss: 0.6652\n",
            "Epoch 36/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6656 - val_loss: 0.6661\n",
            "Epoch 37/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6655 - val_loss: 0.6648\n",
            "Epoch 38/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6652 - val_loss: 0.6645\n",
            "Epoch 39/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6654 - val_loss: 0.6646\n",
            "Epoch 40/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6655 - val_loss: 0.6656\n",
            "Epoch 41/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6653 - val_loss: 0.6645\n",
            "Epoch 42/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6652 - val_loss: 0.6650\n",
            "Epoch 43/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6655 - val_loss: 0.6651\n",
            "Epoch 44/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6653 - val_loss: 0.6655\n",
            "Epoch 45/500\n",
            "479/479 [==============================] - 6s 14ms/step - loss: 0.6652 - val_loss: 0.6656\n",
            "Epoch 46/500\n",
            "479/479 [==============================] - 7s 16ms/step - loss: 0.6652 - val_loss: 0.6652\n",
            "Epoch 47/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6654 - val_loss: 0.6649\n",
            "Epoch 48/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6654 - val_loss: 0.6649\n",
            "Epoch 49/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6651 - val_loss: 0.6661\n",
            "Epoch 50/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6652 - val_loss: 0.6650\n",
            "Epoch 51/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6653 - val_loss: 0.6675\n",
            "385/385 [==============================] - 2s 3ms/step\n",
            "Predicted Class Labels: [0 1 0 ... 0 1 0]\n",
            "Accuracy: 0.24357723577235774\n",
            "Precision: 0.23757630365608157\n",
            "Recall: 0.24357723577235774\n",
            "F-score: 0.2392280277693018\n",
            "ROC AUC: 0.24357723577235774\n",
            "False Positive Rate (FPR): 0.6808130081300813\n",
            "False Negative Rate (FNR): 0.8320325203252033\n",
            "TN: 1963 FP: 4187 FN: 5117 TP: 1033\n",
            "Units: 288 Batch Size: 160 Dropout Rate: 0.1\n",
            "Epoch 1/500\n",
            "144/144 [==============================] - 6s 26ms/step - loss: 0.6772 - val_loss: 0.6702\n",
            "Epoch 2/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6692 - val_loss: 0.6681\n",
            "Epoch 3/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6689 - val_loss: 0.6678\n",
            "Epoch 4/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6686 - val_loss: 0.6678\n",
            "Epoch 5/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6685 - val_loss: 0.6678\n",
            "Epoch 6/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6685 - val_loss: 0.6677\n",
            "Epoch 7/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6685 - val_loss: 0.6676\n",
            "Epoch 8/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6684 - val_loss: 0.6676\n",
            "Epoch 9/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6684 - val_loss: 0.6676\n",
            "Epoch 10/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6683 - val_loss: 0.6675\n",
            "Epoch 11/500\n",
            "144/144 [==============================] - 4s 30ms/step - loss: 0.6686 - val_loss: 0.6684\n",
            "Epoch 12/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6682 - val_loss: 0.6676\n",
            "Epoch 13/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6684 - val_loss: 0.6681\n",
            "Epoch 14/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6683 - val_loss: 0.6673\n",
            "Epoch 15/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6682 - val_loss: 0.6676\n",
            "Epoch 16/500\n",
            "144/144 [==============================] - 4s 31ms/step - loss: 0.6681 - val_loss: 0.6672\n",
            "Epoch 17/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6680 - val_loss: 0.6671\n",
            "Epoch 18/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6679 - val_loss: 0.6670\n",
            "Epoch 19/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6676 - val_loss: 0.6669\n",
            "Epoch 20/500\n",
            "144/144 [==============================] - 3s 24ms/step - loss: 0.6677 - val_loss: 0.6664\n",
            "Epoch 21/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6674 - val_loss: 0.6668\n",
            "Epoch 22/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6673 - val_loss: 0.6659\n",
            "Epoch 23/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6672 - val_loss: 0.6665\n",
            "Epoch 24/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6672 - val_loss: 0.6658\n",
            "Epoch 25/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6667 - val_loss: 0.6659\n",
            "Epoch 26/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6667 - val_loss: 0.6653\n",
            "Epoch 27/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6664 - val_loss: 0.6654\n",
            "Epoch 28/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6665 - val_loss: 0.6655\n",
            "Epoch 29/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6665 - val_loss: 0.6651\n",
            "Epoch 30/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6664 - val_loss: 0.6655\n",
            "Epoch 31/500\n",
            "144/144 [==============================] - 3s 24ms/step - loss: 0.6664 - val_loss: 0.6653\n",
            "Epoch 32/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6662 - val_loss: 0.6649\n",
            "Epoch 33/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6663 - val_loss: 0.6653\n",
            "Epoch 34/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6662 - val_loss: 0.6653\n",
            "Epoch 35/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6661 - val_loss: 0.6658\n",
            "Epoch 36/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6663 - val_loss: 0.6649\n",
            "Epoch 37/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6660 - val_loss: 0.6649\n",
            "Epoch 38/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6659 - val_loss: 0.6652\n",
            "Epoch 39/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6660 - val_loss: 0.6652\n",
            "Epoch 40/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6660 - val_loss: 0.6647\n",
            "Epoch 41/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6659 - val_loss: 0.6649\n",
            "Epoch 42/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6657 - val_loss: 0.6651\n",
            "Epoch 43/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6657 - val_loss: 0.6648\n",
            "Epoch 44/500\n",
            "144/144 [==============================] - 3s 23ms/step - loss: 0.6657 - val_loss: 0.6650\n",
            "Epoch 45/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6656 - val_loss: 0.6650\n",
            "Epoch 46/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6655 - val_loss: 0.6650\n",
            "Epoch 47/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6655 - val_loss: 0.6646\n",
            "Epoch 48/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6656 - val_loss: 0.6656\n",
            "Epoch 49/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6655 - val_loss: 0.6646\n",
            "Epoch 50/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6653 - val_loss: 0.6652\n",
            "Epoch 51/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6654 - val_loss: 0.6664\n",
            "Epoch 52/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6654 - val_loss: 0.6646\n",
            "Epoch 53/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6653 - val_loss: 0.6649\n",
            "Epoch 54/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6652 - val_loss: 0.6655\n",
            "Epoch 55/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6655 - val_loss: 0.6647\n",
            "Epoch 56/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6654 - val_loss: 0.6647\n",
            "Epoch 57/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6650 - val_loss: 0.6652\n",
            "385/385 [==============================] - 2s 3ms/step\n",
            "Predicted Class Labels: [0 1 0 ... 0 1 1]\n",
            "Accuracy: 0.23772357723577237\n",
            "Precision: 0.23676044304714486\n",
            "Recall: 0.23772357723577237\n",
            "F-score: 0.2370256894904782\n",
            "ROC AUC: 0.23772357723577237\n",
            "False Positive Rate (FPR): 0.7320325203252033\n",
            "False Negative Rate (FNR): 0.792520325203252\n",
            "TN: 1648 FP: 4502 FN: 4874 TP: 1276\n",
            "Units: 288 Batch Size: 160 Dropout Rate: 0.1\n",
            "Epoch 1/500\n",
            "144/144 [==============================] - 8s 33ms/step - loss: 0.6781 - val_loss: 0.6692\n",
            "Epoch 2/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6692 - val_loss: 0.6684\n",
            "Epoch 3/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6687 - val_loss: 0.6680\n",
            "Epoch 4/500\n",
            "144/144 [==============================] - 4s 30ms/step - loss: 0.6686 - val_loss: 0.6679\n",
            "Epoch 5/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6686 - val_loss: 0.6678\n",
            "Epoch 6/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6686 - val_loss: 0.6678\n",
            "Epoch 7/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6685 - val_loss: 0.6678\n",
            "Epoch 8/500\n",
            "144/144 [==============================] - 3s 24ms/step - loss: 0.6684 - val_loss: 0.6683\n",
            "Epoch 9/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6685 - val_loss: 0.6675\n",
            "Epoch 10/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6685 - val_loss: 0.6676\n",
            "Epoch 11/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6682 - val_loss: 0.6678\n",
            "Epoch 12/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6679 - val_loss: 0.6667\n",
            "Epoch 13/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6676 - val_loss: 0.6672\n",
            "Epoch 14/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6674 - val_loss: 0.6665\n",
            "Epoch 15/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6672 - val_loss: 0.6659\n",
            "Epoch 16/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6674 - val_loss: 0.6658\n",
            "Epoch 17/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6672 - val_loss: 0.6665\n",
            "Epoch 18/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6669 - val_loss: 0.6661\n",
            "Epoch 19/500\n",
            "144/144 [==============================] - 3s 23ms/step - loss: 0.6672 - val_loss: 0.6655\n",
            "Epoch 20/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6670 - val_loss: 0.6655\n",
            "Epoch 21/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6667 - val_loss: 0.6666\n",
            "Epoch 22/500\n",
            "144/144 [==============================] - 3s 23ms/step - loss: 0.6669 - val_loss: 0.6654\n",
            "Epoch 23/500\n",
            "144/144 [==============================] - 5s 32ms/step - loss: 0.6669 - val_loss: 0.6665\n",
            "Epoch 24/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6669 - val_loss: 0.6653\n",
            "Epoch 25/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6667 - val_loss: 0.6652\n",
            "Epoch 26/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6667 - val_loss: 0.6658\n",
            "Epoch 27/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6668 - val_loss: 0.6659\n",
            "Epoch 28/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6663 - val_loss: 0.6659\n",
            "Epoch 29/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6666 - val_loss: 0.6654\n",
            "Epoch 30/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6666 - val_loss: 0.6650\n",
            "Epoch 31/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6665 - val_loss: 0.6657\n",
            "Epoch 32/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6664 - val_loss: 0.6656\n",
            "Epoch 33/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6661 - val_loss: 0.6659\n",
            "Epoch 34/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6664 - val_loss: 0.6649\n",
            "Epoch 35/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6664 - val_loss: 0.6653\n",
            "Epoch 36/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6663 - val_loss: 0.6654\n",
            "Epoch 37/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6661 - val_loss: 0.6648\n",
            "Epoch 38/500\n",
            "144/144 [==============================] - 3s 24ms/step - loss: 0.6661 - val_loss: 0.6653\n",
            "Epoch 39/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6661 - val_loss: 0.6650\n",
            "Epoch 40/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6658 - val_loss: 0.6649\n",
            "Epoch 41/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6658 - val_loss: 0.6652\n",
            "Epoch 42/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6659 - val_loss: 0.6649\n",
            "Epoch 43/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6657 - val_loss: 0.6646\n",
            "Epoch 44/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6656 - val_loss: 0.6654\n",
            "Epoch 45/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6658 - val_loss: 0.6650\n",
            "Epoch 46/500\n",
            "144/144 [==============================] - 3s 23ms/step - loss: 0.6655 - val_loss: 0.6658\n",
            "Epoch 47/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6657 - val_loss: 0.6650\n",
            "Epoch 48/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6656 - val_loss: 0.6653\n",
            "Epoch 49/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6656 - val_loss: 0.6646\n",
            "Epoch 50/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6653 - val_loss: 0.6648\n",
            "Epoch 51/500\n",
            "144/144 [==============================] - 4s 24ms/step - loss: 0.6653 - val_loss: 0.6648\n",
            "Epoch 52/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6653 - val_loss: 0.6645\n",
            "Epoch 53/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6654 - val_loss: 0.6651\n",
            "Epoch 54/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6653 - val_loss: 0.6650\n",
            "Epoch 55/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6651 - val_loss: 0.6647\n",
            "Epoch 56/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6651 - val_loss: 0.6648\n",
            "Epoch 57/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6652 - val_loss: 0.6646\n",
            "Epoch 58/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6650 - val_loss: 0.6645\n",
            "Epoch 59/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6651 - val_loss: 0.6648\n",
            "Epoch 60/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6651 - val_loss: 0.6646\n",
            "Epoch 61/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6651 - val_loss: 0.6646\n",
            "Epoch 62/500\n",
            "144/144 [==============================] - 3s 24ms/step - loss: 0.6651 - val_loss: 0.6646\n",
            "385/385 [==============================] - 2s 4ms/step\n",
            "Predicted Class Labels: [0 1 0 ... 0 1 1]\n",
            "Accuracy: 0.23414634146341465\n",
            "Precision: 0.2335456682625087\n",
            "Recall: 0.23414634146341465\n",
            "F-score: 0.23371447836131273\n",
            "ROC AUC: 0.23414634146341465\n",
            "False Positive Rate (FPR): 0.7895934959349593\n",
            "False Negative Rate (FNR): 0.7421138211382113\n",
            "TN: 1294 FP: 4856 FN: 4564 TP: 1586\n",
            "1  \t4     \t0.237581\t0.234146\t0.243577\n",
            "Units: 288 Batch Size: 160 Dropout Rate: 0.1\n",
            "Epoch 1/500\n",
            "144/144 [==============================] - 7s 24ms/step - loss: 0.6758 - val_loss: 0.6692\n",
            "Epoch 2/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6693 - val_loss: 0.6682\n",
            "Epoch 3/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6688 - val_loss: 0.6681\n",
            "Epoch 4/500\n",
            "144/144 [==============================] - 5s 33ms/step - loss: 0.6688 - val_loss: 0.6678\n",
            "Epoch 5/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6685 - val_loss: 0.6677\n",
            "Epoch 6/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6686 - val_loss: 0.6681\n",
            "Epoch 7/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6687 - val_loss: 0.6680\n",
            "Epoch 8/500\n",
            "144/144 [==============================] - 3s 24ms/step - loss: 0.6684 - val_loss: 0.6677\n",
            "Epoch 9/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6684 - val_loss: 0.6675\n",
            "Epoch 10/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6683 - val_loss: 0.6669\n",
            "Epoch 11/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6679 - val_loss: 0.6667\n",
            "Epoch 12/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6676 - val_loss: 0.6667\n",
            "Epoch 13/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6674 - val_loss: 0.6676\n",
            "Epoch 14/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6677 - val_loss: 0.6658\n",
            "Epoch 15/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6674 - val_loss: 0.6656\n",
            "Epoch 16/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6673 - val_loss: 0.6666\n",
            "Epoch 17/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6672 - val_loss: 0.6655\n",
            "Epoch 18/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6669 - val_loss: 0.6654\n",
            "Epoch 19/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6669 - val_loss: 0.6655\n",
            "Epoch 20/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6671 - val_loss: 0.6654\n",
            "Epoch 21/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6668 - val_loss: 0.6651\n",
            "Epoch 22/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6668 - val_loss: 0.6662\n",
            "Epoch 23/500\n",
            "144/144 [==============================] - 5s 32ms/step - loss: 0.6669 - val_loss: 0.6653\n",
            "Epoch 24/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6668 - val_loss: 0.6651\n",
            "Epoch 25/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6668 - val_loss: 0.6653\n",
            "Epoch 26/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6665 - val_loss: 0.6659\n",
            "Epoch 27/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6671 - val_loss: 0.6650\n",
            "Epoch 28/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6665 - val_loss: 0.6649\n",
            "Epoch 29/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6664 - val_loss: 0.6653\n",
            "Epoch 30/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6665 - val_loss: 0.6660\n",
            "Epoch 31/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6664 - val_loss: 0.6656\n",
            "Epoch 32/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6665 - val_loss: 0.6651\n",
            "Epoch 33/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6663 - val_loss: 0.6662\n",
            "Epoch 34/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6662 - val_loss: 0.6649\n",
            "Epoch 35/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6662 - val_loss: 0.6652\n",
            "Epoch 36/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6662 - val_loss: 0.6653\n",
            "Epoch 37/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6660 - val_loss: 0.6650\n",
            "Epoch 38/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6662 - val_loss: 0.6647\n",
            "Epoch 39/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6659 - val_loss: 0.6647\n",
            "Epoch 40/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6660 - val_loss: 0.6657\n",
            "Epoch 41/500\n",
            "144/144 [==============================] - 3s 24ms/step - loss: 0.6660 - val_loss: 0.6645\n",
            "Epoch 42/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6657 - val_loss: 0.6650\n",
            "Epoch 43/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6656 - val_loss: 0.6649\n",
            "Epoch 44/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6657 - val_loss: 0.6651\n",
            "Epoch 45/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6655 - val_loss: 0.6645\n",
            "Epoch 46/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6656 - val_loss: 0.6646\n",
            "Epoch 47/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6653 - val_loss: 0.6653\n",
            "Epoch 48/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6654 - val_loss: 0.6645\n",
            "Epoch 49/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6653 - val_loss: 0.6649\n",
            "Epoch 50/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6653 - val_loss: 0.6647\n",
            "Epoch 51/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6651 - val_loss: 0.6648\n",
            "Epoch 52/500\n",
            "144/144 [==============================] - 4s 31ms/step - loss: 0.6652 - val_loss: 0.6646\n",
            "Epoch 53/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6651 - val_loss: 0.6647\n",
            "Epoch 54/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6652 - val_loss: 0.6647\n",
            "Epoch 55/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6652 - val_loss: 0.6646\n",
            "Epoch 56/500\n",
            "119/144 [=======================>......] - ETA: 0s - loss: 0.6649"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"#Google Usgae Data\"\"\"\n",
        "\n",
        "# merged = pd.read_csv('/content/drive/My Drive/sine3.csv',  header=None)\n",
        "# merged = pd.read_csv('/content/drive/My Drive/hyperplane1.csv', header=None)\n",
        "merged = pd.read_csv('/content/drive/My Drive/mixed5.csv', header=None)\n",
        "# merged = pd.read_csv('/content/drive/My Drive/stagger4.csv', header=None)\n",
        "\n",
        "\n",
        "#dataframe.head()\n",
        "#set n one less\n",
        "n=4\n",
        "merged=pd.DataFrame(data=merged)\n",
        "merged.fillna(merged.mean())\n",
        "# merged =merged[~merged.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
        "merged = merged[~merged.isin([np.nan, np.inf, -np.inf]).any(axis=1)]\n",
        "\n",
        "merged=pd.DataFrame(data=merged)\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Normalize all columns of the merged dataframe\n",
        "merged_normalized = scaler.fit_transform(merged.values)\n",
        "\n",
        "# Create a new dataframe with the normalized values\n",
        "merged = pd.DataFrame(merged_normalized, columns=merged.columns)\n",
        "\n",
        "\n",
        "# data = merged\n",
        "# plt.figure(figsize=(16, 6))\n",
        "# # Store heatmap object in a variable to easily access it when you want to include more features (such as title).\n",
        "# # Set the range of values to be displayed on the colormap from -1 to 1, and set the annotation to True to display the correlation values on the heatmap.\n",
        "# heatmap = sb.heatmap(data.corr(), vmin=-1, vmax=1, annot=True)\n",
        "# # Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.\n",
        "# heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);\n",
        "# # displaying heatmap\n",
        "# mp.show()\n",
        "merged=np.array(merged)\n",
        "y =merged[:,n] #5\n",
        "#  z=merged[:,6]\n",
        "merged= np.delete(merged,[n], axis=1)\n",
        "\n",
        "#56\n",
        "#merged=np.delete(merged,[0,1,2,3x,4,7,8,9,10,11,12,13,14,15,16,17,18], axis=1)\n",
        "#4-14\n",
        "# merged=np.delete(merged,[0,1,2,3,4,15,16,17,18], axis=1)\n",
        "#5-10\n",
        "#  merged=np.delete(merged,[0,1,2,3,4,11,12,13,15,16,17,18], axis=1)\n",
        "\n",
        "\"\"\"#Preprocessing AELSTM \"\"\"\n",
        "\n",
        "labels =y#np.array(y11)# raw_data[:, -1]\n",
        "data = merged\n",
        "\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "data, labels, test_size=0.3, shuffle=False)\n",
        "num_ones = np.count_nonzero(labels)\n",
        "num_zeros = len(labels) - num_ones\n",
        "print(f\"Number of ones: {num_ones}\")\n",
        "print(f\"Number of zeros: {num_zeros}\")\n",
        "# Convert the data to numpy arrays\n",
        "X_train = np.array(train_data)\n",
        "X_test = np.array(test_data)\n",
        "train_labels= np.array(train_labels)\n",
        "# Reshape the data to 3D arrays\n",
        "X_train = np.reshape(X_train, (X_train.shape[0],1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "test_labels=np.reshape(test_labels,(test_labels.shape[0], 1))\n",
        "train_labels=np.reshape(train_labels,(train_labels.shape[0], 1))\n",
        "\n",
        "\"\"\"#Non- Gaussian Code  #LSTM AT HT\"\"\"\n",
        "\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(train_labels)\n",
        "X_test  = np.array(X_test)\n",
        "y_test  = np.array(test_labels)\n",
        "\n",
        "\"\"\"#LSTM AT HY GA\"\"\"\n",
        "\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
        "\n",
        "X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(individual):\n",
        "    # Extract the hyperparameters from the individual\n",
        "    units = individual[0]\n",
        "    batch_size = individual[1]\n",
        "    dropout_rate = individual[2]\n",
        "    if units == 0:\n",
        "        units = 32\n",
        "    if batch_size==0:\n",
        "     batch_size=1\n",
        "    if dropout_rate==0:\n",
        "      dropout_rate=0.1\n",
        "    print(\"Units:\", units, \"Batch Size:\", batch_size, \"Dropout Rate:\", dropout_rate)\n",
        "\n",
        "    # Build the LSTM model with the specified hyperparameters\n",
        "    inputs = Input(shape=(None, n))\n",
        "    lstm_out, state_h, state_c = LSTM(units, return_sequences=True, return_state=True)(inputs)\n",
        "    attention_out = Attention()([lstm_out, lstm_out])\n",
        "\n",
        "    dense_out = Dense(256, activation='relu')(attention_out)\n",
        "    dense_out = Dense(64, activation='relu')(dense_out)\n",
        "    dense_out = Dense(32, activation='relu')(attention_out)\n",
        "    dense_out = Dense(16, activation='relu')(dense_out)\n",
        "    dense_out = Dense(8, activation='relu')(attention_out)\n",
        "    outputs = Dense(1, activation='sigmoid')(dense_out)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train1, y_train1, epochs=500, batch_size=batch_size, shuffle=True,\n",
        "                        validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "    # Make predictions on test data\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_binary = np.round(y_pred).flatten()\n",
        "\n",
        "    # Convert predictions to class labels\n",
        "    y_pred_classes = np.where(y_pred > 0.5, 1, 0)\n",
        "    y_pred_classes = np.squeeze(y_pred_classes)\n",
        "\n",
        "    # Print the predicted class labels\n",
        "    print(\"Predicted Class Labels:\", y_pred_classes)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "    precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred_classes, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_binary)\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F-score:\", f1)\n",
        "    print(\"ROC AUC:\", roc_auc)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_classes).ravel()\n",
        "    fpr = fp / (fp + tn)\n",
        "    fnr = fn / (fn + tp)\n",
        "\n",
        "    print(\"False Positive Rate (FPR):\", fpr)\n",
        "    print(\"False Negative Rate (FNR):\", fnr)\n",
        "    print(\"TN:\", tn, \"FP:\", fp, \"FN:\", fn, \"TP:\", tp)\n",
        "\n",
        "    # Return the fitness value (e.g., accuracy) as a tuple\n",
        "    return accuracy,\n",
        "# Define the individual and population size\n",
        "INDIVIDUAL_SIZE = 3\n",
        "POPULATION_SIZE = 4\n",
        "\n",
        "# Create the individual and population classes\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "# Create the toolbox\n",
        "toolbox = base.Toolbox()\n",
        "\n",
        "# Define the hyperparameter ranges\n",
        "units_range = [32,64, 128,256]\n",
        "batch_size_range = [10,16, 32, 64,128]\n",
        "dropout_rate_range = [0.01,0.1, 0.2, 0.3]\n",
        "# Register the hyperparameters in the toolbox\n",
        "# Define the custom initialization function for units\n",
        "# def init_units():\n",
        "#     return np.random.choice(units_range)\n",
        "def init_units():\n",
        "    return int(np.random.choice(units_range)) + 32\n",
        "\n",
        "# Register the units attribute with the custom initialization function\n",
        "toolbox.register(\"units\", init_units)\n",
        "\n",
        "# toolbox.register(\"units\", np.random.choice, units_range)\n",
        "def init_batch_size_range():\n",
        "    return int(np.random.choice(batch_size_range)) + 32\n",
        "\n",
        "toolbox.register(\"batch_size\", init_batch_size_range)\n",
        "def init_dropout_rate_range():\n",
        "    return int(np.random.choice(dropout_rate_range)) + 0.1\n",
        "toolbox.register(\"dropout_rate\", init_dropout_rate_range)\n",
        "\n",
        "# # Define the individual initialization function\n",
        "# toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
        "#                  (toolbox.units, toolbox.batch_size, toolbox.dropout_rate), n=1)\n",
        "# Define the individual initialization function\n",
        "def init_individual():\n",
        "    return creator.Individual([toolbox.units(), toolbox.batch_size(), toolbox.dropout_rate()])\n",
        "\n",
        "# Register the individual initialization function\n",
        "toolbox.register(\"individual\", init_individual)\n",
        "# Define the population initialization function\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "# Register the evaluation function\n",
        "toolbox.register(\"evaluate\", evaluate_model)\n",
        "\n",
        "# Register the selection, crossover, and mutation operations\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutUniformInt, low=0, up=2, indpb=0.5)\n",
        "\n",
        "# Define the number of generations and probability of crossover/mutation\n",
        "N_GENERATIONS = 20\n",
        "CROSSOVER_PROB = 0.8\n",
        "MUTATION_PROB = 0.2\n",
        "\n",
        "# Create the population\n",
        "population = toolbox.population(n=POPULATION_SIZE)\n",
        "\n",
        "# Create the hall of fame to store the best individuals\n",
        "hof = tools.HallOfFame(maxsize=1)\n",
        "\n",
        "# Define the statistics to collect during evolution\n",
        "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "stats.register(\"avg\", np.mean)\n",
        "stats.register(\"min\", np.min)\n",
        "stats.register(\"max\", np.max)\n",
        "\n",
        "# Run the evolution\n",
        "population, logbook = algorithms.eaSimple(\n",
        "    population,\n",
        "    toolbox,\n",
        "    cxpb=CROSSOVER_PROB,\n",
        "    mutpb=MUTATION_PROB,\n",
        "    ngen=N_GENERATIONS,\n",
        "    stats=stats,\n",
        "    halloffame=hof,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Get the best individual from the hall of fame\n",
        "best_individual = hof[0]\n",
        "\n",
        "# Print the best individual and its fitness value\n",
        "print(\"Best Individual:\", best_individual)\n",
        "print(\"Fitness Value:\", best_individual.fitness.values[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uajrkrt9NQik"
      },
      "source": [
        "#Google Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfVnbzU6jtMV"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "#LSTMDD"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#epochs=1000\n",
        "#batchsize=32\n",
        "nooffile=2\n",
        "n=10\n",
        "\n",
        "\n",
        "\"\"\"#Google Usgae Data\"\"\"\n",
        "\n",
        "n=5\n",
        " # Download the dataset\n",
        "merged = pd.read_csv('/content/drive/My Drive/part-00001-of-00500.csv', header=None)\n",
        "#  merged = pd.read_csv('/content/drive/My Drive/mixed_0101_gradual.csv', header='infer')\n",
        "#  merged = pd.read_csv('/content/drive/My Drive/sea4.csv', header='infer')\n",
        " #dataframe.head()\n",
        "# merged = merged.head(500)\n",
        "\n",
        "i=1\n",
        "# while i<nooffile:\n",
        "#   a=pd.read_csv(\"/content/drive/My Drive/part-0000\"+str(i)+\"-of-00500.csv\")\n",
        "#   merged=np.concatenate([merged, a])\n",
        "#   a=0\n",
        "#   i=i+1\n",
        "# del a\n",
        "merged=pd.DataFrame(data=merged)\n",
        "merged.fillna(merged.mean())\n",
        "merged =merged[~merged.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
        "# merged = merged.sample(n=50000, random_state=42)\n",
        "\n",
        "\n",
        "#  merged = merged.sample(n=50000, random_state=42)\n",
        "merged=np.array(merged)\n",
        "y =merged[:,n] #5\n",
        "z=merged[:,6]\n",
        "#  merged= np.delete(merged,[n], axis=1)\n",
        "#56\n",
        "#merged=np.delete(merged,[0,1,2,3,4,7,8,9,10,11,12,13,14,15,16,17,18], axis=1)\n",
        "#4-14\n",
        "merged=np.delete(merged,[0,1,2,3,4,15,16,17,18], axis=1)\n",
        "#5-10\n",
        "#  merged=np.delete(merged,[0,1,2,3,4,11,12,13,15,16,17,18], axis=1)\n",
        "#print(merged)\n",
        "def min_max_scaling(df):\n",
        "    # copy the dataframe\n",
        "    df_norm = df\n",
        "    # apply min-max scaling\n",
        "   # for column in df_norm.columns:\n",
        "    df_norm = ((df_norm - 0) / (1 - 0))*((1-0)+0)\n",
        "    #print(\"number \"+str(df)+\"converted\"+str(df_norm))\n",
        "    return df_norm\n",
        "y1 = DynamicArray()\n",
        "y11 = DynamicArray()\n",
        "z1 = DynamicArray()\n",
        "z11 = DynamicArray()\n",
        "c=0\n",
        "c1=0\n",
        "c2=0\n",
        "c3=0\n",
        "c4=0\n",
        "c5=0\n",
        "y=np.array(y)\n",
        "for a in y:\n",
        " y11.append(min_max_scaling(a))\n",
        "for a in y11:\n",
        " if  0.05> a>= 0:\n",
        "  y1.append(0)\n",
        "  c1+=1\n",
        " elif  0.125> a>= 0.05:\n",
        "  y1.append(1)\n",
        "  c2+=1\n",
        " elif 1>= a>= 0.125:\n",
        "  y1.append(2)\n",
        "  #print(a)\n",
        "  c3+=1\n",
        " else:\n",
        "  y1.append(0)\n",
        "  c+=1\n",
        "y11 = []\n",
        "for a in range(len(y1)):\n",
        " if (y1[a]-y1[a-1]>=2) :\n",
        "  #print(abs(y1[a]-y1[a-1]))\n",
        "  y11.append(1)\n",
        " else:\n",
        "  #print(abs(y1[a]-y1[a-1]))\n",
        "  y11.append(0)\n",
        "  #index=index+1\n",
        "\n",
        "#del y1\n",
        "print(\"c=\"+str(c)+\"c1=\"+str(c1)+\"c2=\"+str(c2)+\"c3=\"+str(c3)+\"c4=\"+str(c4)+\"c5=\"+str(c5))\n",
        "c3=0\n",
        "c2=0\n",
        "for q in y11:\n",
        " if q==1:\n",
        "  c3=c3+1\n",
        " else:\n",
        "  c2=c2+1\n",
        "print(c3)\n",
        "labels =y11#np.array(y11)# raw_data[:, -1]\n",
        "data = merged\n",
        "\n",
        "# # # The other dat\n",
        "# # from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# # ros = RandomOverSampler(random_state=42)\n",
        "# # data,labels = ros.fit_resample(data1,labels1)\n",
        "\n",
        "# train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "#     data, labels, test_size=0.3, shuffle=False)\n",
        "# num_ones = np.count_nonzero(labels)\n",
        "# num_zeros = len(labels) - num_ones\n",
        "# print(f\"Number of ones: {num_ones}\")\n",
        "# print(f\"Number of zeros: {num_zeros}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WriT8EfIDOAH",
        "outputId": "be9fe331-0a40-426c-b132-b0a56e5835cf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-1b1cb8161d11>:26: FutureWarning: In a future version of pandas all arguments of DataFrame.any and Series.any will be keyword-only.\n",
            "  merged =merged[~merged.isin([np.nan, np.inf, -np.inf]).any(1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c=0c1=1779122c2=185961c3=34267c4=0c5=0\n",
            "5067\n",
            "Number of ones: 5067\n",
            "Number of zeros: 1994283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Assuming you already have 'labels' and 'data' arrays\n",
        "labels = np.array(y11)  # Ensure labels are integers\n",
        "data = merged\n",
        "\n",
        "# Step 2: Identify unique classes\n",
        "unique_classes = np.unique(labels)\n",
        "\n",
        "# Step 3: Determine the minority class size\n",
        "minority_class_size = min(np.bincount(labels))\n",
        "\n",
        "# Initialize arrays to store balanced data and labels\n",
        "balanced_data = []\n",
        "balanced_labels = []\n",
        "\n",
        "# Step 4: Undersample the majority class for each unique class\n",
        "for class_label in unique_classes:\n",
        "    class_data = data[labels == class_label]\n",
        "    undersampled_class_data = resample(class_data,\n",
        "                                       replace=False,\n",
        "                                       n_samples=minority_class_size,\n",
        "                                       random_state=42)\n",
        "\n",
        "    # Append undersampled data and labels to balanced arrays\n",
        "    balanced_data.extend(undersampled_class_data)\n",
        "    balanced_labels.extend([class_label] * minority_class_size)\n",
        "\n",
        "    print(f\"Class {class_label}: Number of Records after Undersampling = {minority_class_size}\")\n",
        "\n",
        "# Convert balanced_data and balanced_labels to numpy arrays\n",
        "balanced_data = np.array(balanced_data)\n",
        "balanced_labels = np.array(balanced_labels)\n",
        "\n",
        "# Combine the undersampled data and labels\n",
        "balanced_data_with_labels = np.column_stack((balanced_data, balanced_labels))\n",
        "\n",
        "# Step 5: Shuffle data\n",
        "np.random.shuffle(balanced_data_with_labels)\n",
        "\n",
        "# Step 6: Separate labels and features for the balanced data\n",
        "balanced_labels = balanced_data_with_labels[:, -1].astype(int)  # Convert labels to integers\n",
        "balanced_features = balanced_data_with_labels[:, :-1]\n",
        "\n",
        "# Verify that the label class is changed correctly\n",
        "print(\"Balanced Labels:\", balanced_labels)\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(balanced_features, balanced_labels, test_size=0.40, shuffle=False)\n",
        "# num_onesa = np.count_nonzero(test_labels)\n",
        "# num_zeros = len(test_labels) - num_ones\n",
        "# print(f\"Number of ones: {num_ones}\")\n",
        "# print(f\"Number of zeros: {num_zeros}\")\n",
        "# Convert the data to numpy arrays\n",
        "X_train = np.array(train_data)\n",
        "X_test = np.array(test_data)\n",
        "train_labels= np.array(train_labels)\n",
        "test_labels= np.array(test_labels)\n",
        "\n",
        "# Reshape the data to 3D arrays\n",
        "X_train = np.reshape(X_train, (X_train.shape[0],1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "test_labels=np.reshape(test_labels,(test_labels.shape[0], 1))\n",
        "train_labels=np.reshape(train_labels,(train_labels.shape[0], 1))\n",
        "\"\"#Non- Gaussian Code  #LSTM AT HT\"\"\"\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(train_labels)\n",
        "X_test  = np.array(X_test)\n",
        "y_test  = np.array(test_labels)\n",
        "# train_labels = train_labels.astype(bool)\n",
        "# test_labels = test_labels.astype(bool)\n",
        "# normal_train_data=train_data\n",
        "# normal_train_data = train_data[~train_labels]\n",
        "# normal_test_data = test_data[~test_labels]\n",
        "\n",
        "# anomalous_train_data = train_data[train_labels]\n",
        "# anomalous_test_data = test_data[test_labels]\n",
        "import keras.backend as K\n",
        "# Convert the data to numpy arrays\n",
        "# X_train = np.array(normal_train_data)\n",
        "# X_test = np.array(test_data)\n",
        "# train_labels= np.array(train_labels)\n",
        "\n",
        "# Reshape the data to 3D arrays\n",
        "# X_train = np.reshape(X_train, (X_train.shape[0],1, X_train.shape[1]))\n",
        "# X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "# train_labels=np.reshape(train_labels,(train_labels.shape[0], 1))\n",
        "# Define the input shape\n",
        "input_shape = ( 1,7)\n",
        "\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
        "\n",
        "X_train1, X_val, y_train1, y_val = train_test_split(X_train, train_labels, test_size=0.3, shuffle=False)\n",
        "TrainX=X_train1\n",
        "ValidationX= X_val\n",
        "TestingX=X_test\n",
        "TrainY=y_train1\n",
        "ValidationY= y_val\n",
        "TestingY=y_test\n",
        "trainX_count = len(TrainX)\n",
        "ValidationX_count = len(ValidationX)\n",
        "testingX_count = len(TestingX)\n",
        "trainY_count = len(TrainY)\n",
        "ValidationY_count = len(ValidationY)\n",
        "testingY_count = len(TestingY)\n",
        "\n",
        "trainX_shape = TrainX.shape\n",
        "ValidationX_shape = ValidationX.shape\n",
        "testingX_shape = TestingX.shape\n",
        "trainY_shape = TrainY.shape\n",
        "ValidationY_shape = ValidationY.shape\n",
        "testingY_shape = TestingY.shape\n",
        "\n",
        "print(f\"TrainX Count: {trainX_count}, TrainX Shape: {trainX_shape}\")#, TrainX: {TrainX}\")\n",
        "print(f\"ValidationX Count: {ValidationX_count}, ValidationX Shape: {ValidationX_shape}\")#, validationX: {ValidationX}\")\n",
        "print(f\"TestingX Count: {testingX_count}, TestingX Shape: {testingX_shape}\")#, TestingX: {TestingX}\")\n",
        "print(f\"TrainY Count: {trainY_count}, TrainY Shape: {trainY_shape}\")#, TrainY: {TrainY}\")\n",
        "print(f\"ValidationY Count: {ValidationY_count}, ValidationY Shape: {ValidationY_shape}\")#, validationY: {ValidationY}\")\n",
        "print(f\"TestingY Count: {testingY_count}, TestingY Shape: {testingY_shape}\")#, TestingY: {TestingY}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zzedy-znzgLb",
        "outputId": "d8d87702-e4ee-4ab2-b828-c2c0ff856864"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0: Number of Records after Undersampling = 5067\n",
            "Class 1: Number of Records after Undersampling = 5067\n",
            "Balanced Labels: [0 1 1 ... 1 1 1]\n",
            "TrainX Count: 4256, TrainX Shape: (4256, 1, 10)\n",
            "ValidationX Count: 1824, ValidationX Shape: (1824, 1, 10)\n",
            "TestingX Count: 4054, TestingX Shape: (4054, 1, 10)\n",
            "TrainY Count: 4256, TrainY Shape: (4256, 1)\n",
            "ValidationY Count: 1824, ValidationY Shape: (1824, 1)\n",
            "TestingY Count: 4054, TestingY Shape: (4054, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Convert the data to numpy arrays\n",
        "X_train = np.array(train_data)\n",
        "X_test = np.array(test_data)\n",
        "train_labels= np.array(train_labels)\n",
        "test_labels= np.array(test_labels)\n",
        "\n",
        "# Reshape the data to 3D arrays\n",
        "X_train = np.reshape(X_train, (X_train.shape[0],1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "test_labels=np.reshape(test_labels,(test_labels.shape[0], 1))\n",
        "train_labels=np.reshape(train_labels,(train_labels.shape[0], 1))\n",
        "\n",
        "\"\"\"#Non- Gaussian Code  #LSTM AT HT\"\"\"\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(train_labels)\n",
        "X_test  = np.array(X_test)\n",
        "y_test  = np.array(test_labels)\n",
        "# y_test = y_test.flatten()\n",
        "\"\"\"#LSTM AT HY GA\"\"\"\n",
        "\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
        "\n",
        "X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the fitness function\n",
        "def evaluate_model(individual):\n",
        "    # Extract the hyperparameters from the individual\n",
        "    units = individual[0]\n",
        "    batch_size = individual[1]\n",
        "    dropout_rate = individual[2]\n",
        "    if units == 0:\n",
        "        units = 32\n",
        "    if batch_size == 0:\n",
        "        batch_size = 32\n",
        "    if dropout_rate == 0:\n",
        "        dropout_rate = 0.1\n",
        "\n",
        "    print(\"Units:\", units, \"Batch Size:\", batch_size, \"Dropout Rate:\", dropout_rate)\n",
        "\n",
        "    # Build the LSTM model with the specified hyperparameters\n",
        "    inputs = Input(shape=(1, 10))\n",
        "    lstm_out, state_h, state_c = LSTM(units, return_sequences=True, return_state=True)(inputs)\n",
        "    attention_out = Attention()([lstm_out, lstm_out])\n",
        "    dense_out = Dense(16, activation='relu')(attention_out)\n",
        "    outputs = Dense(1, activation='sigmoid')(dense_out)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train1, y_train1, epochs=1, batch_size=batch_size, shuffle=True, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "    # Make predictions on test data\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.where(y_pred > 0.5, 1, 0)\n",
        "    y_pred_binary = np.round(y_pred).flatten()\n",
        "    if len(y_test) != len(y_pred_classes):\n",
        "     raise ValueError(\"Mismatch in target shapes.\")\n",
        "    y_pred_binary=np.array(y_pred_binary)\n",
        "    print(\"y_test:\", y_test)\n",
        "    print(\"y_pred_binary:\", y_pred_binary)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "    precision = precision_score(y_test, y_pred_binary)\n",
        "    recall = recall_score(y_test, y_pred_binary)\n",
        "    f1 = f1_score(y_test, y_pred_binary)\n",
        "    # Ensure the targets have the same shape\n",
        "\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F-score:\", f1)\n",
        "    # macro_f1 = f1_score(y_test, y_pred_classes, average='macro')\n",
        "    # print(\"Macro F-score:\",macro_f1)\n",
        "    # macro_f1 = f1_score(y_test, y_pred_classes, average='micro')\n",
        "    # print(\"Micro F-score:\",macro_f1)\n",
        "\n",
        "    if len(y_test) != len(y_pred_binary) or len(y_test) != len(y_pred_classes):\n",
        "     raise ValueError(\"Mismatch in target shapes.\")\n",
        "    # Check the shapes and data types of y_test and y_pred_binary\n",
        "    print(\"y_test shape:\", y_test.shape)\n",
        "    print(\"y_pred_binary shape:\", y_pred_binary.shape)\n",
        "    print(\"y_pred_class shape:\", y_pred_classes.shape)\n",
        "    print(\"y_test data type:\", y_test.dtype)\n",
        "    print(\"y_pred_binary data type:\", y_pred_binary.dtype)\n",
        "\n",
        "    # Calculate the confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "    print(\"Confusion matrix:\", conf_matrix)\n",
        "\n",
        "    # Unpack the values from the confusion matrix\n",
        "    tn, fp, fn, tp = conf_matrix.ravel()\n",
        "    fpr = fp / (fp + tn)\n",
        "    fnr = fn / (fn + tp)\n",
        "\n",
        "    print(\"False Positive Rate (FPR):\", fpr)\n",
        "    print(\"False Negative Rate (FNR):\", fnr)\n",
        "    print(\"TN\", tn, \"  FP\",fp,\"  FN\",fn,\"  TP\",tp)\n",
        "\n",
        "    # Return the fitness value (accuracy) as a tuple\n",
        "    return accuracy,\n",
        "\n",
        "# Define the individual and population size\n",
        "INDIVIDUAL_SIZE = 3\n",
        "POPULATION_SIZE = 4\n",
        "\n",
        "# Create the individual and population classes\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "# Create the toolbox\n",
        "toolbox = base.Toolbox()\n",
        "\n",
        "# Define the hyperparameter ranges\n",
        "units_range = [32,64, 128]\n",
        "batch_size_range = [16, 32, 64,500,1000]\n",
        "dropout_rate_range = [0.1, 0.2, 0.3]\n",
        "# Register the hyperparameters in the toolbox\n",
        "# Define the custom initialization function for units\n",
        "# def init_units():\n",
        "#     return np.random.choice(units_range)\n",
        "#     return np.random.choice(units_range)\n",
        "def init_units():\n",
        "    return int(np.random.choice(units_range)) + 32\n",
        "\n",
        "# Register the units attribute with the custom initialization function\n",
        "toolbox.register(\"units\", init_units)\n",
        "\n",
        "# toolbox.register(\"units\", np.random.choice, units_range)\n",
        "def init_batch_size_range():\n",
        "    return int(np.random.choice(batch_size_range)) + 32\n",
        "\n",
        "toolbox.register(\"batch_size\", init_batch_size_range)\n",
        "def init_dropout_rate_range():\n",
        "    return int(np.random.choice(dropout_rate_range))\n",
        "toolbox.register(\"dropout_rate\", init_dropout_rate_range)\n",
        "\n",
        "# # Define the individual initialization function\n",
        "# toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
        "#                  (toolbox.units, toolbox.batch_size, toolbox.dropout_rate), n=1)\n",
        "# Define the individual initialization function\n",
        "def init_individual():\n",
        "    return creator.Individual([toolbox.units(), toolbox.batch_size(), toolbox.dropout_rate()])\n",
        "\n",
        "# Register the individual initialization function\n",
        "toolbox.register(\"individual\", init_individual)\n",
        "# Define the population initialization function\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "# Register the evaluation function\n",
        "toolbox.register(\"evaluate\", evaluate_model)\n",
        "\n",
        "# Register the selection, crossover, and mutation operations\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutUniformInt, low=0, up=2, indpb=0.5)\n",
        "\n",
        "# Define the number of generations and probability of crossover/mutation\n",
        "N_GENERATIONS = 10\n",
        "CROSSOVER_PROB = 0.8\n",
        "MUTATION_PROB = 0.2\n",
        "\n",
        "# Create the population\n",
        "population = toolbox.population(n=POPULATION_SIZE)\n",
        "\n",
        "# Create the hall of fame to store the best individuals\n",
        "hof = tools.HallOfFame(maxsize=1)\n",
        "\n",
        "# Define the statistics to collect during evolution\n",
        "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "stats.register(\"avg\", np.mean)\n",
        "stats.register(\"min\", np.min)\n",
        "stats.register(\"max\", np.max)\n",
        "\n",
        "# Run the evolution\n",
        "population, logbook = algorithms.eaSimple(\n",
        "    population,\n",
        "    toolbox,\n",
        "    cxpb=CROSSOVER_PROB,\n",
        "    mutpb=MUTATION_PROB,\n",
        "    ngen=N_GENERATIONS,\n",
        "    stats=stats,\n",
        "    halloffame=hof,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Get the best individual from the hall of fame\n",
        "best_individual = hof[0]\n",
        "\n",
        "# Print the best individual and its fitness value\n",
        "print(\"Best Individual:\", best_individual)\n",
        "print(\"Fitness Value:\", best_individual.fitness.values[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6J3MKIaCDL_",
        "outputId": "a4be4bd6-31e0-409e-ad63-c3a852c77f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Units: 96 Batch Size: 64 Dropout Rate: 0.1\n",
            "76/76 [==============================] - 3s 13ms/step - loss: 0.6383 - val_loss: 0.5366\n",
            "127/127 [==============================] - 1s 2ms/step\n",
            "y_test: [[0]\n",
            " [1]\n",
            " [0]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "y_pred_binary: [0. 1. 1. ... 1. 1. 1.]\n",
            "Accuracy: 0.9509126788357178\n",
            "Precision: 0.9313725490196079\n",
            "Recall: 0.9745969711773327\n",
            "F-score: 0.9524946287896873\n",
            "y_test shape: (4054, 1)\n",
            "y_pred_binary shape: (4054,)\n",
            "y_pred_class shape: (4054, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[1860  147]\n",
            " [  52 1995]]\n",
            "False Positive Rate (FPR): 0.07324364723467862\n",
            "False Negative Rate (FNR): 0.025403028822667317\n",
            "TN 1860   FP 147   FN 52   TP 1995\n",
            "Units: 160 Batch Size: 96 Dropout Rate: 0.1\n",
            "51/51 [==============================] - 5s 31ms/step - loss: 0.6668 - val_loss: 0.6228\n",
            "127/127 [==============================] - 1s 2ms/step\n",
            "y_test: [[0]\n",
            " [1]\n",
            " [0]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "y_pred_binary: [0. 1. 1. ... 1. 1. 1.]\n",
            "Accuracy: 0.9501726689689196\n",
            "Precision: 0.925703737886479\n",
            "Recall: 0.979970688812897\n",
            "F-score: 0.9520645467489322\n",
            "y_test shape: (4054, 1)\n",
            "y_pred_binary shape: (4054,)\n",
            "y_pred_class shape: (4054, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[1846  161]\n",
            " [  41 2006]]\n",
            "False Positive Rate (FPR): 0.0802192326856004\n",
            "False Negative Rate (FNR): 0.020029311187103077\n",
            "TN 1846   FP 161   FN 41   TP 2006\n",
            "Units: 64 Batch Size: 64 Dropout Rate: 0.1\n",
            "76/76 [==============================] - 3s 12ms/step - loss: 0.6643 - val_loss: 0.6131\n",
            "127/127 [==============================] - 1s 2ms/step\n",
            "y_test: [[0]\n",
            " [1]\n",
            " [0]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "y_pred_binary: [0. 0. 0. ... 1. 1. 1.]\n",
            "Accuracy: 0.9121854958066108\n",
            "Precision: 0.981217985202049\n",
            "Recall: 0.8422081094284318\n",
            "F-score: 0.9064143007360673\n",
            "y_test shape: (4054, 1)\n",
            "y_pred_binary shape: (4054,)\n",
            "y_pred_class shape: (4054, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[1974   33]\n",
            " [ 323 1724]]\n",
            "False Positive Rate (FPR): 0.016442451420029897\n",
            "False Negative Rate (FNR): 0.15779189057156814\n",
            "TN 1974   FP 33   FN 323   TP 1724\n",
            "Units: 160 Batch Size: 1032 Dropout Rate: 0.1\n",
            "5/5 [==============================] - 3s 173ms/step - loss: 0.6917 - val_loss: 0.6881\n",
            "127/127 [==============================] - 1s 3ms/step\n",
            "y_test: [[0]\n",
            " [1]\n",
            " [0]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "y_pred_binary: [0. 0. 1. ... 1. 1. 1.]\n",
            "Accuracy: 0.9467192895905279\n",
            "Precision: 0.9602815485168427\n",
            "Recall: 0.9330727894479727\n",
            "F-score: 0.9464816650148663\n",
            "y_test shape: (4054, 1)\n",
            "y_pred_binary shape: (4054,)\n",
            "y_pred_class shape: (4054, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[1928   79]\n",
            " [ 137 1910]]\n",
            "False Positive Rate (FPR): 0.0393622321873443\n",
            "False Negative Rate (FNR): 0.06692721055202736\n",
            "TN 1928   FP 79   FN 137   TP 1910\n",
            "gen\tnevals\tavg     \tmin     \tmax     \n",
            "0  \t4     \t0.939998\t0.912185\t0.950913\n",
            "Units: 160 Batch Size: 32 Dropout Rate: 1\n",
            "152/152 [==============================] - 4s 10ms/step - loss: 0.4755 - val_loss: 0.1942\n",
            "127/127 [==============================] - 1s 2ms/step\n",
            "y_test: [[0]\n",
            " [1]\n",
            " [0]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "y_pred_binary: [0. 1. 1. ... 1. 1. 1.]\n",
            "Accuracy: 0.9553527380365071\n",
            "Precision: 0.9388523047977423\n",
            "Recall: 0.975085490962384\n",
            "F-score: 0.9566259285885454\n",
            "y_test shape: (4054, 1)\n",
            "y_pred_binary shape: (4054,)\n",
            "y_pred_class shape: (4054, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[1877  130]\n",
            " [  51 1996]]\n",
            "False Positive Rate (FPR): 0.06477329347284504\n",
            "False Negative Rate (FNR): 0.024914509037616023\n",
            "TN 1877   FP 130   FN 51   TP 1996\n",
            "Units: 96 Batch Size: 96 Dropout Rate: 0.1\n",
            "51/51 [==============================] - 4s 29ms/step - loss: 0.6560 - val_loss: 0.6023\n",
            "127/127 [==============================] - 1s 2ms/step\n",
            "y_test: [[0]\n",
            " [1]\n",
            " [0]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "y_pred_binary: [0. 0. 1. ... 1. 1. 1.]\n",
            "Accuracy: 0.9459792797237296\n",
            "Precision: 0.9454191033138402\n",
            "Recall: 0.9477283829995115\n",
            "F-score: 0.9465723347157843\n",
            "y_test shape: (4054, 1)\n",
            "y_pred_binary shape: (4054,)\n",
            "y_pred_class shape: (4054, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[1895  112]\n",
            " [ 107 1940]]\n",
            "False Positive Rate (FPR): 0.05580468360737419\n",
            "False Negative Rate (FNR): 0.05227161700048852\n",
            "TN 1895   FP 112   FN 107   TP 1940\n",
            "1  \t2     \t0.950604\t0.945979\t0.955353\n",
            "Units: 96 Batch Size: 96 Dropout Rate: 0.1\n",
            "51/51 [==============================] - 5s 23ms/step - loss: 0.6731 - val_loss: 0.6398\n",
            "127/127 [==============================] - 1s 2ms/step\n",
            "y_test: [[0]\n",
            " [1]\n",
            " [0]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "y_pred_binary: [0. 1. 1. ... 1. 1. 1.]\n",
            "Accuracy: 0.9348791317217563\n",
            "Precision: 0.8871037776812852\n",
            "Recall: 0.9980459208597948\n",
            "F-score: 0.9393103448275861\n",
            "y_test shape: (4054, 1)\n",
            "y_pred_binary shape: (4054,)\n",
            "y_pred_class shape: (4054, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[1747  260]\n",
            " [   4 2043]]\n",
            "False Positive Rate (FPR): 0.12954658694569007\n",
            "False Negative Rate (FNR): 0.0019540791402051783\n",
            "TN 1747   FP 260   FN 4   TP 2043\n",
            "Units: 160 Batch Size: 64 Dropout Rate: 0.1\n",
            "76/76 [==============================] - 3s 14ms/step - loss: 0.6294 - val_loss: 0.5122\n",
            "127/127 [==============================] - 1s 2ms/step\n",
            "y_test: [[0]\n",
            " [1]\n",
            " [0]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "y_pred_binary: [0. 0. 1. ... 1. 1. 1.]\n",
            "Accuracy: 0.9430192402565367\n",
            "Precision: 0.9508440913604767\n",
            "Recall: 0.9355153883732291\n",
            "F-score: 0.9431174587540015\n",
            "y_test shape: (4054, 1)\n",
            "y_pred_binary shape: (4054,)\n",
            "y_pred_class shape: (4054, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[1908   99]\n",
            " [ 132 1915]]\n",
            "False Positive Rate (FPR): 0.04932735426008968\n",
            "False Negative Rate (FNR): 0.06448461162677088\n",
            "TN 1908   FP 99   FN 132   TP 1915\n",
            "Units: 1 Batch Size: 2 Dropout Rate: 0.1\n",
            "2432/2432 [==============================] - 11s 4ms/step - loss: 0.2941 - val_loss: 0.1320\n",
            "127/127 [==============================] - 1s 2ms/step\n",
            "y_test: [[0]\n",
            " [1]\n",
            " [0]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "y_pred_binary: [0. 0. 1. ... 1. 1. 1.]\n",
            "Accuracy: 0.9472126295017267\n",
            "Precision: 0.961248112732763\n",
            "Recall: 0.9330727894479727\n",
            "F-score: 0.946950917203768\n",
            "y_test shape: (4054, 1)\n",
            "y_pred_binary shape: (4054,)\n",
            "y_pred_class shape: (4054, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[1930   77]\n",
            " [ 137 1910]]\n",
            "False Positive Rate (FPR): 0.038365719980069754\n",
            "False Negative Rate (FNR): 0.06692721055202736\n",
            "TN 1930   FP 77   FN 137   TP 1910\n",
            "Units: 32 Batch Size: 32 Dropout Rate: 0.1\n",
            "152/152 [==============================] - 4s 12ms/step - loss: 0.6019 - val_loss: 0.4116\n",
            "127/127 [==============================] - 1s 3ms/step\n",
            "y_test: [[0]\n",
            " [1]\n",
            " [0]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "y_pred_binary: [0. 0. 1. ... 1. 1. 1.]\n",
            "Accuracy: 0.946225949679329\n",
            "Precision: 0.9332070108953103\n",
            "Recall: 0.9623839765510503\n",
            "F-score: 0.9475709475709476\n",
            "y_test shape: (4054, 1)\n",
            "y_pred_binary shape: (4054,)\n",
            "y_pred_class shape: (4054, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[1866  141]\n",
            " [  77 1970]]\n",
            "False Positive Rate (FPR): 0.07025411061285501\n",
            "False Negative Rate (FNR): 0.03761602344894968\n",
            "TN 1866   FP 141   FN 77   TP 1970\n",
            "2  \t4     \t0.942834\t0.934879\t0.947213\n",
            "Units: 32 Batch Size: 32 Dropout Rate: 0.1\n",
            "152/152 [==============================] - 4s 7ms/step - loss: 0.5489 - val_loss: 0.3282\n",
            "127/127 [==============================] - 1s 2ms/step\n",
            "y_test: [[0]\n",
            " [1]\n",
            " [0]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "y_pred_binary: [0. 0. 1. ... 1. 1. 1.]\n",
            "Accuracy: 0.9447459299457326\n",
            "Precision: 0.9519087754090233\n",
            "Recall: 0.9379579872984856\n",
            "F-score: 0.9448818897637796\n",
            "y_test shape: (4054, 1)\n",
            "y_pred_binary shape: (4054,)\n",
            "y_pred_class shape: (4054, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[1910   97]\n",
            " [ 127 1920]]\n",
            "False Positive Rate (FPR): 0.04833084205281515\n",
            "False Negative Rate (FNR): 0.06204201270151441\n",
            "TN 1910   FP 97   FN 127   TP 1920\n",
            "Units: 32 Batch Size: 2 Dropout Rate: 2\n",
            "2432/2432 [==============================] - 11s 4ms/step - loss: 0.1605 - val_loss: 0.0665\n",
            "127/127 [==============================] - 1s 2ms/step\n",
            "y_test: [[0]\n",
            " [1]\n",
            " [0]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "y_pred_binary: [0. 1. 1. ... 1. 1. 1.]\n",
            "Accuracy: 0.981253083374445\n",
            "Precision: 0.9655172413793104\n",
            "Recall: 0.9985344406448461\n",
            "F-score: 0.9817483189241114\n",
            "y_test shape: (4054, 1)\n",
            "y_pred_binary shape: (4054,)\n",
            "y_pred_class shape: (4054, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[1934   73]\n",
            " [   3 2044]]\n",
            "False Positive Rate (FPR): 0.036372695565520675\n",
            "False Negative Rate (FNR): 0.0014655593551538837\n",
            "TN 1934   FP 73   FN 3   TP 2044\n",
            "Units: 1 Batch Size: 2 Dropout Rate: 1\n",
            "2432/2432 [==============================] - 9s 3ms/step - loss: 0.2409 - val_loss: 0.0751\n",
            "127/127 [==============================] - 1s 2ms/step\n",
            "y_test: [[0]\n",
            " [1]\n",
            " [0]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "y_pred_binary: [0. 1. 1. ... 1. 1. 1.]\n",
            "Accuracy: 0.9736063147508633\n",
            "Precision: 0.9667949951876804\n",
            "Recall: 0.9814362481680508\n",
            "F-score: 0.9740606060606061\n",
            "y_test shape: (4054, 1)\n",
            "y_pred_binary shape: (4054,)\n",
            "y_pred_class shape: (4054, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[1938   69]\n",
            " [  38 2009]]\n",
            "False Positive Rate (FPR): 0.0343796711509716\n",
            "False Negative Rate (FNR): 0.018563751831949193\n",
            "TN 1938   FP 69   FN 38   TP 2009\n",
            "Units: 2 Batch Size: 32 Dropout Rate: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ge49ChYpbXBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea926506-84d1-4810-dd7e-fd46a30b3075"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-4e7cc2da8b60>:26: FutureWarning: In a future version of pandas all arguments of DataFrame.any and Series.any will be keyword-only.\n",
            "  merged =merged[~merged.isin([np.nan, np.inf, -np.inf]).any(1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c=0c1=1779122c2=185961c3=34267c4=0c5=0\n",
            "5067\n",
            "Number of ones: 5067\n",
            "Number of zeros: 1994283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
            "/usr/local/lib/python3.10/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Units: 64 Batch Size: 532 Dropout Rate: 0.1\n",
            "2105/2105 [==============================] - 31s 13ms/step - loss: 0.0387 - val_loss: 0.0078\n",
            "18744/18744 [==============================] - 37s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9969473412192296\n",
            "Precision: 0.3836150845253576\n",
            "Recall: 0.17857142857142858\n",
            "F-score: 0.24370095002065265\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597679    474]\n",
            " [  1357    295]]\n",
            "False Positive Rate (FPR): 0.0007924393925968774\n",
            "False Negative Rate (FNR): 0.8214285714285714\n",
            "TN 597679   FP 474   FN 1357   TP 295\n",
            "Units: 64 Batch Size: 532 Dropout Rate: 0.1\n",
            "2105/2105 [==============================] - 30s 12ms/step - loss: 0.0376 - val_loss: 0.0077\n",
            "18744/18744 [==============================] - 39s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9968923233384184\n",
            "Precision: 0.3738095238095238\n",
            "Recall: 0.1900726392251816\n",
            "F-score: 0.2520064205457464\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597627    526]\n",
            " [  1338    314]]\n",
            "False Positive Rate (FPR): 0.0008793736719534969\n",
            "False Negative Rate (FNR): 0.8099273607748184\n",
            "TN 597627   FP 526   FN 1338   TP 314\n",
            "Units: 96 Batch Size: 48 Dropout Rate: 0.1\n",
            "23326/23326 [==============================] - 125s 5ms/step - loss: 0.0101 - val_loss: 0.0063\n",
            "18744/18744 [==============================] - 39s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9967055959853619\n",
            "Precision: 0.42656391659111514\n",
            "Recall: 0.5696125907990315\n",
            "F-score: 0.487817522032141\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[596888   1265]\n",
            " [   711    941]]\n",
            "False Positive Rate (FPR): 0.0021148435266562233\n",
            "False Negative Rate (FNR): 0.4303874092009685\n",
            "TN 596888   FP 1265   FN 711   TP 941\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 18s 14ms/step - loss: 0.0610 - val_loss: 0.0089\n",
            "18744/18744 [==============================] - 34s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9970156967681163\n",
            "Precision: 0.2553191489361702\n",
            "Recall: 0.043583535108958835\n",
            "F-score: 0.07445708376421922\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597943    210]\n",
            " [  1580     72]]\n",
            "False Positive Rate (FPR): 0.0003510807435555786\n",
            "False Negative Rate (FNR): 0.9564164648910412\n",
            "TN 597943   FP 210   FN 1580   TP 72\n",
            "gen\tnevals\tavg    \tmin     \tmax     \n",
            "0  \t4     \t0.99689\t0.996706\t0.997016\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 19s 15ms/step - loss: 0.0802 - val_loss: 0.0100\n",
            "18744/18744 [==============================] - 35s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.997169079950984\n",
            "Precision: 0.020833333333333332\n",
            "Recall: 0.0006053268765133172\n",
            "F-score: 0.0011764705882352942\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598106     47]\n",
            " [  1651      1]]\n",
            "False Positive Rate (FPR): 7.857521403386759e-05\n",
            "False Negative Rate (FNR): 0.9993946731234867\n",
            "TN 598106   FP 47   FN 1651   TP 1\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 20s 16ms/step - loss: 0.0797 - val_loss: 0.0110\n",
            "18744/18744 [==============================] - 35s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.997245771542418\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F-score: 0.0\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598153      0]\n",
            " [  1652      0]]\n",
            "False Positive Rate (FPR): 0.0\n",
            "False Negative Rate (FNR): 1.0\n",
            "TN 598153   FP 0   FN 1652   TP 0\n",
            "Units: 64 Batch Size: 532 Dropout Rate: 0.1\n",
            "2105/2105 [==============================] - 25s 10ms/step - loss: 0.0447 - val_loss: 0.0079\n",
            "18744/18744 [==============================] - 33s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9970273672276824\n",
            "Precision: 0.3910149750415973\n",
            "Recall: 0.14225181598062955\n",
            "F-score: 0.20861074123391035\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597787    366]\n",
            " [  1417    235]]\n",
            "False Positive Rate (FPR): 0.000611883581625437\n",
            "False Negative Rate (FNR): 0.8577481840193705\n",
            "TN 597787   FP 366   FN 1417   TP 235\n",
            "Units: 32 Batch Size: 2 Dropout Rate: 1\n",
            "559818/559818 [==============================] - 1714s 3ms/step - loss: 0.0069 - val_loss: 0.0055\n",
            "18744/18744 [==============================] - 34s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9967772859512675\n",
            "Precision: 0.44267645858833127\n",
            "Recall: 0.6567796610169492\n",
            "F-score: 0.5288813063611992\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[596787   1366]\n",
            " [   567   1085]]\n",
            "False Positive Rate (FPR): 0.002283696646175811\n",
            "False Negative Rate (FNR): 0.3432203389830508\n",
            "TN 596787   FP 1366   FN 567   TP 1085\n",
            "1  \t4     \t0.997055\t0.996777\t0.997246\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 18s 14ms/step - loss: 0.0546 - val_loss: 0.0086\n",
            "18744/18744 [==============================] - 33s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.997245771542418\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F-score: 0.0\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598153      0]\n",
            " [  1652      0]]\n",
            "False Positive Rate (FPR): 0.0\n",
            "False Negative Rate (FNR): 1.0\n",
            "TN 598153   FP 0   FN 1652   TP 0\n",
            "Units: 64 Batch Size: 532 Dropout Rate: 0.1\n",
            "2105/2105 [==============================] - 23s 10ms/step - loss: 0.0436 - val_loss: 0.0079\n",
            "18744/18744 [==============================] - 34s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9972424371253991\n",
            "Precision: 0.45\n",
            "Recall: 0.005447941888619854\n",
            "F-score: 0.010765550239234449\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598142     11]\n",
            " [  1643      9]]\n",
            "False Positive Rate (FPR): 1.8389943710054116e-05\n",
            "False Negative Rate (FNR): 0.9945520581113801\n",
            "TN 598142   FP 11   FN 1643   TP 9\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 20s 15ms/step - loss: 0.0783 - val_loss: 0.0105\n",
            "18744/18744 [==============================] - 34s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9972157617892482\n",
            "Precision: 0.05\n",
            "Recall: 0.0006053268765133172\n",
            "F-score: 0.001196172248803828\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598134     19]\n",
            " [  1651      1]]\n",
            "False Positive Rate (FPR): 3.176444822645711e-05\n",
            "False Negative Rate (FNR): 0.9993946731234867\n",
            "TN 598134   FP 19   FN 1651   TP 1\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 19s 14ms/step - loss: 0.0630 - val_loss: 0.0089\n",
            "18744/18744 [==============================] - 34s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9970123623510975\n",
            "Precision: 0.23880597014925373\n",
            "Recall: 0.0387409200968523\n",
            "F-score: 0.06666666666666667\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597949    204]\n",
            " [  1588     64]]\n",
            "False Positive Rate (FPR): 0.00034104986516827633\n",
            "False Negative Rate (FNR): 0.9612590799031477\n",
            "TN 597949   FP 204   FN 1588   TP 64\n",
            "2  \t4     \t0.997179\t0.997012\t0.997246\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 2\n",
            "1085/1085 [==============================] - 29s 22ms/step - loss: 0.0633 - val_loss: 0.0087\n",
            "18744/18744 [==============================] - 35s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.996987354223456\n",
            "Precision: 0.29442970822281167\n",
            "Recall: 0.0671912832929782\n",
            "F-score: 0.10941350418925579\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597887    266]\n",
            " [  1541    111]]\n",
            "False Positive Rate (FPR): 0.00044470227517039954\n",
            "False Negative Rate (FNR): 0.9328087167070218\n",
            "TN 597887   FP 266   FN 1541   TP 111\n",
            "Units: 2 Batch Size: 1032 Dropout Rate: 2\n",
            "1085/1085 [==============================] - 9s 6ms/step - loss: 0.1365 - val_loss: 0.0199\n",
            "18744/18744 [==============================] - 32s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.997245771542418\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F-score: 0.0\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598153      0]\n",
            " [  1652      0]]\n",
            "False Positive Rate (FPR): 0.0\n",
            "False Negative Rate (FNR): 1.0\n",
            "TN 598153   FP 0   FN 1652   TP 0\n",
            "3  \t2     \t0.99718 \t0.996987\t0.997246\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 18s 14ms/step - loss: 0.0605 - val_loss: 0.0092\n",
            "18744/18744 [==============================] - 36s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9971157292786822\n",
            "Precision: 0.20454545454545456\n",
            "Recall: 0.016343825665859565\n",
            "F-score: 0.030269058295964126\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598048    105]\n",
            " [  1625     27]]\n",
            "False Positive Rate (FPR): 0.0001755403717777893\n",
            "False Negative Rate (FNR): 0.9836561743341404\n",
            "TN 598048   FP 105   FN 1625   TP 27\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 19s 14ms/step - loss: 0.0583 - val_loss: 0.0087\n",
            "18744/18744 [==============================] - 33s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.997245771542418\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F-score: 0.0\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598153      0]\n",
            " [  1652      0]]\n",
            "False Positive Rate (FPR): 0.0\n",
            "False Negative Rate (FNR): 1.0\n",
            "TN 598153   FP 0   FN 1652   TP 0\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 18s 14ms/step - loss: 0.0707 - val_loss: 0.0093\n",
            "18744/18744 [==============================] - 35s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9970973899850785\n",
            "Precision: 0.23353293413173654\n",
            "Recall: 0.02360774818401937\n",
            "F-score: 0.04288070368334249\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598025    128]\n",
            " [  1613     39]]\n",
            "False Positive Rate (FPR): 0.0002139920722624479\n",
            "False Negative Rate (FNR): 0.9763922518159807\n",
            "TN 598025   FP 128   FN 1613   TP 39\n",
            "Units: 2 Batch Size: 1032 Dropout Rate: 2\n",
            "1085/1085 [==============================] - 9s 6ms/step - loss: 0.1165 - val_loss: 0.0189\n",
            "18744/18744 [==============================] - 31s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.997245771542418\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F-score: 0.0\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598153      0]\n",
            " [  1652      0]]\n",
            "False Positive Rate (FPR): 0.0\n",
            "False Negative Rate (FNR): 1.0\n",
            "TN 598153   FP 0   FN 1652   TP 0\n",
            "4  \t4     \t0.997176\t0.997097\t0.997246\n",
            "Units: 64 Batch Size: 1 Dropout Rate: 0.1\n",
            "1119636/1119636 [==============================] - 3249s 3ms/step - loss: 0.0073 - val_loss: 0.0070\n",
            "18744/18744 [==============================] - 35s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9971874192445879\n",
            "Precision: 0.4744525547445255\n",
            "Recall: 0.1967312348668281\n",
            "F-score: 0.2781343602909713\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597793    360]\n",
            " [  1327    325]]\n",
            "False Positive Rate (FPR): 0.0006018527032381348\n",
            "False Negative Rate (FNR): 0.8032687651331719\n",
            "TN 597793   FP 360   FN 1327   TP 325\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 18s 14ms/step - loss: 0.0614 - val_loss: 0.0089\n",
            "18744/18744 [==============================] - 36s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.997245771542418\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F-score: 0.0\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598153      0]\n",
            " [  1652      0]]\n",
            "False Positive Rate (FPR): 0.0\n",
            "False Negative Rate (FNR): 1.0\n",
            "TN 598153   FP 0   FN 1652   TP 0\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 19s 14ms/step - loss: 0.0656 - val_loss: 0.0091\n",
            "18744/18744 [==============================] - 35s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9970507081468144\n",
            "Precision: 0.23766816143497757\n",
            "Recall: 0.03208232445520581\n",
            "F-score: 0.056533333333333345\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597983    170]\n",
            " [  1599     53]]\n",
            "False Positive Rate (FPR): 0.0002842082209735636\n",
            "False Negative Rate (FNR): 0.9679176755447942\n",
            "TN 597983   FP 170   FN 1599   TP 53\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 20s 15ms/step - loss: 0.0478 - val_loss: 0.0083\n",
            "18744/18744 [==============================] - 34s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.997245771542418\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F-score: 0.0\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598153      0]\n",
            " [  1652      0]]\n",
            "False Positive Rate (FPR): 0.0\n",
            "False Negative Rate (FNR): 1.0\n",
            "TN 598153   FP 0   FN 1652   TP 0\n",
            "5  \t4     \t0.997182\t0.997051\t0.997246\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 18s 14ms/step - loss: 0.0612 - val_loss: 0.0088\n",
            "18744/18744 [==============================] - 33s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.996949008427739\n",
            "Precision: 0.24277456647398843\n",
            "Recall: 0.05084745762711865\n",
            "F-score: 0.08408408408408409\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597891    262]\n",
            " [  1568     84]]\n",
            "False Positive Rate (FPR): 0.00043801502291219806\n",
            "False Negative Rate (FNR): 0.9491525423728814\n",
            "TN 597891   FP 262   FN 1568   TP 84\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 18s 14ms/step - loss: 0.0551 - val_loss: 0.0086\n",
            "18744/18744 [==============================] - 34s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9969423395937013\n",
            "Precision: 0.2769607843137255\n",
            "Recall: 0.06840193704600485\n",
            "F-score: 0.10970873786407767\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597858    295]\n",
            " [  1539    113]]\n",
            "False Positive Rate (FPR): 0.0004931848540423604\n",
            "False Negative Rate (FNR): 0.9315980629539952\n",
            "TN 597858   FP 295   FN 1539   TP 113\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 20s 16ms/step - loss: 0.0580 - val_loss: 0.0088\n",
            "18744/18744 [==============================] - 34s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9969623460958145\n",
            "Precision: 0.2360248447204969\n",
            "Recall: 0.04600484261501211\n",
            "F-score: 0.07700101317122594\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597907    246]\n",
            " [  1576     76]]\n",
            "False Positive Rate (FPR): 0.0004112660138793921\n",
            "False Negative Rate (FNR): 0.9539951573849879\n",
            "TN 597907   FP 246   FN 1576   TP 76\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 18s 14ms/step - loss: 0.0709 - val_loss: 0.0095\n",
            "18744/18744 [==============================] - 34s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9971524078658898\n",
            "Precision: 0.07575757575757576\n",
            "Recall: 0.003026634382566586\n",
            "F-score: 0.005820721769499417\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598092     61]\n",
            " [  1647      5]]\n",
            "False Positive Rate (FPR): 0.00010198059693757283\n",
            "False Negative Rate (FNR): 0.9969733656174334\n",
            "TN 598092   FP 61   FN 1647   TP 5\n",
            "6  \t4     \t0.997002\t0.996942\t0.997152\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 18s 14ms/step - loss: 0.0611 - val_loss: 0.0089\n",
            "18744/18744 [==============================] - 34s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.997245771542418\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F-score: 0.0\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598153      0]\n",
            " [  1652      0]]\n",
            "False Positive Rate (FPR): 0.0\n",
            "False Negative Rate (FNR): 1.0\n",
            "TN 598153   FP 0   FN 1652   TP 0\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 18s 14ms/step - loss: 0.0634 - val_loss: 0.0088\n",
            "18744/18744 [==============================] - 35s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9969990246830219\n",
            "Precision: 0.27439024390243905\n",
            "Recall: 0.05447941888619855\n",
            "F-score: 0.09090909090909091\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597915    238]\n",
            " [  1562     90]]\n",
            "False Positive Rate (FPR): 0.00039789150936298906\n",
            "False Negative Rate (FNR): 0.9455205811138014\n",
            "TN 597915   FP 238   FN 1562   TP 90\n",
            "7  \t2     \t0.997137\t0.996999\t0.997246\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 19s 15ms/step - loss: 0.0646 - val_loss: 0.0085\n",
            "18744/18744 [==============================] - 33s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9970557097723427\n",
            "Precision: 0.3333333333333333\n",
            "Recall: 0.06900726392251816\n",
            "F-score: 0.11434302908726178\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597925    228]\n",
            " [  1538    114]]\n",
            "False Positive Rate (FPR): 0.00038117337871748533\n",
            "False Negative Rate (FNR): 0.9309927360774818\n",
            "TN 597925   FP 228   FN 1538   TP 114\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 19s 14ms/step - loss: 0.0641 - val_loss: 0.0089\n",
            "18744/18744 [==============================] - 34s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9969456740107201\n",
            "Precision: 0.2398843930635838\n",
            "Recall: 0.050242130750605324\n",
            "F-score: 0.08308308308308308\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597890    263]\n",
            " [  1569     83]]\n",
            "False Positive Rate (FPR): 0.00043968683597674844\n",
            "False Negative Rate (FNR): 0.9497578692493946\n",
            "TN 597890   FP 263   FN 1569   TP 83\n",
            "8  \t2     \t0.9971  \t0.996946\t0.997246\n",
            "Units: 32 Batch Size: 32 Dropout Rate: 0.1\n",
            "34989/34989 [==============================] - 129s 4ms/step - loss: 0.0095 - val_loss: 0.0060\n",
            "18744/18744 [==============================] - 33s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9968573119597203\n",
            "Precision: 0.4378666666666667\n",
            "Recall: 0.49697336561743344\n",
            "F-score: 0.4655514601644457\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597099   1054]\n",
            " [   831    821]]\n",
            "False Positive Rate (FPR): 0.0017620909700360944\n",
            "False Negative Rate (FNR): 0.5030266343825666\n",
            "TN 597099   FP 1054   FN 831   TP 821\n",
            "9  \t1     \t0.997149\t0.996857\t0.997246\n",
            "Units: 32 Batch Size: 32 Dropout Rate: 0.1\n",
            "34989/34989 [==============================] - 134s 4ms/step - loss: 0.0093 - val_loss: 0.0059\n",
            "18744/18744 [==============================] - 33s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9970123623510975\n",
            "Precision: 0.45145631067961167\n",
            "Recall: 0.3940677966101695\n",
            "F-score: 0.42081447963800905\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597362    791]\n",
            " [  1001    651]]\n",
            "False Positive Rate (FPR): 0.001322404134059346\n",
            "False Negative Rate (FNR): 0.6059322033898306\n",
            "TN 597362   FP 791   FN 1001   TP 651\n",
            "Units: 64 Batch Size: 1 Dropout Rate: 0.1\n",
            "1119636/1119636 [==============================] - 3292s 3ms/step - loss: 0.0075 - val_loss: 0.0062\n",
            "18744/18744 [==============================] - 34s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9970823851084936\n",
            "Precision: 0.4541198501872659\n",
            "Recall: 0.29358353510895885\n",
            "F-score: 0.35661764705882354\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597570    583]\n",
            " [  1167    485]]\n",
            "False Positive Rate (FPR): 0.0009746670166328682\n",
            "False Negative Rate (FNR): 0.7064164648910412\n",
            "TN 597570   FP 583   FN 1167   TP 485\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 18s 15ms/step - loss: 0.0640 - val_loss: 0.0090\n",
            "18744/18744 [==============================] - 34s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9971290669467577\n",
            "Precision: 0.25\n",
            "Recall: 0.0211864406779661\n",
            "F-score: 0.0390625\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598048    105]\n",
            " [  1617     35]]\n",
            "False Positive Rate (FPR): 0.0001755403717777893\n",
            "False Negative Rate (FNR): 0.9788135593220338\n",
            "TN 598048   FP 105   FN 1617   TP 35\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 18s 14ms/step - loss: 0.0584 - val_loss: 0.0087\n",
            "18744/18744 [==============================] - 35s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9969340035511541\n",
            "Precision: 0.2545931758530184\n",
            "Recall: 0.05871670702179177\n",
            "F-score: 0.09542547958681752\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597869    284]\n",
            " [  1555     97]]\n",
            "False Positive Rate (FPR): 0.0004747949103323063\n",
            "False Negative Rate (FNR): 0.9412832929782082\n",
            "TN 597869   FP 284   FN 1555   TP 97\n",
            "10 \t4     \t0.997039\t0.996934\t0.997129\n",
            "Best Individual: [64, 1032, 0]\n",
            "Fitness Value: 0.997245771542418\n"
          ]
        }
      ],
      "source": [
        "#epochs=1000\n",
        "#batchsize=32\n",
        "nooffile=2\n",
        "n=10\n",
        "\n",
        "\n",
        "\"\"\"#Google Usgae Data\"\"\"\n",
        "\n",
        "n=5\n",
        " # Download the dataset\n",
        "merged = pd.read_csv('/content/drive/My Drive/part-00001-of-00500.csv', header=None)\n",
        "#  merged = pd.read_csv('/content/drive/My Drive/mixed_0101_gradual.csv', header='infer')\n",
        "#  merged = pd.read_csv('/content/drive/My Drive/sea4.csv', header='infer')\n",
        " #dataframe.head()\n",
        "# merged = merged.head(500)\n",
        "\n",
        "i=1\n",
        "# while i<nooffile:\n",
        "#   a=pd.read_csv(\"/content/drive/My Drive/part-0000\"+str(i)+\"-of-00500.csv\")\n",
        "#   merged=np.concatenate([merged, a])\n",
        "#   a=0\n",
        "#   i=i+1\n",
        "# del a\n",
        "merged=pd.DataFrame(data=merged)\n",
        "merged.fillna(merged.mean())\n",
        "merged =merged[~merged.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
        "# merged = merged.sample(n=50000, random_state=42)\n",
        "\n",
        "\n",
        "#  merged = merged.sample(n=50000, random_state=42)\n",
        "merged=np.array(merged)\n",
        "y =merged[:,n] #5\n",
        "z=merged[:,6]\n",
        "#  merged= np.delete(merged,[n], axis=1)\n",
        "#56\n",
        "#merged=np.delete(merged,[0,1,2,3,4,7,8,9,10,11,12,13,14,15,16,17,18], axis=1)\n",
        "#4-14\n",
        "merged=np.delete(merged,[0,1,2,3,4,15,16,17,18], axis=1)\n",
        "#5-10\n",
        "#  merged=np.delete(merged,[0,1,2,3,4,11,12,13,15,16,17,18], axis=1)\n",
        "#print(merged)\n",
        "def min_max_scaling(df):\n",
        "    # copy the dataframe\n",
        "    df_norm = df\n",
        "    # apply min-max scaling\n",
        "   # for column in df_norm.columns:\n",
        "    df_norm = ((df_norm - 0) / (1 - 0))*((1-0)+0)\n",
        "    #print(\"number \"+str(df)+\"converted\"+str(df_norm))\n",
        "    return df_norm\n",
        "y1 = DynamicArray()\n",
        "y11 = DynamicArray()\n",
        "z1 = DynamicArray()\n",
        "z11 = DynamicArray()\n",
        "c=0\n",
        "c1=0\n",
        "c2=0\n",
        "c3=0\n",
        "c4=0\n",
        "c5=0\n",
        "y=np.array(y)\n",
        "for a in y:\n",
        " y11.append(min_max_scaling(a))\n",
        "for a in y11:\n",
        " if  0.05> a>= 0:\n",
        "  y1.append(0)\n",
        "  c1+=1\n",
        " elif  0.125> a>= 0.05:\n",
        "  y1.append(1)\n",
        "  c2+=1\n",
        " elif 1>= a>= 0.125:\n",
        "  y1.append(2)\n",
        "  #print(a)\n",
        "  c3+=1\n",
        " else:\n",
        "  y1.append(0)\n",
        "  c+=1\n",
        "y11 = []\n",
        "for a in range(len(y1)):\n",
        " if (y1[a]-y1[a-1]>=2) :\n",
        "  #print(abs(y1[a]-y1[a-1]))\n",
        "  y11.append(1)\n",
        " else:\n",
        "  #print(abs(y1[a]-y1[a-1]))\n",
        "  y11.append(0)\n",
        "  #index=index+1\n",
        "\n",
        "#del y1\n",
        "print(\"c=\"+str(c)+\"c1=\"+str(c1)+\"c2=\"+str(c2)+\"c3=\"+str(c3)+\"c4=\"+str(c4)+\"c5=\"+str(c5))\n",
        "c3=0\n",
        "c2=0\n",
        "for q in y11:\n",
        " if q==1:\n",
        "  c3=c3+1\n",
        " else:\n",
        "  c2=c2+1\n",
        "print(c3)\n",
        "labels =y11#np.array(y11)# raw_data[:, -1]\n",
        "data = merged\n",
        "\n",
        "# # The other dat\n",
        "# from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# ros = RandomOverSampler(random_state=42)\n",
        "# data,labels = ros.fit_resample(data1,labels1)\n",
        "\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    data, labels, test_size=0.3, shuffle=False)\n",
        "num_ones = np.count_nonzero(labels)\n",
        "num_zeros = len(labels) - num_ones\n",
        "print(f\"Number of ones: {num_ones}\")\n",
        "print(f\"Number of zeros: {num_zeros}\")\n",
        "# Convert the data to numpy arrays\n",
        "X_train = np.array(train_data)\n",
        "X_test = np.array(test_data)\n",
        "train_labels= np.array(train_labels)\n",
        "test_labels= np.array(test_labels)\n",
        "\n",
        "# Reshape the data to 3D arrays\n",
        "X_train = np.reshape(X_train, (X_train.shape[0],1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "test_labels=np.reshape(test_labels,(test_labels.shape[0], 1))\n",
        "train_labels=np.reshape(train_labels,(train_labels.shape[0], 1))\n",
        "\n",
        "\"\"\"#Non- Gaussian Code  #LSTM AT HT\"\"\"\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(train_labels)\n",
        "X_test  = np.array(X_test)\n",
        "y_test  = np.array(test_labels)\n",
        "# y_test = y_test.flatten()\n",
        "\"\"\"#LSTM AT HY GA\"\"\"\n",
        "\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
        "\n",
        "X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the fitness function\n",
        "def evaluate_model(individual):\n",
        "    # Extract the hyperparameters from the individual\n",
        "    units = individual[0]\n",
        "    batch_size = individual[1]\n",
        "    dropout_rate = individual[2]\n",
        "    if units == 0:\n",
        "        units = 32\n",
        "    if batch_size == 0:\n",
        "        batch_size = 32\n",
        "    if dropout_rate == 0:\n",
        "        dropout_rate = 0.1\n",
        "\n",
        "    print(\"Units:\", units, \"Batch Size:\", batch_size, \"Dropout Rate:\", dropout_rate)\n",
        "\n",
        "    # Build the LSTM model with the specified hyperparameters\n",
        "    inputs = Input(shape=(1, 10))\n",
        "    lstm_out, state_h, state_c = LSTM(units, return_sequences=True, return_state=True)(inputs)\n",
        "    attention_out = Attention()([lstm_out, lstm_out])\n",
        "    dense_out = Dense(16, activation='relu')(attention_out)\n",
        "    outputs = Dense(1, activation='sigmoid')(dense_out)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train1, y_train1, epochs=1, batch_size=batch_size, shuffle=True, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "    # Make predictions on test data\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.where(y_pred > 0.5, 1, 0)\n",
        "    y_pred_binary = np.round(y_pred).flatten()\n",
        "    if len(y_test) != len(y_pred_classes):\n",
        "     raise ValueError(\"Mismatch in target shapes.\")\n",
        "    y_pred_binary=np.array(y_pred_binary)\n",
        "    print(\"y_test:\", y_test)\n",
        "    print(\"y_pred_binary:\", y_pred_binary)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "    precision = precision_score(y_test, y_pred_binary)\n",
        "    recall = recall_score(y_test, y_pred_binary)\n",
        "    f1 = f1_score(y_test, y_pred_binary)\n",
        "    # Ensure the targets have the same shape\n",
        "\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F-score:\", f1)\n",
        "    # macro_f1 = f1_score(y_test, y_pred_classes, average='macro')\n",
        "    # print(\"Macro F-score:\",macro_f1)\n",
        "    # macro_f1 = f1_score(y_test, y_pred_classes, average='micro')\n",
        "    # print(\"Micro F-score:\",macro_f1)\n",
        "\n",
        "    if len(y_test) != len(y_pred_binary) or len(y_test) != len(y_pred_classes):\n",
        "     raise ValueError(\"Mismatch in target shapes.\")\n",
        "    # Check the shapes and data types of y_test and y_pred_binary\n",
        "    print(\"y_test shape:\", y_test.shape)\n",
        "    print(\"y_pred_binary shape:\", y_pred_binary.shape)\n",
        "    print(\"y_pred_class shape:\", y_pred_classes.shape)\n",
        "    print(\"y_test data type:\", y_test.dtype)\n",
        "    print(\"y_pred_binary data type:\", y_pred_binary.dtype)\n",
        "\n",
        "    # Calculate the confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "    print(\"Confusion matrix:\", conf_matrix)\n",
        "\n",
        "    # Unpack the values from the confusion matrix\n",
        "    tn, fp, fn, tp = conf_matrix.ravel()\n",
        "    fpr = fp / (fp + tn)\n",
        "    fnr = fn / (fn + tp)\n",
        "\n",
        "    print(\"False Positive Rate (FPR):\", fpr)\n",
        "    print(\"False Negative Rate (FNR):\", fnr)\n",
        "    print(\"TN\", tn, \"  FP\",fp,\"  FN\",fn,\"  TP\",tp)\n",
        "\n",
        "    # Return the fitness value (accuracy) as a tuple\n",
        "    return accuracy,\n",
        "\n",
        "# Define the individual and population size\n",
        "INDIVIDUAL_SIZE = 3\n",
        "POPULATION_SIZE = 4\n",
        "\n",
        "# Create the individual and population classes\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "# Create the toolbox\n",
        "toolbox = base.Toolbox()\n",
        "\n",
        "# Define the hyperparameter ranges\n",
        "units_range = [32,64, 128]\n",
        "batch_size_range = [16, 32, 64,500,1000]\n",
        "dropout_rate_range = [0.1, 0.2, 0.3]\n",
        "# Register the hyperparameters in the toolbox\n",
        "# Define the custom initialization function for units\n",
        "# def init_units():\n",
        "#     return np.random.choice(units_range)\n",
        "#     return np.random.choice(units_range)\n",
        "def init_units():\n",
        "    return int(np.random.choice(units_range)) + 32\n",
        "\n",
        "# Register the units attribute with the custom initialization function\n",
        "toolbox.register(\"units\", init_units)\n",
        "\n",
        "# toolbox.register(\"units\", np.random.choice, units_range)\n",
        "def init_batch_size_range():\n",
        "    return int(np.random.choice(batch_size_range)) + 32\n",
        "\n",
        "toolbox.register(\"batch_size\", init_batch_size_range)\n",
        "def init_dropout_rate_range():\n",
        "    return int(np.random.choice(dropout_rate_range))\n",
        "toolbox.register(\"dropout_rate\", init_dropout_rate_range)\n",
        "\n",
        "# # Define the individual initialization function\n",
        "# toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
        "#                  (toolbox.units, toolbox.batch_size, toolbox.dropout_rate), n=1)\n",
        "# Define the individual initialization function\n",
        "def init_individual():\n",
        "    return creator.Individual([toolbox.units(), toolbox.batch_size(), toolbox.dropout_rate()])\n",
        "\n",
        "# Register the individual initialization function\n",
        "toolbox.register(\"individual\", init_individual)\n",
        "# Define the population initialization function\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "# Register the evaluation function\n",
        "toolbox.register(\"evaluate\", evaluate_model)\n",
        "\n",
        "# Register the selection, crossover, and mutation operations\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutUniformInt, low=0, up=2, indpb=0.5)\n",
        "\n",
        "# Define the number of generations and probability of crossover/mutation\n",
        "N_GENERATIONS = 10\n",
        "CROSSOVER_PROB = 0.8\n",
        "MUTATION_PROB = 0.2\n",
        "\n",
        "# Create the population\n",
        "population = toolbox.population(n=POPULATION_SIZE)\n",
        "\n",
        "# Create the hall of fame to store the best individuals\n",
        "hof = tools.HallOfFame(maxsize=1)\n",
        "\n",
        "# Define the statistics to collect during evolution\n",
        "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "stats.register(\"avg\", np.mean)\n",
        "stats.register(\"min\", np.min)\n",
        "stats.register(\"max\", np.max)\n",
        "\n",
        "# Run the evolution\n",
        "population, logbook = algorithms.eaSimple(\n",
        "    population,\n",
        "    toolbox,\n",
        "    cxpb=CROSSOVER_PROB,\n",
        "    mutpb=MUTATION_PROB,\n",
        "    ngen=N_GENERATIONS,\n",
        "    stats=stats,\n",
        "    halloffame=hof,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Get the best individual from the hall of fame\n",
        "best_individual = hof[0]\n",
        "\n",
        "# Print the best individual and its fitness value\n",
        "print(\"Best Individual:\", best_individual)\n",
        "print(\"Fitness Value:\", best_individual.fitness.values[0])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "erdvdFBzPzoT",
        "pHrhxJ5xNafj",
        "uajrkrt9NQik"
      ],
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1tpBnshFcFzlR3qOEb-Xl04LgNycwfipJ",
      "authorship_tag": "ABX9TyNWFsUxXf/XX7JK0/HPZ+Ey",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}