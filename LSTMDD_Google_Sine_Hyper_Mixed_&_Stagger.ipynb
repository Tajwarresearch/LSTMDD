{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tajwarresearch/LSTMDD/blob/main/LSTMDD_Google_Sine_Hyper_Mixed_%26_Stagger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4kidZ3cM2Nv"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erdvdFBzPzoT"
      },
      "source": [
        "#libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdD9SIa3KUON",
        "outputId": "fddbae04-d3eb-420d-a7be-61e21f2d540b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dynarray\n",
            "  Downloading dynarray-0.1.3.tar.gz (2.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dynarray) (1.23.5)\n",
            "Building wheels for collected packages: dynarray\n",
            "  Building wheel for dynarray (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dynarray: filename=dynarray-0.1.3-py3-none-any.whl size=3416 sha256=152df555aca5065350609c8f8cf56991f69aa3b3a471c32055cfe8e4cc585d72\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/48/46/1984e3ea5a27e3012101e41f291452f2a45deeb7d04526f25a\n",
            "Successfully built dynarray\n",
            "Installing collected packages: dynarray\n",
            "Successfully installed dynarray-0.1.3\n",
            "Collecting deap\n",
            "  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deap) (1.23.5)\n",
            "Installing collected packages: deap\n",
            "Successfully installed deap-1.4.1\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.3.5-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2023.7.22)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.3.5 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install dynarray\n",
        "!pip install deap\n",
        "!pip install keras-tuner\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wGiZM8dOVCel"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import kstest\n",
        "from dynarray import DynamicArray\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import csv\n",
        "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as stats\n",
        "import seaborn as sb\n",
        "from keras.layers import Layer, TimeDistributed, RepeatVector\n",
        "from keras.layers import Bidirectional, Flatten, Concatenate\n",
        "from keras.layers import Lambda, Permute, dot\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from keras.layers import Attention\n",
        "from deap import base, creator, tools, algorithms\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Lcgw8wlLP1uW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# importing the necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model# Download the dataset\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import kstest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
        "from matplotlib import pyplot as plt\n",
        "from dynarray import DynamicArray\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "import csv\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import kstest\n",
        "# importing the necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model# Download the dataset\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import kstest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
        "from matplotlib import pyplot as plt\n",
        "from dynarray import DynamicArray\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "import csv\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import kstest\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as mp\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "sns.set()\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import kstest\n",
        "import keras.backend as K\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout\n",
        "from keras.layers import Layer, TimeDistributed, RepeatVector\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Flatten, Input, Concatenate\n",
        "from keras.layers import Lambda, Permute, dot\n",
        "from keras import backend as K\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "import numpy as np\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.layers import Attention\n",
        "from keras.models import Model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from deap import base, creator, tools, algorithms\n",
        "from keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.layers import Attention\n",
        "from keras.models import Model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from deap import base, creator, tools, algorithms\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "from sklearn import metrics\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import kstest\n",
        "from dynarray import DynamicArray\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import roc_curve\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as stats\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "from keras.layers import Layer, TimeDistributed, RepeatVector\n",
        "from keras.layers import Bidirectional, Flatten, Concatenate\n",
        "from keras.layers import Lambda, Permute, dot\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from keras.layers import Attention\n",
        "from deap import base, creator, tools, algorithms\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.models import Model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHrhxJ5xNafj"
      },
      "source": [
        "#Artifical Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKRdFchGNC1d",
        "outputId": "7045c1a4-ff4a-4add-95fd-46934a6ce370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of ones: 20501\n",
            "Number of zeros: 20499\n",
            "Units: 64 Batch Size: 48 Dropout Rate: 0.1\n",
            "Epoch 1/500\n",
            "479/479 [==============================] - 9s 9ms/step - loss: 0.6751 - val_loss: 0.6685\n",
            "Epoch 2/500\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.6689 - val_loss: 0.6680\n",
            "Epoch 3/500\n",
            "479/479 [==============================] - 5s 10ms/step - loss: 0.6689 - val_loss: 0.6678\n",
            "Epoch 4/500\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.6686 - val_loss: 0.6685\n",
            "Epoch 5/500\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.6684 - val_loss: 0.6684\n",
            "Epoch 6/500\n",
            "479/479 [==============================] - 5s 10ms/step - loss: 0.6684 - val_loss: 0.6674\n",
            "Epoch 7/500\n",
            "479/479 [==============================] - 5s 10ms/step - loss: 0.6682 - val_loss: 0.6678\n",
            "Epoch 8/500\n",
            "479/479 [==============================] - 4s 9ms/step - loss: 0.6682 - val_loss: 0.6672\n",
            "Epoch 9/500\n",
            "479/479 [==============================] - 4s 8ms/step - loss: 0.6682 - val_loss: 0.6670\n",
            "Epoch 10/500\n",
            "479/479 [==============================] - 3s 6ms/step - loss: 0.6680 - val_loss: 0.6669\n",
            "Epoch 11/500\n",
            "479/479 [==============================] - 3s 6ms/step - loss: 0.6679 - val_loss: 0.6668\n",
            "Epoch 12/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6679 - val_loss: 0.6677\n",
            "Epoch 13/500\n",
            "479/479 [==============================] - 2s 5ms/step - loss: 0.6674 - val_loss: 0.6671\n",
            "Epoch 14/500\n",
            "479/479 [==============================] - 2s 5ms/step - loss: 0.6676 - val_loss: 0.6660\n",
            "Epoch 15/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6674 - val_loss: 0.6660\n",
            "Epoch 16/500\n",
            "479/479 [==============================] - 3s 5ms/step - loss: 0.6672 - val_loss: 0.6658\n",
            "Epoch 17/500\n",
            "479/479 [==============================] - 3s 6ms/step - loss: 0.6669 - val_loss: 0.6655\n",
            "Epoch 18/500\n",
            "479/479 [==============================] - 2s 5ms/step - loss: 0.6670 - val_loss: 0.6659\n",
            "Epoch 19/500\n",
            "479/479 [==============================] - 2s 5ms/step - loss: 0.6669 - val_loss: 0.6661\n",
            "Epoch 20/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6665 - val_loss: 0.6653\n",
            "Epoch 21/500\n",
            "479/479 [==============================] - 2s 5ms/step - loss: 0.6665 - val_loss: 0.6654\n",
            "Epoch 22/500\n",
            "479/479 [==============================] - 3s 5ms/step - loss: 0.6665 - val_loss: 0.6649\n",
            "Epoch 23/500\n",
            "479/479 [==============================] - 3s 6ms/step - loss: 0.6664 - val_loss: 0.6654\n",
            "Epoch 24/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6663 - val_loss: 0.6657\n",
            "Epoch 25/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6661 - val_loss: 0.6658\n",
            "Epoch 26/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6665 - val_loss: 0.6653\n",
            "Epoch 27/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6660 - val_loss: 0.6648\n",
            "Epoch 28/500\n",
            "479/479 [==============================] - 2s 5ms/step - loss: 0.6658 - val_loss: 0.6648\n",
            "Epoch 29/500\n",
            "479/479 [==============================] - 3s 6ms/step - loss: 0.6658 - val_loss: 0.6649\n",
            "Epoch 30/500\n",
            "479/479 [==============================] - 2s 5ms/step - loss: 0.6658 - val_loss: 0.6657\n",
            "Epoch 31/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6657 - val_loss: 0.6646\n",
            "Epoch 32/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6657 - val_loss: 0.6648\n",
            "Epoch 33/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6657 - val_loss: 0.6647\n",
            "Epoch 34/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6656 - val_loss: 0.6651\n",
            "Epoch 35/500\n",
            "479/479 [==============================] - 3s 6ms/step - loss: 0.6657 - val_loss: 0.6650\n",
            "Epoch 36/500\n",
            "479/479 [==============================] - 3s 6ms/step - loss: 0.6656 - val_loss: 0.6655\n",
            "Epoch 37/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6655 - val_loss: 0.6644\n",
            "Epoch 38/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6654 - val_loss: 0.6648\n",
            "Epoch 39/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6652 - val_loss: 0.6649\n",
            "Epoch 40/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6653 - val_loss: 0.6652\n",
            "Epoch 41/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6654 - val_loss: 0.6649\n",
            "Epoch 42/500\n",
            "479/479 [==============================] - 3s 6ms/step - loss: 0.6653 - val_loss: 0.6648\n",
            "Epoch 43/500\n",
            "479/479 [==============================] - 2s 5ms/step - loss: 0.6652 - val_loss: 0.6650\n",
            "Epoch 44/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6651 - val_loss: 0.6650\n",
            "Epoch 45/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6653 - val_loss: 0.6653\n",
            "Epoch 46/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6652 - val_loss: 0.6645\n",
            "Epoch 47/500\n",
            "479/479 [==============================] - 2s 4ms/step - loss: 0.6651 - val_loss: 0.6648\n",
            "385/385 [==============================] - 1s 2ms/step\n",
            "Predicted Class Labels: [0 1 0 ... 0 1 1]\n",
            "Accuracy: 0.23260162601626017\n",
            "Precision: 0.23259958282185036\n",
            "Recall: 0.23260162601626017\n",
            "F-score: 0.23260016009945317\n",
            "ROC AUC: 0.23260162601626017\n",
            "False Positive Rate (FPR): 0.7660162601626016\n",
            "False Negative Rate (FNR): 0.7687804878048781\n",
            "TN: 1439 FP: 4711 FN: 4728 TP: 1422\n",
            "Units: 288 Batch Size: 160 Dropout Rate: 0.1\n",
            "Epoch 1/500\n",
            "144/144 [==============================] - 6s 22ms/step - loss: 0.6763 - val_loss: 0.6701\n",
            "Epoch 2/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6696 - val_loss: 0.6679\n",
            "Epoch 3/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6690 - val_loss: 0.6677\n",
            "Epoch 4/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6688 - val_loss: 0.6679\n",
            "Epoch 5/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6685 - val_loss: 0.6677\n",
            "Epoch 6/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6684 - val_loss: 0.6680\n",
            "Epoch 7/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6686 - val_loss: 0.6676\n",
            "Epoch 8/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6684 - val_loss: 0.6674\n",
            "Epoch 9/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6686 - val_loss: 0.6673\n",
            "Epoch 10/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6683 - val_loss: 0.6681\n",
            "Epoch 11/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6683 - val_loss: 0.6684\n",
            "Epoch 12/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6684 - val_loss: 0.6678\n",
            "Epoch 13/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6682 - val_loss: 0.6672\n",
            "Epoch 14/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6680 - val_loss: 0.6670\n",
            "Epoch 15/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6679 - val_loss: 0.6667\n",
            "Epoch 16/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6678 - val_loss: 0.6665\n",
            "Epoch 17/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6674 - val_loss: 0.6667\n",
            "Epoch 18/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6674 - val_loss: 0.6663\n",
            "Epoch 19/500\n",
            "144/144 [==============================] - 3s 23ms/step - loss: 0.6672 - val_loss: 0.6662\n",
            "Epoch 20/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6673 - val_loss: 0.6669\n",
            "Epoch 21/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6673 - val_loss: 0.6665\n",
            "Epoch 22/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6672 - val_loss: 0.6656\n",
            "Epoch 23/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6670 - val_loss: 0.6655\n",
            "Epoch 24/500\n",
            "144/144 [==============================] - 3s 23ms/step - loss: 0.6669 - val_loss: 0.6654\n",
            "Epoch 25/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6667 - val_loss: 0.6654\n",
            "Epoch 26/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6667 - val_loss: 0.6656\n",
            "Epoch 27/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6665 - val_loss: 0.6652\n",
            "Epoch 28/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6666 - val_loss: 0.6652\n",
            "Epoch 29/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6667 - val_loss: 0.6651\n",
            "Epoch 30/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6664 - val_loss: 0.6651\n",
            "Epoch 31/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6665 - val_loss: 0.6652\n",
            "Epoch 32/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6665 - val_loss: 0.6655\n",
            "Epoch 33/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6665 - val_loss: 0.6651\n",
            "Epoch 34/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6663 - val_loss: 0.6649\n",
            "Epoch 35/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6663 - val_loss: 0.6652\n",
            "Epoch 36/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6661 - val_loss: 0.6657\n",
            "Epoch 37/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6661 - val_loss: 0.6648\n",
            "Epoch 38/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6662 - val_loss: 0.6648\n",
            "Epoch 39/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6661 - val_loss: 0.6653\n",
            "Epoch 40/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6661 - val_loss: 0.6647\n",
            "Epoch 41/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6659 - val_loss: 0.6648\n",
            "Epoch 42/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6658 - val_loss: 0.6648\n",
            "Epoch 43/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6660 - val_loss: 0.6654\n",
            "Epoch 44/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6658 - val_loss: 0.6646\n",
            "Epoch 45/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6657 - val_loss: 0.6644\n",
            "Epoch 46/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6657 - val_loss: 0.6651\n",
            "Epoch 47/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6658 - val_loss: 0.6654\n",
            "Epoch 48/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6659 - val_loss: 0.6645\n",
            "Epoch 49/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6653 - val_loss: 0.6661\n",
            "Epoch 50/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6657 - val_loss: 0.6645\n",
            "Epoch 51/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6658 - val_loss: 0.6644\n",
            "Epoch 52/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6655 - val_loss: 0.6645\n",
            "Epoch 53/500\n",
            "144/144 [==============================] - 4s 31ms/step - loss: 0.6656 - val_loss: 0.6645\n",
            "Epoch 54/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6655 - val_loss: 0.6650\n",
            "Epoch 55/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6657 - val_loss: 0.6651\n",
            "Epoch 56/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6654 - val_loss: 0.6648\n",
            "Epoch 57/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6655 - val_loss: 0.6646\n",
            "Epoch 58/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6655 - val_loss: 0.6647\n",
            "Epoch 59/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6653 - val_loss: 0.6646\n",
            "Epoch 60/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6653 - val_loss: 0.6648\n",
            "Epoch 61/500\n",
            "144/144 [==============================] - 3s 18ms/step - loss: 0.6653 - val_loss: 0.6645\n",
            "385/385 [==============================] - 2s 3ms/step\n",
            "Predicted Class Labels: [0 1 0 ... 0 1 1]\n",
            "Accuracy: 0.2348780487804878\n",
            "Precision: 0.23479165491387918\n",
            "Recall: 0.2348780487804878\n",
            "F-score: 0.23481573246506512\n",
            "ROC AUC: 0.2348780487804878\n",
            "False Positive Rate (FPR): 0.7560975609756098\n",
            "False Negative Rate (FNR): 0.7741463414634147\n",
            "TN: 1500 FP: 4650 FN: 4761 TP: 1389\n",
            "Units: 288 Batch Size: 48 Dropout Rate: 0.1\n",
            "Epoch 1/500\n",
            "479/479 [==============================] - 10s 13ms/step - loss: 0.6747 - val_loss: 0.6706\n",
            "Epoch 2/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6693 - val_loss: 0.6686\n",
            "Epoch 3/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6691 - val_loss: 0.6683\n",
            "Epoch 4/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6689 - val_loss: 0.6676\n",
            "Epoch 5/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6688 - val_loss: 0.6677\n",
            "Epoch 6/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6687 - val_loss: 0.6675\n",
            "Epoch 7/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6687 - val_loss: 0.6672\n",
            "Epoch 8/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6684 - val_loss: 0.6682\n",
            "Epoch 9/500\n",
            "479/479 [==============================] - 7s 16ms/step - loss: 0.6683 - val_loss: 0.6683\n",
            "Epoch 10/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6680 - val_loss: 0.6670\n",
            "Epoch 11/500\n",
            "479/479 [==============================] - 9s 19ms/step - loss: 0.6677 - val_loss: 0.6668\n",
            "Epoch 12/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6680 - val_loss: 0.6663\n",
            "Epoch 13/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6676 - val_loss: 0.6660\n",
            "Epoch 14/500\n",
            "479/479 [==============================] - 6s 14ms/step - loss: 0.6675 - val_loss: 0.6658\n",
            "Epoch 15/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6673 - val_loss: 0.6655\n",
            "Epoch 16/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6674 - val_loss: 0.6654\n",
            "Epoch 17/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6669 - val_loss: 0.6659\n",
            "Epoch 18/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6671 - val_loss: 0.6658\n",
            "Epoch 19/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6668 - val_loss: 0.6664\n",
            "Epoch 20/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6669 - val_loss: 0.6666\n",
            "Epoch 21/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6666 - val_loss: 0.6652\n",
            "Epoch 22/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6671 - val_loss: 0.6653\n",
            "Epoch 23/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6664 - val_loss: 0.6653\n",
            "Epoch 24/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6666 - val_loss: 0.6650\n",
            "Epoch 25/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6663 - val_loss: 0.6650\n",
            "Epoch 26/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6663 - val_loss: 0.6658\n",
            "Epoch 27/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6663 - val_loss: 0.6656\n",
            "Epoch 28/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6663 - val_loss: 0.6651\n",
            "Epoch 29/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6658 - val_loss: 0.6659\n",
            "Epoch 30/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6664 - val_loss: 0.6648\n",
            "Epoch 31/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6656 - val_loss: 0.6656\n",
            "Epoch 32/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6658 - val_loss: 0.6649\n",
            "Epoch 33/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6658 - val_loss: 0.6646\n",
            "Epoch 34/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6655 - val_loss: 0.6649\n",
            "Epoch 35/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6655 - val_loss: 0.6651\n",
            "Epoch 36/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6653 - val_loss: 0.6651\n",
            "Epoch 37/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6656 - val_loss: 0.6660\n",
            "Epoch 38/500\n",
            "479/479 [==============================] - 6s 14ms/step - loss: 0.6655 - val_loss: 0.6659\n",
            "Epoch 39/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6652 - val_loss: 0.6647\n",
            "Epoch 40/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6653 - val_loss: 0.6648\n",
            "Epoch 41/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6654 - val_loss: 0.6652\n",
            "Epoch 42/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6652 - val_loss: 0.6647\n",
            "Epoch 43/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6652 - val_loss: 0.6652\n",
            "385/385 [==============================] - 3s 5ms/step\n",
            "Predicted Class Labels: [0 1 0 ... 0 1 1]\n",
            "Accuracy: 0.23536585365853657\n",
            "Precision: 0.234704770703524\n",
            "Recall: 0.23536585365853657\n",
            "F-score: 0.23488921325573497\n",
            "ROC AUC: 0.23536585365853663\n",
            "False Positive Rate (FPR): 0.7396747967479674\n",
            "False Negative Rate (FNR): 0.7895934959349593\n",
            "TN: 1601 FP: 4549 FN: 4856 TP: 1294\n",
            "Units: 288 Batch Size: 64 Dropout Rate: 0.1\n",
            "Epoch 1/500\n",
            "359/359 [==============================] - 8s 17ms/step - loss: 0.6744 - val_loss: 0.6679\n",
            "Epoch 2/500\n",
            "359/359 [==============================] - 5s 15ms/step - loss: 0.6687 - val_loss: 0.6679\n",
            "Epoch 3/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6689 - val_loss: 0.6694\n",
            "Epoch 4/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6688 - val_loss: 0.6677\n",
            "Epoch 5/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6687 - val_loss: 0.6678\n",
            "Epoch 6/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6687 - val_loss: 0.6676\n",
            "Epoch 7/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6686 - val_loss: 0.6698\n",
            "Epoch 8/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6685 - val_loss: 0.6675\n",
            "Epoch 9/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6682 - val_loss: 0.6676\n",
            "Epoch 10/500\n",
            "359/359 [==============================] - 6s 17ms/step - loss: 0.6684 - val_loss: 0.6673\n",
            "Epoch 11/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6683 - val_loss: 0.6674\n",
            "Epoch 12/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6680 - val_loss: 0.6673\n",
            "Epoch 13/500\n",
            "359/359 [==============================] - 6s 18ms/step - loss: 0.6677 - val_loss: 0.6667\n",
            "Epoch 14/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6675 - val_loss: 0.6665\n",
            "Epoch 15/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6674 - val_loss: 0.6658\n",
            "Epoch 16/500\n",
            "359/359 [==============================] - 6s 15ms/step - loss: 0.6669 - val_loss: 0.6656\n",
            "Epoch 17/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6669 - val_loss: 0.6666\n",
            "Epoch 18/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6669 - val_loss: 0.6661\n",
            "Epoch 19/500\n",
            "359/359 [==============================] - 5s 15ms/step - loss: 0.6668 - val_loss: 0.6653\n",
            "Epoch 20/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6668 - val_loss: 0.6652\n",
            "Epoch 21/500\n",
            "359/359 [==============================] - 6s 18ms/step - loss: 0.6667 - val_loss: 0.6663\n",
            "Epoch 22/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6668 - val_loss: 0.6650\n",
            "Epoch 23/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6666 - val_loss: 0.6657\n",
            "Epoch 24/500\n",
            "359/359 [==============================] - 6s 17ms/step - loss: 0.6667 - val_loss: 0.6651\n",
            "Epoch 25/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6665 - val_loss: 0.6656\n",
            "Epoch 26/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6664 - val_loss: 0.6652\n",
            "Epoch 27/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6663 - val_loss: 0.6654\n",
            "Epoch 28/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6662 - val_loss: 0.6650\n",
            "Epoch 29/500\n",
            "359/359 [==============================] - 5s 15ms/step - loss: 0.6660 - val_loss: 0.6654\n",
            "Epoch 30/500\n",
            "359/359 [==============================] - 6s 17ms/step - loss: 0.6662 - val_loss: 0.6650\n",
            "Epoch 31/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6659 - val_loss: 0.6647\n",
            "Epoch 32/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6658 - val_loss: 0.6656\n",
            "Epoch 33/500\n",
            "359/359 [==============================] - 5s 15ms/step - loss: 0.6659 - val_loss: 0.6646\n",
            "Epoch 34/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6661 - val_loss: 0.6655\n",
            "Epoch 35/500\n",
            "359/359 [==============================] - 6s 17ms/step - loss: 0.6659 - val_loss: 0.6652\n",
            "Epoch 36/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6656 - val_loss: 0.6656\n",
            "Epoch 37/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6658 - val_loss: 0.6646\n",
            "Epoch 38/500\n",
            "359/359 [==============================] - 6s 18ms/step - loss: 0.6657 - val_loss: 0.6655\n",
            "Epoch 39/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6655 - val_loss: 0.6646\n",
            "Epoch 40/500\n",
            "359/359 [==============================] - 7s 19ms/step - loss: 0.6654 - val_loss: 0.6647\n",
            "Epoch 41/500\n",
            "359/359 [==============================] - 6s 15ms/step - loss: 0.6653 - val_loss: 0.6645\n",
            "Epoch 42/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6651 - val_loss: 0.6655\n",
            "Epoch 43/500\n",
            "359/359 [==============================] - 6s 15ms/step - loss: 0.6652 - val_loss: 0.6650\n",
            "Epoch 44/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6653 - val_loss: 0.6645\n",
            "Epoch 45/500\n",
            "359/359 [==============================] - 5s 15ms/step - loss: 0.6652 - val_loss: 0.6645\n",
            "Epoch 46/500\n",
            "359/359 [==============================] - 8s 23ms/step - loss: 0.6651 - val_loss: 0.6643\n",
            "Epoch 47/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6651 - val_loss: 0.6647\n",
            "Epoch 48/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6650 - val_loss: 0.6642\n",
            "Epoch 49/500\n",
            "359/359 [==============================] - 6s 17ms/step - loss: 0.6649 - val_loss: 0.6644\n",
            "Epoch 50/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6648 - val_loss: 0.6647\n",
            "Epoch 51/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6649 - val_loss: 0.6647\n",
            "Epoch 52/500\n",
            "359/359 [==============================] - 7s 18ms/step - loss: 0.6648 - val_loss: 0.6642\n",
            "Epoch 53/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6649 - val_loss: 0.6643\n",
            "Epoch 54/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6648 - val_loss: 0.6649\n",
            "Epoch 55/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6648 - val_loss: 0.6643\n",
            "Epoch 56/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6648 - val_loss: 0.6646\n",
            "Epoch 57/500\n",
            "359/359 [==============================] - 5s 15ms/step - loss: 0.6645 - val_loss: 0.6640\n",
            "Epoch 58/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6643 - val_loss: 0.6648\n",
            "Epoch 59/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6646 - val_loss: 0.6641\n",
            "Epoch 60/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6645 - val_loss: 0.6642\n",
            "Epoch 61/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6643 - val_loss: 0.6644\n",
            "Epoch 62/500\n",
            "359/359 [==============================] - 4s 12ms/step - loss: 0.6643 - val_loss: 0.6645\n",
            "Epoch 63/500\n",
            "359/359 [==============================] - 6s 16ms/step - loss: 0.6641 - val_loss: 0.6649\n",
            "Epoch 64/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6643 - val_loss: 0.6639\n",
            "Epoch 65/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6642 - val_loss: 0.6642\n",
            "Epoch 66/500\n",
            "359/359 [==============================] - 6s 17ms/step - loss: 0.6642 - val_loss: 0.6647\n",
            "Epoch 67/500\n",
            "359/359 [==============================] - 5s 14ms/step - loss: 0.6644 - val_loss: 0.6642\n",
            "Epoch 68/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6639 - val_loss: 0.6642\n",
            "Epoch 69/500\n",
            "359/359 [==============================] - 6s 17ms/step - loss: 0.6638 - val_loss: 0.6644\n",
            "Epoch 70/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6637 - val_loss: 0.6642\n",
            "Epoch 71/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6637 - val_loss: 0.6648\n",
            "Epoch 72/500\n",
            "359/359 [==============================] - 6s 17ms/step - loss: 0.6632 - val_loss: 0.6639\n",
            "Epoch 73/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6635 - val_loss: 0.6642\n",
            "Epoch 74/500\n",
            "359/359 [==============================] - 5s 13ms/step - loss: 0.6634 - val_loss: 0.6651\n",
            "385/385 [==============================] - 2s 4ms/step\n",
            "Predicted Class Labels: [0 1 0 ... 0 1 0]\n",
            "Accuracy: 0.2202439024390244\n",
            "Precision: 0.21919988057919096\n",
            "Recall: 0.2202439024390244\n",
            "F-score: 0.2195184399538536\n",
            "ROC AUC: 0.22024390243902436\n",
            "False Positive Rate (FPR): 0.7492682926829268\n",
            "False Negative Rate (FNR): 0.8102439024390244\n",
            "TN: 1542 FP: 4608 FN: 4983 TP: 1167\n",
            "gen\tnevals\tavg     \tmin     \tmax     \n",
            "0  \t4     \t0.230772\t0.220244\t0.235366\n",
            "Units: 288 Batch Size: 48 Dropout Rate: 0.1\n",
            "Epoch 1/500\n",
            "479/479 [==============================] - 10s 13ms/step - loss: 0.6738 - val_loss: 0.6683\n",
            "Epoch 2/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6691 - val_loss: 0.6680\n",
            "Epoch 3/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6689 - val_loss: 0.6687\n",
            "Epoch 4/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6688 - val_loss: 0.6676\n",
            "Epoch 5/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6688 - val_loss: 0.6681\n",
            "Epoch 6/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6686 - val_loss: 0.6681\n",
            "Epoch 7/500\n",
            "479/479 [==============================] - 7s 16ms/step - loss: 0.6684 - val_loss: 0.6674\n",
            "Epoch 8/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6687 - val_loss: 0.6678\n",
            "Epoch 9/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6683 - val_loss: 0.6673\n",
            "Epoch 10/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6682 - val_loss: 0.6679\n",
            "Epoch 11/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6683 - val_loss: 0.6668\n",
            "Epoch 12/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6680 - val_loss: 0.6665\n",
            "Epoch 13/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6675 - val_loss: 0.6663\n",
            "Epoch 14/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6675 - val_loss: 0.6672\n",
            "Epoch 15/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6671 - val_loss: 0.6660\n",
            "Epoch 16/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6671 - val_loss: 0.6666\n",
            "Epoch 17/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6668 - val_loss: 0.6656\n",
            "Epoch 18/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6669 - val_loss: 0.6657\n",
            "Epoch 19/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6668 - val_loss: 0.6654\n",
            "Epoch 20/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6668 - val_loss: 0.6654\n",
            "Epoch 21/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6667 - val_loss: 0.6650\n",
            "Epoch 22/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6665 - val_loss: 0.6651\n",
            "Epoch 23/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6665 - val_loss: 0.6652\n",
            "Epoch 24/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6664 - val_loss: 0.6650\n",
            "Epoch 25/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6664 - val_loss: 0.6649\n",
            "Epoch 26/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6661 - val_loss: 0.6651\n",
            "Epoch 27/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6664 - val_loss: 0.6650\n",
            "Epoch 28/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6664 - val_loss: 0.6645\n",
            "Epoch 29/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6659 - val_loss: 0.6652\n",
            "Epoch 30/500\n",
            "479/479 [==============================] - 8s 17ms/step - loss: 0.6661 - val_loss: 0.6649\n",
            "Epoch 31/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6659 - val_loss: 0.6655\n",
            "Epoch 32/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6657 - val_loss: 0.6656\n",
            "Epoch 33/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6657 - val_loss: 0.6650\n",
            "Epoch 34/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6658 - val_loss: 0.6652\n",
            "Epoch 35/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6656 - val_loss: 0.6651\n",
            "Epoch 36/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6656 - val_loss: 0.6647\n",
            "Epoch 37/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6656 - val_loss: 0.6647\n",
            "Epoch 38/500\n",
            "479/479 [==============================] - 6s 12ms/step - loss: 0.6654 - val_loss: 0.6648\n",
            "385/385 [==============================] - 2s 3ms/step\n",
            "Predicted Class Labels: [0 1 0 ... 0 1 1]\n",
            "Accuracy: 0.2348780487804878\n",
            "Precision: 0.23484657879019605\n",
            "Recall: 0.2348780487804878\n",
            "F-score: 0.2348553457971266\n",
            "ROC AUC: 0.23487804878048782\n",
            "False Positive Rate (FPR): 0.7596747967479675\n",
            "False Negative Rate (FNR): 0.770569105691057\n",
            "TN: 1478 FP: 4672 FN: 4739 TP: 1411\n",
            "Units: 288 Batch Size: 48 Dropout Rate: 0.1\n",
            "Epoch 1/500\n",
            "479/479 [==============================] - 11s 16ms/step - loss: 0.6726 - val_loss: 0.6687\n",
            "Epoch 2/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6693 - val_loss: 0.6682\n",
            "Epoch 3/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6689 - val_loss: 0.6679\n",
            "Epoch 4/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6691 - val_loss: 0.6685\n",
            "Epoch 5/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6687 - val_loss: 0.6676\n",
            "Epoch 6/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6688 - val_loss: 0.6673\n",
            "Epoch 7/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6688 - val_loss: 0.6676\n",
            "Epoch 8/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6686 - val_loss: 0.6672\n",
            "Epoch 9/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6681 - val_loss: 0.6667\n",
            "Epoch 10/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6680 - val_loss: 0.6671\n",
            "Epoch 11/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6678 - val_loss: 0.6664\n",
            "Epoch 12/500\n",
            "479/479 [==============================] - 8s 17ms/step - loss: 0.6675 - val_loss: 0.6668\n",
            "Epoch 13/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6674 - val_loss: 0.6664\n",
            "Epoch 14/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6673 - val_loss: 0.6659\n",
            "Epoch 15/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6672 - val_loss: 0.6660\n",
            "Epoch 16/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6670 - val_loss: 0.6652\n",
            "Epoch 17/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6668 - val_loss: 0.6651\n",
            "Epoch 18/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6670 - val_loss: 0.6653\n",
            "Epoch 19/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6669 - val_loss: 0.6653\n",
            "Epoch 20/500\n",
            "479/479 [==============================] - 8s 18ms/step - loss: 0.6668 - val_loss: 0.6676\n",
            "Epoch 21/500\n",
            "479/479 [==============================] - 9s 19ms/step - loss: 0.6669 - val_loss: 0.6654\n",
            "Epoch 22/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6667 - val_loss: 0.6651\n",
            "Epoch 23/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6665 - val_loss: 0.6658\n",
            "Epoch 24/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6662 - val_loss: 0.6656\n",
            "Epoch 25/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6661 - val_loss: 0.6663\n",
            "Epoch 26/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6663 - val_loss: 0.6652\n",
            "Epoch 27/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6661 - val_loss: 0.6650\n",
            "Epoch 28/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6660 - val_loss: 0.6651\n",
            "Epoch 29/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6663 - val_loss: 0.6657\n",
            "Epoch 30/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6658 - val_loss: 0.6653\n",
            "Epoch 31/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6659 - val_loss: 0.6648\n",
            "Epoch 32/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6658 - val_loss: 0.6658\n",
            "Epoch 33/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6657 - val_loss: 0.6647\n",
            "Epoch 34/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6658 - val_loss: 0.6657\n",
            "Epoch 35/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6654 - val_loss: 0.6652\n",
            "Epoch 36/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6656 - val_loss: 0.6661\n",
            "Epoch 37/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6655 - val_loss: 0.6648\n",
            "Epoch 38/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6652 - val_loss: 0.6645\n",
            "Epoch 39/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6654 - val_loss: 0.6646\n",
            "Epoch 40/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6655 - val_loss: 0.6656\n",
            "Epoch 41/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6653 - val_loss: 0.6645\n",
            "Epoch 42/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6652 - val_loss: 0.6650\n",
            "Epoch 43/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6655 - val_loss: 0.6651\n",
            "Epoch 44/500\n",
            "479/479 [==============================] - 8s 16ms/step - loss: 0.6653 - val_loss: 0.6655\n",
            "Epoch 45/500\n",
            "479/479 [==============================] - 6s 14ms/step - loss: 0.6652 - val_loss: 0.6656\n",
            "Epoch 46/500\n",
            "479/479 [==============================] - 7s 16ms/step - loss: 0.6652 - val_loss: 0.6652\n",
            "Epoch 47/500\n",
            "479/479 [==============================] - 6s 13ms/step - loss: 0.6654 - val_loss: 0.6649\n",
            "Epoch 48/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6654 - val_loss: 0.6649\n",
            "Epoch 49/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6651 - val_loss: 0.6661\n",
            "Epoch 50/500\n",
            "479/479 [==============================] - 7s 14ms/step - loss: 0.6652 - val_loss: 0.6650\n",
            "Epoch 51/500\n",
            "479/479 [==============================] - 7s 15ms/step - loss: 0.6653 - val_loss: 0.6675\n",
            "385/385 [==============================] - 2s 3ms/step\n",
            "Predicted Class Labels: [0 1 0 ... 0 1 0]\n",
            "Accuracy: 0.24357723577235774\n",
            "Precision: 0.23757630365608157\n",
            "Recall: 0.24357723577235774\n",
            "F-score: 0.2392280277693018\n",
            "ROC AUC: 0.24357723577235774\n",
            "False Positive Rate (FPR): 0.6808130081300813\n",
            "False Negative Rate (FNR): 0.8320325203252033\n",
            "TN: 1963 FP: 4187 FN: 5117 TP: 1033\n",
            "Units: 288 Batch Size: 160 Dropout Rate: 0.1\n",
            "Epoch 1/500\n",
            "144/144 [==============================] - 6s 26ms/step - loss: 0.6772 - val_loss: 0.6702\n",
            "Epoch 2/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6692 - val_loss: 0.6681\n",
            "Epoch 3/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6689 - val_loss: 0.6678\n",
            "Epoch 4/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6686 - val_loss: 0.6678\n",
            "Epoch 5/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6685 - val_loss: 0.6678\n",
            "Epoch 6/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6685 - val_loss: 0.6677\n",
            "Epoch 7/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6685 - val_loss: 0.6676\n",
            "Epoch 8/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6684 - val_loss: 0.6676\n",
            "Epoch 9/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6684 - val_loss: 0.6676\n",
            "Epoch 10/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6683 - val_loss: 0.6675\n",
            "Epoch 11/500\n",
            "144/144 [==============================] - 4s 30ms/step - loss: 0.6686 - val_loss: 0.6684\n",
            "Epoch 12/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6682 - val_loss: 0.6676\n",
            "Epoch 13/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6684 - val_loss: 0.6681\n",
            "Epoch 14/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6683 - val_loss: 0.6673\n",
            "Epoch 15/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6682 - val_loss: 0.6676\n",
            "Epoch 16/500\n",
            "144/144 [==============================] - 4s 31ms/step - loss: 0.6681 - val_loss: 0.6672\n",
            "Epoch 17/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6680 - val_loss: 0.6671\n",
            "Epoch 18/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6679 - val_loss: 0.6670\n",
            "Epoch 19/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6676 - val_loss: 0.6669\n",
            "Epoch 20/500\n",
            "144/144 [==============================] - 3s 24ms/step - loss: 0.6677 - val_loss: 0.6664\n",
            "Epoch 21/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6674 - val_loss: 0.6668\n",
            "Epoch 22/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6673 - val_loss: 0.6659\n",
            "Epoch 23/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6672 - val_loss: 0.6665\n",
            "Epoch 24/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6672 - val_loss: 0.6658\n",
            "Epoch 25/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6667 - val_loss: 0.6659\n",
            "Epoch 26/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6667 - val_loss: 0.6653\n",
            "Epoch 27/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6664 - val_loss: 0.6654\n",
            "Epoch 28/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6665 - val_loss: 0.6655\n",
            "Epoch 29/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6665 - val_loss: 0.6651\n",
            "Epoch 30/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6664 - val_loss: 0.6655\n",
            "Epoch 31/500\n",
            "144/144 [==============================] - 3s 24ms/step - loss: 0.6664 - val_loss: 0.6653\n",
            "Epoch 32/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6662 - val_loss: 0.6649\n",
            "Epoch 33/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6663 - val_loss: 0.6653\n",
            "Epoch 34/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6662 - val_loss: 0.6653\n",
            "Epoch 35/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6661 - val_loss: 0.6658\n",
            "Epoch 36/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6663 - val_loss: 0.6649\n",
            "Epoch 37/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6660 - val_loss: 0.6649\n",
            "Epoch 38/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6659 - val_loss: 0.6652\n",
            "Epoch 39/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6660 - val_loss: 0.6652\n",
            "Epoch 40/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6660 - val_loss: 0.6647\n",
            "Epoch 41/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6659 - val_loss: 0.6649\n",
            "Epoch 42/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6657 - val_loss: 0.6651\n",
            "Epoch 43/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6657 - val_loss: 0.6648\n",
            "Epoch 44/500\n",
            "144/144 [==============================] - 3s 23ms/step - loss: 0.6657 - val_loss: 0.6650\n",
            "Epoch 45/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6656 - val_loss: 0.6650\n",
            "Epoch 46/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6655 - val_loss: 0.6650\n",
            "Epoch 47/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6655 - val_loss: 0.6646\n",
            "Epoch 48/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6656 - val_loss: 0.6656\n",
            "Epoch 49/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6655 - val_loss: 0.6646\n",
            "Epoch 50/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6653 - val_loss: 0.6652\n",
            "Epoch 51/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6654 - val_loss: 0.6664\n",
            "Epoch 52/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6654 - val_loss: 0.6646\n",
            "Epoch 53/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6653 - val_loss: 0.6649\n",
            "Epoch 54/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6652 - val_loss: 0.6655\n",
            "Epoch 55/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6655 - val_loss: 0.6647\n",
            "Epoch 56/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6654 - val_loss: 0.6647\n",
            "Epoch 57/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6650 - val_loss: 0.6652\n",
            "385/385 [==============================] - 2s 3ms/step\n",
            "Predicted Class Labels: [0 1 0 ... 0 1 1]\n",
            "Accuracy: 0.23772357723577237\n",
            "Precision: 0.23676044304714486\n",
            "Recall: 0.23772357723577237\n",
            "F-score: 0.2370256894904782\n",
            "ROC AUC: 0.23772357723577237\n",
            "False Positive Rate (FPR): 0.7320325203252033\n",
            "False Negative Rate (FNR): 0.792520325203252\n",
            "TN: 1648 FP: 4502 FN: 4874 TP: 1276\n",
            "Units: 288 Batch Size: 160 Dropout Rate: 0.1\n",
            "Epoch 1/500\n",
            "144/144 [==============================] - 8s 33ms/step - loss: 0.6781 - val_loss: 0.6692\n",
            "Epoch 2/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6692 - val_loss: 0.6684\n",
            "Epoch 3/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6687 - val_loss: 0.6680\n",
            "Epoch 4/500\n",
            "144/144 [==============================] - 4s 30ms/step - loss: 0.6686 - val_loss: 0.6679\n",
            "Epoch 5/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6686 - val_loss: 0.6678\n",
            "Epoch 6/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6686 - val_loss: 0.6678\n",
            "Epoch 7/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6685 - val_loss: 0.6678\n",
            "Epoch 8/500\n",
            "144/144 [==============================] - 3s 24ms/step - loss: 0.6684 - val_loss: 0.6683\n",
            "Epoch 9/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6685 - val_loss: 0.6675\n",
            "Epoch 10/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6685 - val_loss: 0.6676\n",
            "Epoch 11/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6682 - val_loss: 0.6678\n",
            "Epoch 12/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6679 - val_loss: 0.6667\n",
            "Epoch 13/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6676 - val_loss: 0.6672\n",
            "Epoch 14/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6674 - val_loss: 0.6665\n",
            "Epoch 15/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6672 - val_loss: 0.6659\n",
            "Epoch 16/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6674 - val_loss: 0.6658\n",
            "Epoch 17/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6672 - val_loss: 0.6665\n",
            "Epoch 18/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6669 - val_loss: 0.6661\n",
            "Epoch 19/500\n",
            "144/144 [==============================] - 3s 23ms/step - loss: 0.6672 - val_loss: 0.6655\n",
            "Epoch 20/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6670 - val_loss: 0.6655\n",
            "Epoch 21/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6667 - val_loss: 0.6666\n",
            "Epoch 22/500\n",
            "144/144 [==============================] - 3s 23ms/step - loss: 0.6669 - val_loss: 0.6654\n",
            "Epoch 23/500\n",
            "144/144 [==============================] - 5s 32ms/step - loss: 0.6669 - val_loss: 0.6665\n",
            "Epoch 24/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6669 - val_loss: 0.6653\n",
            "Epoch 25/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6667 - val_loss: 0.6652\n",
            "Epoch 26/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6667 - val_loss: 0.6658\n",
            "Epoch 27/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6668 - val_loss: 0.6659\n",
            "Epoch 28/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6663 - val_loss: 0.6659\n",
            "Epoch 29/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6666 - val_loss: 0.6654\n",
            "Epoch 30/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6666 - val_loss: 0.6650\n",
            "Epoch 31/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6665 - val_loss: 0.6657\n",
            "Epoch 32/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6664 - val_loss: 0.6656\n",
            "Epoch 33/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6661 - val_loss: 0.6659\n",
            "Epoch 34/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6664 - val_loss: 0.6649\n",
            "Epoch 35/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6664 - val_loss: 0.6653\n",
            "Epoch 36/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6663 - val_loss: 0.6654\n",
            "Epoch 37/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6661 - val_loss: 0.6648\n",
            "Epoch 38/500\n",
            "144/144 [==============================] - 3s 24ms/step - loss: 0.6661 - val_loss: 0.6653\n",
            "Epoch 39/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6661 - val_loss: 0.6650\n",
            "Epoch 40/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6658 - val_loss: 0.6649\n",
            "Epoch 41/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6658 - val_loss: 0.6652\n",
            "Epoch 42/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6659 - val_loss: 0.6649\n",
            "Epoch 43/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6657 - val_loss: 0.6646\n",
            "Epoch 44/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6656 - val_loss: 0.6654\n",
            "Epoch 45/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6658 - val_loss: 0.6650\n",
            "Epoch 46/500\n",
            "144/144 [==============================] - 3s 23ms/step - loss: 0.6655 - val_loss: 0.6658\n",
            "Epoch 47/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6657 - val_loss: 0.6650\n",
            "Epoch 48/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6656 - val_loss: 0.6653\n",
            "Epoch 49/500\n",
            "144/144 [==============================] - 3s 19ms/step - loss: 0.6656 - val_loss: 0.6646\n",
            "Epoch 50/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6653 - val_loss: 0.6648\n",
            "Epoch 51/500\n",
            "144/144 [==============================] - 4s 24ms/step - loss: 0.6653 - val_loss: 0.6648\n",
            "Epoch 52/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6653 - val_loss: 0.6645\n",
            "Epoch 53/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6654 - val_loss: 0.6651\n",
            "Epoch 54/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6653 - val_loss: 0.6650\n",
            "Epoch 55/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6651 - val_loss: 0.6647\n",
            "Epoch 56/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6651 - val_loss: 0.6648\n",
            "Epoch 57/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6652 - val_loss: 0.6646\n",
            "Epoch 58/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6650 - val_loss: 0.6645\n",
            "Epoch 59/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6651 - val_loss: 0.6648\n",
            "Epoch 60/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6651 - val_loss: 0.6646\n",
            "Epoch 61/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6651 - val_loss: 0.6646\n",
            "Epoch 62/500\n",
            "144/144 [==============================] - 3s 24ms/step - loss: 0.6651 - val_loss: 0.6646\n",
            "385/385 [==============================] - 2s 4ms/step\n",
            "Predicted Class Labels: [0 1 0 ... 0 1 1]\n",
            "Accuracy: 0.23414634146341465\n",
            "Precision: 0.2335456682625087\n",
            "Recall: 0.23414634146341465\n",
            "F-score: 0.23371447836131273\n",
            "ROC AUC: 0.23414634146341465\n",
            "False Positive Rate (FPR): 0.7895934959349593\n",
            "False Negative Rate (FNR): 0.7421138211382113\n",
            "TN: 1294 FP: 4856 FN: 4564 TP: 1586\n",
            "1  \t4     \t0.237581\t0.234146\t0.243577\n",
            "Units: 288 Batch Size: 160 Dropout Rate: 0.1\n",
            "Epoch 1/500\n",
            "144/144 [==============================] - 7s 24ms/step - loss: 0.6758 - val_loss: 0.6692\n",
            "Epoch 2/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6693 - val_loss: 0.6682\n",
            "Epoch 3/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6688 - val_loss: 0.6681\n",
            "Epoch 4/500\n",
            "144/144 [==============================] - 5s 33ms/step - loss: 0.6688 - val_loss: 0.6678\n",
            "Epoch 5/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6685 - val_loss: 0.6677\n",
            "Epoch 6/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6686 - val_loss: 0.6681\n",
            "Epoch 7/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6687 - val_loss: 0.6680\n",
            "Epoch 8/500\n",
            "144/144 [==============================] - 3s 24ms/step - loss: 0.6684 - val_loss: 0.6677\n",
            "Epoch 9/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6684 - val_loss: 0.6675\n",
            "Epoch 10/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6683 - val_loss: 0.6669\n",
            "Epoch 11/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6679 - val_loss: 0.6667\n",
            "Epoch 12/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6676 - val_loss: 0.6667\n",
            "Epoch 13/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6674 - val_loss: 0.6676\n",
            "Epoch 14/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6677 - val_loss: 0.6658\n",
            "Epoch 15/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6674 - val_loss: 0.6656\n",
            "Epoch 16/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6673 - val_loss: 0.6666\n",
            "Epoch 17/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6672 - val_loss: 0.6655\n",
            "Epoch 18/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6669 - val_loss: 0.6654\n",
            "Epoch 19/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6669 - val_loss: 0.6655\n",
            "Epoch 20/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6671 - val_loss: 0.6654\n",
            "Epoch 21/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6668 - val_loss: 0.6651\n",
            "Epoch 22/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6668 - val_loss: 0.6662\n",
            "Epoch 23/500\n",
            "144/144 [==============================] - 5s 32ms/step - loss: 0.6669 - val_loss: 0.6653\n",
            "Epoch 24/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6668 - val_loss: 0.6651\n",
            "Epoch 25/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6668 - val_loss: 0.6653\n",
            "Epoch 26/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6665 - val_loss: 0.6659\n",
            "Epoch 27/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6671 - val_loss: 0.6650\n",
            "Epoch 28/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6665 - val_loss: 0.6649\n",
            "Epoch 29/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6664 - val_loss: 0.6653\n",
            "Epoch 30/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6665 - val_loss: 0.6660\n",
            "Epoch 31/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6664 - val_loss: 0.6656\n",
            "Epoch 32/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6665 - val_loss: 0.6651\n",
            "Epoch 33/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6663 - val_loss: 0.6662\n",
            "Epoch 34/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6662 - val_loss: 0.6649\n",
            "Epoch 35/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6662 - val_loss: 0.6652\n",
            "Epoch 36/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6662 - val_loss: 0.6653\n",
            "Epoch 37/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6660 - val_loss: 0.6650\n",
            "Epoch 38/500\n",
            "144/144 [==============================] - 3s 22ms/step - loss: 0.6662 - val_loss: 0.6647\n",
            "Epoch 39/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6659 - val_loss: 0.6647\n",
            "Epoch 40/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6660 - val_loss: 0.6657\n",
            "Epoch 41/500\n",
            "144/144 [==============================] - 3s 24ms/step - loss: 0.6660 - val_loss: 0.6645\n",
            "Epoch 42/500\n",
            "144/144 [==============================] - 4s 29ms/step - loss: 0.6657 - val_loss: 0.6650\n",
            "Epoch 43/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6656 - val_loss: 0.6649\n",
            "Epoch 44/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6657 - val_loss: 0.6651\n",
            "Epoch 45/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6655 - val_loss: 0.6645\n",
            "Epoch 46/500\n",
            "144/144 [==============================] - 4s 26ms/step - loss: 0.6656 - val_loss: 0.6646\n",
            "Epoch 47/500\n",
            "144/144 [==============================] - 4s 27ms/step - loss: 0.6653 - val_loss: 0.6653\n",
            "Epoch 48/500\n",
            "144/144 [==============================] - 3s 21ms/step - loss: 0.6654 - val_loss: 0.6645\n",
            "Epoch 49/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6653 - val_loss: 0.6649\n",
            "Epoch 50/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6653 - val_loss: 0.6647\n",
            "Epoch 51/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6651 - val_loss: 0.6648\n",
            "Epoch 52/500\n",
            "144/144 [==============================] - 4s 31ms/step - loss: 0.6652 - val_loss: 0.6646\n",
            "Epoch 53/500\n",
            "144/144 [==============================] - 4s 28ms/step - loss: 0.6651 - val_loss: 0.6647\n",
            "Epoch 54/500\n",
            "144/144 [==============================] - 3s 20ms/step - loss: 0.6652 - val_loss: 0.6647\n",
            "Epoch 55/500\n",
            "144/144 [==============================] - 4s 25ms/step - loss: 0.6652 - val_loss: 0.6646\n",
            "Epoch 56/500\n",
            "119/144 [=======================>......] - ETA: 0s - loss: 0.6649"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"#Google Usgae Data\"\"\"\n",
        "\n",
        "# merged = pd.read_csv('/content/drive/My Drive/sine3.csv',  header=None)\n",
        "# merged = pd.read_csv('/content/drive/My Drive/hyperplane1.csv', header=None)\n",
        "merged = pd.read_csv('/content/drive/My Drive/mixed5.csv', header=None)\n",
        "# merged = pd.read_csv('/content/drive/My Drive/stagger4.csv', header=None)\n",
        "\n",
        "\n",
        "#dataframe.head()\n",
        "#set n one less\n",
        "n=4\n",
        "merged=pd.DataFrame(data=merged)\n",
        "merged.fillna(merged.mean())\n",
        "# merged =merged[~merged.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
        "merged = merged[~merged.isin([np.nan, np.inf, -np.inf]).any(axis=1)]\n",
        "\n",
        "merged=pd.DataFrame(data=merged)\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Normalize all columns of the merged dataframe\n",
        "merged_normalized = scaler.fit_transform(merged.values)\n",
        "\n",
        "# Create a new dataframe with the normalized values\n",
        "merged = pd.DataFrame(merged_normalized, columns=merged.columns)\n",
        "\n",
        "\n",
        "# data = merged\n",
        "# plt.figure(figsize=(16, 6))\n",
        "# # Store heatmap object in a variable to easily access it when you want to include more features (such as title).\n",
        "# # Set the range of values to be displayed on the colormap from -1 to 1, and set the annotation to True to display the correlation values on the heatmap.\n",
        "# heatmap = sb.heatmap(data.corr(), vmin=-1, vmax=1, annot=True)\n",
        "# # Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.\n",
        "# heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);\n",
        "# # displaying heatmap\n",
        "# mp.show()\n",
        "merged=np.array(merged)\n",
        "y =merged[:,n] #5\n",
        "#  z=merged[:,6]\n",
        "merged= np.delete(merged,[n], axis=1)\n",
        "\n",
        "#56\n",
        "#merged=np.delete(merged,[0,1,2,3x,4,7,8,9,10,11,12,13,14,15,16,17,18], axis=1)\n",
        "#4-14\n",
        "# merged=np.delete(merged,[0,1,2,3,4,15,16,17,18], axis=1)\n",
        "#5-10\n",
        "#  merged=np.delete(merged,[0,1,2,3,4,11,12,13,15,16,17,18], axis=1)\n",
        "\n",
        "\"\"\"#Preprocessing AELSTM \"\"\"\n",
        "\n",
        "labels =y#np.array(y11)# raw_data[:, -1]\n",
        "data = merged\n",
        "\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "data, labels, test_size=0.3, shuffle=False)\n",
        "num_ones = np.count_nonzero(labels)\n",
        "num_zeros = len(labels) - num_ones\n",
        "print(f\"Number of ones: {num_ones}\")\n",
        "print(f\"Number of zeros: {num_zeros}\")\n",
        "# Convert the data to numpy arrays\n",
        "X_train = np.array(train_data)\n",
        "X_test = np.array(test_data)\n",
        "train_labels= np.array(train_labels)\n",
        "# Reshape the data to 3D arrays\n",
        "X_train = np.reshape(X_train, (X_train.shape[0],1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "test_labels=np.reshape(test_labels,(test_labels.shape[0], 1))\n",
        "train_labels=np.reshape(train_labels,(train_labels.shape[0], 1))\n",
        "\n",
        "\"\"\"#Non- Gaussian Code  #LSTM AT HT\"\"\"\n",
        "\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(train_labels)\n",
        "X_test  = np.array(X_test)\n",
        "y_test  = np.array(test_labels)\n",
        "\n",
        "\"\"\"#LSTM AT HY GA\"\"\"\n",
        "\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
        "\n",
        "X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(individual):\n",
        "    # Extract the hyperparameters from the individual\n",
        "    units = individual[0]\n",
        "    batch_size = individual[1]\n",
        "    dropout_rate = individual[2]\n",
        "    if units == 0:\n",
        "        units = 32\n",
        "    if batch_size==0:\n",
        "     batch_size=1\n",
        "    if dropout_rate==0:\n",
        "      dropout_rate=0.1\n",
        "    print(\"Units:\", units, \"Batch Size:\", batch_size, \"Dropout Rate:\", dropout_rate)\n",
        "\n",
        "    # Build the LSTM model with the specified hyperparameters\n",
        "    inputs = Input(shape=(None, n))\n",
        "    lstm_out, state_h, state_c = LSTM(units, return_sequences=True, return_state=True)(inputs)\n",
        "    attention_out = Attention()([lstm_out, lstm_out])\n",
        "\n",
        "    dense_out = Dense(256, activation='relu')(attention_out)\n",
        "    dense_out = Dense(64, activation='relu')(dense_out)\n",
        "    dense_out = Dense(32, activation='relu')(attention_out)\n",
        "    dense_out = Dense(16, activation='relu')(dense_out)\n",
        "    dense_out = Dense(8, activation='relu')(attention_out)\n",
        "    outputs = Dense(1, activation='sigmoid')(dense_out)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train1, y_train1, epochs=500, batch_size=batch_size, shuffle=True,\n",
        "                        validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "    # Make predictions on test data\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_binary = np.round(y_pred).flatten()\n",
        "\n",
        "    # Convert predictions to class labels\n",
        "    y_pred_classes = np.where(y_pred > 0.5, 1, 0)\n",
        "    y_pred_classes = np.squeeze(y_pred_classes)\n",
        "\n",
        "    # Print the predicted class labels\n",
        "    print(\"Predicted Class Labels:\", y_pred_classes)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "    precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred_classes, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_binary)\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F-score:\", f1)\n",
        "    print(\"ROC AUC:\", roc_auc)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_classes).ravel()\n",
        "    fpr = fp / (fp + tn)\n",
        "    fnr = fn / (fn + tp)\n",
        "\n",
        "    print(\"False Positive Rate (FPR):\", fpr)\n",
        "    print(\"False Negative Rate (FNR):\", fnr)\n",
        "    print(\"TN:\", tn, \"FP:\", fp, \"FN:\", fn, \"TP:\", tp)\n",
        "\n",
        "    # Return the fitness value (e.g., accuracy) as a tuple\n",
        "    return accuracy,\n",
        "# Define the individual and population size\n",
        "INDIVIDUAL_SIZE = 3\n",
        "POPULATION_SIZE = 4\n",
        "\n",
        "# Create the individual and population classes\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "# Create the toolbox\n",
        "toolbox = base.Toolbox()\n",
        "\n",
        "# Define the hyperparameter ranges\n",
        "units_range = [32,64, 128,256]\n",
        "batch_size_range = [10,16, 32, 64,128]\n",
        "dropout_rate_range = [0.01,0.1, 0.2, 0.3]\n",
        "# Register the hyperparameters in the toolbox\n",
        "# Define the custom initialization function for units\n",
        "# def init_units():\n",
        "#     return np.random.choice(units_range)\n",
        "def init_units():\n",
        "    return int(np.random.choice(units_range)) + 32\n",
        "\n",
        "# Register the units attribute with the custom initialization function\n",
        "toolbox.register(\"units\", init_units)\n",
        "\n",
        "# toolbox.register(\"units\", np.random.choice, units_range)\n",
        "def init_batch_size_range():\n",
        "    return int(np.random.choice(batch_size_range)) + 32\n",
        "\n",
        "toolbox.register(\"batch_size\", init_batch_size_range)\n",
        "def init_dropout_rate_range():\n",
        "    return int(np.random.choice(dropout_rate_range)) + 0.1\n",
        "toolbox.register(\"dropout_rate\", init_dropout_rate_range)\n",
        "\n",
        "# # Define the individual initialization function\n",
        "# toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
        "#                  (toolbox.units, toolbox.batch_size, toolbox.dropout_rate), n=1)\n",
        "# Define the individual initialization function\n",
        "def init_individual():\n",
        "    return creator.Individual([toolbox.units(), toolbox.batch_size(), toolbox.dropout_rate()])\n",
        "\n",
        "# Register the individual initialization function\n",
        "toolbox.register(\"individual\", init_individual)\n",
        "# Define the population initialization function\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "# Register the evaluation function\n",
        "toolbox.register(\"evaluate\", evaluate_model)\n",
        "\n",
        "# Register the selection, crossover, and mutation operations\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutUniformInt, low=0, up=2, indpb=0.5)\n",
        "\n",
        "# Define the number of generations and probability of crossover/mutation\n",
        "N_GENERATIONS = 20\n",
        "CROSSOVER_PROB = 0.8\n",
        "MUTATION_PROB = 0.2\n",
        "\n",
        "# Create the population\n",
        "population = toolbox.population(n=POPULATION_SIZE)\n",
        "\n",
        "# Create the hall of fame to store the best individuals\n",
        "hof = tools.HallOfFame(maxsize=1)\n",
        "\n",
        "# Define the statistics to collect during evolution\n",
        "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "stats.register(\"avg\", np.mean)\n",
        "stats.register(\"min\", np.min)\n",
        "stats.register(\"max\", np.max)\n",
        "\n",
        "# Run the evolution\n",
        "population, logbook = algorithms.eaSimple(\n",
        "    population,\n",
        "    toolbox,\n",
        "    cxpb=CROSSOVER_PROB,\n",
        "    mutpb=MUTATION_PROB,\n",
        "    ngen=N_GENERATIONS,\n",
        "    stats=stats,\n",
        "    halloffame=hof,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Get the best individual from the hall of fame\n",
        "best_individual = hof[0]\n",
        "\n",
        "# Print the best individual and its fitness value\n",
        "print(\"Best Individual:\", best_individual)\n",
        "print(\"Fitness Value:\", best_individual.fitness.values[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uajrkrt9NQik"
      },
      "source": [
        "#Google Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfVnbzU6jtMV"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "#LSTMDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge49ChYpbXBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea926506-84d1-4810-dd7e-fd46a30b3075"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-4e7cc2da8b60>:26: FutureWarning: In a future version of pandas all arguments of DataFrame.any and Series.any will be keyword-only.\n",
            "  merged =merged[~merged.isin([np.nan, np.inf, -np.inf]).any(1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c=0c1=1779122c2=185961c3=34267c4=0c5=0\n",
            "5067\n",
            "Number of ones: 5067\n",
            "Number of zeros: 1994283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
            "/usr/local/lib/python3.10/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Units: 64 Batch Size: 532 Dropout Rate: 0.1\n",
            "2105/2105 [==============================] - 31s 13ms/step - loss: 0.0387 - val_loss: 0.0078\n",
            "18744/18744 [==============================] - 37s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9969473412192296\n",
            "Precision: 0.3836150845253576\n",
            "Recall: 0.17857142857142858\n",
            "F-score: 0.24370095002065265\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597679    474]\n",
            " [  1357    295]]\n",
            "False Positive Rate (FPR): 0.0007924393925968774\n",
            "False Negative Rate (FNR): 0.8214285714285714\n",
            "TN 597679   FP 474   FN 1357   TP 295\n",
            "Units: 64 Batch Size: 532 Dropout Rate: 0.1\n",
            "2105/2105 [==============================] - 30s 12ms/step - loss: 0.0376 - val_loss: 0.0077\n",
            "18744/18744 [==============================] - 39s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9968923233384184\n",
            "Precision: 0.3738095238095238\n",
            "Recall: 0.1900726392251816\n",
            "F-score: 0.2520064205457464\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597627    526]\n",
            " [  1338    314]]\n",
            "False Positive Rate (FPR): 0.0008793736719534969\n",
            "False Negative Rate (FNR): 0.8099273607748184\n",
            "TN 597627   FP 526   FN 1338   TP 314\n",
            "Units: 96 Batch Size: 48 Dropout Rate: 0.1\n",
            "23326/23326 [==============================] - 125s 5ms/step - loss: 0.0101 - val_loss: 0.0063\n",
            "18744/18744 [==============================] - 39s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9967055959853619\n",
            "Precision: 0.42656391659111514\n",
            "Recall: 0.5696125907990315\n",
            "F-score: 0.487817522032141\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[596888   1265]\n",
            " [   711    941]]\n",
            "False Positive Rate (FPR): 0.0021148435266562233\n",
            "False Negative Rate (FNR): 0.4303874092009685\n",
            "TN 596888   FP 1265   FN 711   TP 941\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 18s 14ms/step - loss: 0.0610 - val_loss: 0.0089\n",
            "18744/18744 [==============================] - 34s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9970156967681163\n",
            "Precision: 0.2553191489361702\n",
            "Recall: 0.043583535108958835\n",
            "F-score: 0.07445708376421922\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597943    210]\n",
            " [  1580     72]]\n",
            "False Positive Rate (FPR): 0.0003510807435555786\n",
            "False Negative Rate (FNR): 0.9564164648910412\n",
            "TN 597943   FP 210   FN 1580   TP 72\n",
            "gen\tnevals\tavg    \tmin     \tmax     \n",
            "0  \t4     \t0.99689\t0.996706\t0.997016\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 19s 15ms/step - loss: 0.0802 - val_loss: 0.0100\n",
            "18744/18744 [==============================] - 35s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.997169079950984\n",
            "Precision: 0.020833333333333332\n",
            "Recall: 0.0006053268765133172\n",
            "F-score: 0.0011764705882352942\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598106     47]\n",
            " [  1651      1]]\n",
            "False Positive Rate (FPR): 7.857521403386759e-05\n",
            "False Negative Rate (FNR): 0.9993946731234867\n",
            "TN 598106   FP 47   FN 1651   TP 1\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 20s 16ms/step - loss: 0.0797 - val_loss: 0.0110\n",
            "18744/18744 [==============================] - 35s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.997245771542418\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F-score: 0.0\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598153      0]\n",
            " [  1652      0]]\n",
            "False Positive Rate (FPR): 0.0\n",
            "False Negative Rate (FNR): 1.0\n",
            "TN 598153   FP 0   FN 1652   TP 0\n",
            "Units: 64 Batch Size: 532 Dropout Rate: 0.1\n",
            "2105/2105 [==============================] - 25s 10ms/step - loss: 0.0447 - val_loss: 0.0079\n",
            "18744/18744 [==============================] - 33s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9970273672276824\n",
            "Precision: 0.3910149750415973\n",
            "Recall: 0.14225181598062955\n",
            "F-score: 0.20861074123391035\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597787    366]\n",
            " [  1417    235]]\n",
            "False Positive Rate (FPR): 0.000611883581625437\n",
            "False Negative Rate (FNR): 0.8577481840193705\n",
            "TN 597787   FP 366   FN 1417   TP 235\n",
            "Units: 32 Batch Size: 2 Dropout Rate: 1\n",
            "559818/559818 [==============================] - 1714s 3ms/step - loss: 0.0069 - val_loss: 0.0055\n",
            "18744/18744 [==============================] - 34s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9967772859512675\n",
            "Precision: 0.44267645858833127\n",
            "Recall: 0.6567796610169492\n",
            "F-score: 0.5288813063611992\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[596787   1366]\n",
            " [   567   1085]]\n",
            "False Positive Rate (FPR): 0.002283696646175811\n",
            "False Negative Rate (FNR): 0.3432203389830508\n",
            "TN 596787   FP 1366   FN 567   TP 1085\n",
            "1  \t4     \t0.997055\t0.996777\t0.997246\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 18s 14ms/step - loss: 0.0546 - val_loss: 0.0086\n",
            "18744/18744 [==============================] - 33s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.997245771542418\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F-score: 0.0\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598153      0]\n",
            " [  1652      0]]\n",
            "False Positive Rate (FPR): 0.0\n",
            "False Negative Rate (FNR): 1.0\n",
            "TN 598153   FP 0   FN 1652   TP 0\n",
            "Units: 64 Batch Size: 532 Dropout Rate: 0.1\n",
            "2105/2105 [==============================] - 23s 10ms/step - loss: 0.0436 - val_loss: 0.0079\n",
            "18744/18744 [==============================] - 34s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9972424371253991\n",
            "Precision: 0.45\n",
            "Recall: 0.005447941888619854\n",
            "F-score: 0.010765550239234449\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598142     11]\n",
            " [  1643      9]]\n",
            "False Positive Rate (FPR): 1.8389943710054116e-05\n",
            "False Negative Rate (FNR): 0.9945520581113801\n",
            "TN 598142   FP 11   FN 1643   TP 9\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 20s 15ms/step - loss: 0.0783 - val_loss: 0.0105\n",
            "18744/18744 [==============================] - 34s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9972157617892482\n",
            "Precision: 0.05\n",
            "Recall: 0.0006053268765133172\n",
            "F-score: 0.001196172248803828\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598134     19]\n",
            " [  1651      1]]\n",
            "False Positive Rate (FPR): 3.176444822645711e-05\n",
            "False Negative Rate (FNR): 0.9993946731234867\n",
            "TN 598134   FP 19   FN 1651   TP 1\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 19s 14ms/step - loss: 0.0630 - val_loss: 0.0089\n",
            "18744/18744 [==============================] - 34s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9970123623510975\n",
            "Precision: 0.23880597014925373\n",
            "Recall: 0.0387409200968523\n",
            "F-score: 0.06666666666666667\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597949    204]\n",
            " [  1588     64]]\n",
            "False Positive Rate (FPR): 0.00034104986516827633\n",
            "False Negative Rate (FNR): 0.9612590799031477\n",
            "TN 597949   FP 204   FN 1588   TP 64\n",
            "2  \t4     \t0.997179\t0.997012\t0.997246\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 2\n",
            "1085/1085 [==============================] - 29s 22ms/step - loss: 0.0633 - val_loss: 0.0087\n",
            "18744/18744 [==============================] - 35s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.996987354223456\n",
            "Precision: 0.29442970822281167\n",
            "Recall: 0.0671912832929782\n",
            "F-score: 0.10941350418925579\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[597887    266]\n",
            " [  1541    111]]\n",
            "False Positive Rate (FPR): 0.00044470227517039954\n",
            "False Negative Rate (FNR): 0.9328087167070218\n",
            "TN 597887   FP 266   FN 1541   TP 111\n",
            "Units: 2 Batch Size: 1032 Dropout Rate: 2\n",
            "1085/1085 [==============================] - 9s 6ms/step - loss: 0.1365 - val_loss: 0.0199\n",
            "18744/18744 [==============================] - 32s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.997245771542418\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F-score: 0.0\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598153      0]\n",
            " [  1652      0]]\n",
            "False Positive Rate (FPR): 0.0\n",
            "False Negative Rate (FNR): 1.0\n",
            "TN 598153   FP 0   FN 1652   TP 0\n",
            "3  \t2     \t0.99718 \t0.996987\t0.997246\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            "1085/1085 [==============================] - 18s 14ms/step - loss: 0.0605 - val_loss: 0.0092\n",
            "18744/18744 [==============================] - 36s 2ms/step\n",
            "y_test: [[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "y_pred_binary: [0. 0. 0. ... 0. 0. 0.]\n",
            "Accuracy: 0.9971157292786822\n",
            "Precision: 0.20454545454545456\n",
            "Recall: 0.016343825665859565\n",
            "F-score: 0.030269058295964126\n",
            "y_test shape: (599805, 1)\n",
            "y_pred_binary shape: (599805,)\n",
            "y_pred_class shape: (599805, 1, 1)\n",
            "y_test data type: int64\n",
            "y_pred_binary data type: float32\n",
            "Confusion matrix: [[598048    105]\n",
            " [  1625     27]]\n",
            "False Positive Rate (FPR): 0.0001755403717777893\n",
            "False Negative Rate (FNR): 0.9836561743341404\n",
            "TN 598048   FP 105   FN 1625   TP 27\n",
            "Units: 64 Batch Size: 1032 Dropout Rate: 0.1\n",
            " 379/1085 [=========>....................] - ETA: 8s - loss: 0.1421"
          ]
        }
      ],
      "source": [
        "#epochs=1000\n",
        "#batchsize=32\n",
        "nooffile=2\n",
        "n=10\n",
        "\n",
        "\n",
        "\"\"\"#Google Usgae Data\"\"\"\n",
        "\n",
        "n=5\n",
        " # Download the dataset\n",
        "merged = pd.read_csv('/content/drive/My Drive/part-00001-of-00500.csv', header=None)\n",
        "#  merged = pd.read_csv('/content/drive/My Drive/mixed_0101_gradual.csv', header='infer')\n",
        "#  merged = pd.read_csv('/content/drive/My Drive/sea4.csv', header='infer')\n",
        " #dataframe.head()\n",
        "# merged = merged.head(500)\n",
        "\n",
        "i=1\n",
        "# while i<nooffile:\n",
        "#   a=pd.read_csv(\"/content/drive/My Drive/part-0000\"+str(i)+\"-of-00500.csv\")\n",
        "#   merged=np.concatenate([merged, a])\n",
        "#   a=0\n",
        "#   i=i+1\n",
        "# del a\n",
        "merged=pd.DataFrame(data=merged)\n",
        "merged.fillna(merged.mean())\n",
        "merged =merged[~merged.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
        "# merged = merged.sample(n=50000, random_state=42)\n",
        "\n",
        "\n",
        "#  merged = merged.sample(n=50000, random_state=42)\n",
        "merged=np.array(merged)\n",
        "y =merged[:,n] #5\n",
        "z=merged[:,6]\n",
        "#  merged= np.delete(merged,[n], axis=1)\n",
        "#56\n",
        "#merged=np.delete(merged,[0,1,2,3,4,7,8,9,10,11,12,13,14,15,16,17,18], axis=1)\n",
        "#4-14\n",
        "merged=np.delete(merged,[0,1,2,3,4,15,16,17,18], axis=1)\n",
        "#5-10\n",
        "#  merged=np.delete(merged,[0,1,2,3,4,11,12,13,15,16,17,18], axis=1)\n",
        "#print(merged)\n",
        "def min_max_scaling(df):\n",
        "    # copy the dataframe\n",
        "    df_norm = df\n",
        "    # apply min-max scaling\n",
        "   # for column in df_norm.columns:\n",
        "    df_norm = ((df_norm - 0) / (1 - 0))*((1-0)+0)\n",
        "    #print(\"number \"+str(df)+\"converted\"+str(df_norm))\n",
        "    return df_norm\n",
        "y1 = DynamicArray()\n",
        "y11 = DynamicArray()\n",
        "z1 = DynamicArray()\n",
        "z11 = DynamicArray()\n",
        "c=0\n",
        "c1=0\n",
        "c2=0\n",
        "c3=0\n",
        "c4=0\n",
        "c5=0\n",
        "y=np.array(y)\n",
        "for a in y:\n",
        " y11.append(min_max_scaling(a))\n",
        "for a in y11:\n",
        " if  0.05> a>= 0:\n",
        "  y1.append(0)\n",
        "  c1+=1\n",
        " elif  0.125> a>= 0.05:\n",
        "  y1.append(1)\n",
        "  c2+=1\n",
        " elif 1>= a>= 0.125:\n",
        "  y1.append(2)\n",
        "  #print(a)\n",
        "  c3+=1\n",
        " else:\n",
        "  y1.append(0)\n",
        "  c+=1\n",
        "y11 = []\n",
        "for a in range(len(y1)):\n",
        " if (y1[a]-y1[a-1]>=2) :\n",
        "  #print(abs(y1[a]-y1[a-1]))\n",
        "  y11.append(1)\n",
        " else:\n",
        "  #print(abs(y1[a]-y1[a-1]))\n",
        "  y11.append(0)\n",
        "  #index=index+1\n",
        "\n",
        "#del y1\n",
        "print(\"c=\"+str(c)+\"c1=\"+str(c1)+\"c2=\"+str(c2)+\"c3=\"+str(c3)+\"c4=\"+str(c4)+\"c5=\"+str(c5))\n",
        "c3=0\n",
        "c2=0\n",
        "for q in y11:\n",
        " if q==1:\n",
        "  c3=c3+1\n",
        " else:\n",
        "  c2=c2+1\n",
        "print(c3)\n",
        "labels =y11#np.array(y11)# raw_data[:, -1]\n",
        "data = merged\n",
        "\n",
        "# # The other dat\n",
        "# from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# ros = RandomOverSampler(random_state=42)\n",
        "# data,labels = ros.fit_resample(data1,labels1)\n",
        "\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    data, labels, test_size=0.3, shuffle=False)\n",
        "num_ones = np.count_nonzero(labels)\n",
        "num_zeros = len(labels) - num_ones\n",
        "print(f\"Number of ones: {num_ones}\")\n",
        "print(f\"Number of zeros: {num_zeros}\")\n",
        "# Convert the data to numpy arrays\n",
        "X_train = np.array(train_data)\n",
        "X_test = np.array(test_data)\n",
        "train_labels= np.array(train_labels)\n",
        "test_labels= np.array(test_labels)\n",
        "\n",
        "# Reshape the data to 3D arrays\n",
        "X_train = np.reshape(X_train, (X_train.shape[0],1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "test_labels=np.reshape(test_labels,(test_labels.shape[0], 1))\n",
        "train_labels=np.reshape(train_labels,(train_labels.shape[0], 1))\n",
        "\n",
        "\"\"\"#Non- Gaussian Code  #LSTM AT HT\"\"\"\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(train_labels)\n",
        "X_test  = np.array(X_test)\n",
        "y_test  = np.array(test_labels)\n",
        "# y_test = y_test.flatten()\n",
        "\"\"\"#LSTM AT HY GA\"\"\"\n",
        "\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
        "\n",
        "X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the fitness function\n",
        "def evaluate_model(individual):\n",
        "    # Extract the hyperparameters from the individual\n",
        "    units = individual[0]\n",
        "    batch_size = individual[1]\n",
        "    dropout_rate = individual[2]\n",
        "    if units == 0:\n",
        "        units = 32\n",
        "    if batch_size == 0:\n",
        "        batch_size = 32\n",
        "    if dropout_rate == 0:\n",
        "        dropout_rate = 0.1\n",
        "\n",
        "    print(\"Units:\", units, \"Batch Size:\", batch_size, \"Dropout Rate:\", dropout_rate)\n",
        "\n",
        "    # Build the LSTM model with the specified hyperparameters\n",
        "    inputs = Input(shape=(1, 10))\n",
        "    lstm_out, state_h, state_c = LSTM(units, return_sequences=True, return_state=True)(inputs)\n",
        "    attention_out = Attention()([lstm_out, lstm_out])\n",
        "    dense_out = Dense(16, activation='relu')(attention_out)\n",
        "    outputs = Dense(1, activation='sigmoid')(dense_out)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train1, y_train1, epochs=1, batch_size=batch_size, shuffle=True, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "    # Make predictions on test data\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.where(y_pred > 0.5, 1, 0)\n",
        "    y_pred_binary = np.round(y_pred).flatten()\n",
        "    if len(y_test) != len(y_pred_classes):\n",
        "     raise ValueError(\"Mismatch in target shapes.\")\n",
        "    y_pred_binary=np.array(y_pred_binary)\n",
        "    print(\"y_test:\", y_test)\n",
        "    print(\"y_pred_binary:\", y_pred_binary)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "    precision = precision_score(y_test, y_pred_binary)\n",
        "    recall = recall_score(y_test, y_pred_binary)\n",
        "    f1 = f1_score(y_test, y_pred_binary)\n",
        "    # Ensure the targets have the same shape\n",
        "\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F-score:\", f1)\n",
        "    # macro_f1 = f1_score(y_test, y_pred_classes, average='macro')\n",
        "    # print(\"Macro F-score:\",macro_f1)\n",
        "    # macro_f1 = f1_score(y_test, y_pred_classes, average='micro')\n",
        "    # print(\"Micro F-score:\",macro_f1)\n",
        "\n",
        "    if len(y_test) != len(y_pred_binary) or len(y_test) != len(y_pred_classes):\n",
        "     raise ValueError(\"Mismatch in target shapes.\")\n",
        "    # Check the shapes and data types of y_test and y_pred_binary\n",
        "    print(\"y_test shape:\", y_test.shape)\n",
        "    print(\"y_pred_binary shape:\", y_pred_binary.shape)\n",
        "    print(\"y_pred_class shape:\", y_pred_classes.shape)\n",
        "    print(\"y_test data type:\", y_test.dtype)\n",
        "    print(\"y_pred_binary data type:\", y_pred_binary.dtype)\n",
        "\n",
        "    # Calculate the confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "    print(\"Confusion matrix:\", conf_matrix)\n",
        "\n",
        "    # Unpack the values from the confusion matrix\n",
        "    tn, fp, fn, tp = conf_matrix.ravel()\n",
        "    fpr = fp / (fp + tn)\n",
        "    fnr = fn / (fn + tp)\n",
        "\n",
        "    print(\"False Positive Rate (FPR):\", fpr)\n",
        "    print(\"False Negative Rate (FNR):\", fnr)\n",
        "    print(\"TN\", tn, \"  FP\",fp,\"  FN\",fn,\"  TP\",tp)\n",
        "\n",
        "    # Return the fitness value (accuracy) as a tuple\n",
        "    return accuracy,\n",
        "\n",
        "# Define the individual and population size\n",
        "INDIVIDUAL_SIZE = 3\n",
        "POPULATION_SIZE = 4\n",
        "\n",
        "# Create the individual and population classes\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "# Create the toolbox\n",
        "toolbox = base.Toolbox()\n",
        "\n",
        "# Define the hyperparameter ranges\n",
        "units_range = [32,64, 128]\n",
        "batch_size_range = [16, 32, 64,500,1000]\n",
        "dropout_rate_range = [0.1, 0.2, 0.3]\n",
        "# Register the hyperparameters in the toolbox\n",
        "# Define the custom initialization function for units\n",
        "# def init_units():\n",
        "#     return np.random.choice(units_range)\n",
        "#     return np.random.choice(units_range)\n",
        "def init_units():\n",
        "    return int(np.random.choice(units_range)) + 32\n",
        "\n",
        "# Register the units attribute with the custom initialization function\n",
        "toolbox.register(\"units\", init_units)\n",
        "\n",
        "# toolbox.register(\"units\", np.random.choice, units_range)\n",
        "def init_batch_size_range():\n",
        "    return int(np.random.choice(batch_size_range)) + 32\n",
        "\n",
        "toolbox.register(\"batch_size\", init_batch_size_range)\n",
        "def init_dropout_rate_range():\n",
        "    return int(np.random.choice(dropout_rate_range))\n",
        "toolbox.register(\"dropout_rate\", init_dropout_rate_range)\n",
        "\n",
        "# # Define the individual initialization function\n",
        "# toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
        "#                  (toolbox.units, toolbox.batch_size, toolbox.dropout_rate), n=1)\n",
        "# Define the individual initialization function\n",
        "def init_individual():\n",
        "    return creator.Individual([toolbox.units(), toolbox.batch_size(), toolbox.dropout_rate()])\n",
        "\n",
        "# Register the individual initialization function\n",
        "toolbox.register(\"individual\", init_individual)\n",
        "# Define the population initialization function\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "# Register the evaluation function\n",
        "toolbox.register(\"evaluate\", evaluate_model)\n",
        "\n",
        "# Register the selection, crossover, and mutation operations\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutUniformInt, low=0, up=2, indpb=0.5)\n",
        "\n",
        "# Define the number of generations and probability of crossover/mutation\n",
        "N_GENERATIONS = 10\n",
        "CROSSOVER_PROB = 0.8\n",
        "MUTATION_PROB = 0.2\n",
        "\n",
        "# Create the population\n",
        "population = toolbox.population(n=POPULATION_SIZE)\n",
        "\n",
        "# Create the hall of fame to store the best individuals\n",
        "hof = tools.HallOfFame(maxsize=1)\n",
        "\n",
        "# Define the statistics to collect during evolution\n",
        "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "stats.register(\"avg\", np.mean)\n",
        "stats.register(\"min\", np.min)\n",
        "stats.register(\"max\", np.max)\n",
        "\n",
        "# Run the evolution\n",
        "population, logbook = algorithms.eaSimple(\n",
        "    population,\n",
        "    toolbox,\n",
        "    cxpb=CROSSOVER_PROB,\n",
        "    mutpb=MUTATION_PROB,\n",
        "    ngen=N_GENERATIONS,\n",
        "    stats=stats,\n",
        "    halloffame=hof,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Get the best individual from the hall of fame\n",
        "best_individual = hof[0]\n",
        "\n",
        "# Print the best individual and its fitness value\n",
        "print(\"Best Individual:\", best_individual)\n",
        "print(\"Fitness Value:\", best_individual.fitness.values[0])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "erdvdFBzPzoT",
        "pHrhxJ5xNafj",
        "uajrkrt9NQik"
      ],
      "provenance": [],
      "mount_file_id": "1tpBnshFcFzlR3qOEb-Xl04LgNycwfipJ",
      "authorship_tag": "ABX9TyPVGA2LFUKCm9na539INs8Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}